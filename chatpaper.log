2025-11-09 00:54:09,371 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 00:54:09,374 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 00:54:09,376 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 00:54:09,380 - WARNING - root - 初始化 LLM 客户端失败: unexpected indent (llm_client.py, line 96)
2025-11-09 00:54:09,382 - INFO - root - === 运行配置 ===
2025-11-09 00:54:09,382 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 00:54:09,382 - INFO - root - PDF目录: d:\ChatPaper\myPapers
2025-11-09 00:54:09,383 - INFO - root - 最大处理数量: 2
2025-11-09 00:54:09,384 - INFO - root - 保存图片: 是
2025-11-09 00:54:09,384 - INFO - root - 输出语言: 中文
2025-11-09 00:54:09,385 - INFO - root - 强制重新处理: 否
2025-11-09 00:54:09,385 - INFO - root - ====================
2025-11-09 00:54:09,386 - INFO - root - 从本地目录读取PDF文件：d:\ChatPaper\myPapers
2025-11-09 00:54:10,505 - INFO - root - 成功加载PDF文件：demo.pdf
2025-11-09 00:54:10,510 - INFO - root - 正在总结论文 1/1: “Good Robot!”: Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer
2025-11-09 00:54:10,510 - ERROR - root - LLM 客户端未初始化，将使用备用响应。
2025-11-09 00:54:10,510 - ERROR - root - LLM 客户端未初始化，将使用备用响应。
2025-11-09 00:54:10,510 - ERROR - root - LLM 客户端未初始化，将使用备用响应。
2025-11-09 00:54:10,513 - INFO - root - 正在提取论文图片...
2025-11-09 00:54:10,726 - INFO - root - 已保存图片 1/10：./export\images_“Good Robot!”_ Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with\figure_1_page2.jpeg
2025-11-09 00:54:10,833 - INFO - root - 已保存图片 2/10：./export\images_“Good Robot!”_ Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with\figure_2_page1.jpeg
2025-11-09 00:54:10,920 - INFO - root - 已保存图片 3/10：./export\images_“Good Robot!”_ Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with\figure_3_page7.jpeg
2025-11-09 00:54:10,982 - INFO - root - 已保存图片 4/10：./export\images_“Good Robot!”_ Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with\figure_4_page4.jpeg
2025-11-09 00:54:11,019 - INFO - root - 已保存图片 5/10：./export\images_“Good Robot!”_ Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with\figure_5_page4.png
2025-11-09 00:54:11,053 - INFO - root - 已保存图片 6/10：./export\images_“Good Robot!”_ Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with\figure_6_page7.jpeg
2025-11-09 00:54:11,069 - INFO - root - 成功添加图片 1：./export\images_“Good Robot!”_ Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with\figure_1_page2.jpeg
2025-11-09 00:54:11,069 - INFO - root - 成功添加图片 2：./export\images_“Good Robot!”_ Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with\figure_2_page1.jpeg
2025-11-09 00:54:11,070 - INFO - root - 成功添加图片 3：./export\images_“Good Robot!”_ Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with\figure_3_page7.jpeg
2025-11-09 00:54:11,071 - INFO - root - 成功添加图片 4：./export\images_“Good Robot!”_ Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with\figure_4_page4.jpeg
2025-11-09 00:54:11,071 - INFO - root - 成功添加图片 5：./export\images_“Good Robot!”_ Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with\figure_5_page4.png
2025-11-09 00:54:11,071 - INFO - root - 成功添加图片 6：./export\images_“Good Robot!”_ Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with\figure_6_page7.jpeg
2025-11-09 00:54:11,076 - INFO - root - 论文《“Good Robot!”: Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer》的分析已保存到 ./export\“Good Robot!”_ Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with.md
2025-11-09 00:54:11,080 - INFO - root - summary time: 1.71 seconds
2025-11-09 00:56:18,518 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 00:56:18,518 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 00:56:18,518 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 00:56:18,518 - WARNING - root - 初始化 LLM 客户端失败: unexpected indent (llm_client.py, line 96)
2025-11-09 00:56:18,524 - INFO - root - === 运行配置 ===
2025-11-09 00:56:18,524 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 00:56:18,524 - INFO - root - PDF目录: d:\ChatPaper\myPapers
2025-11-09 00:56:18,525 - INFO - root - 最大处理数量: 2
2025-11-09 00:56:18,525 - INFO - root - 保存图片: 是
2025-11-09 00:56:18,525 - INFO - root - 输出语言: 中文
2025-11-09 00:56:18,525 - INFO - root - 强制重新处理: 否
2025-11-09 00:56:18,527 - INFO - root - ====================
2025-11-09 00:56:18,527 - INFO - root - 从本地目录读取PDF文件：d:\ChatPaper\myPapers
2025-11-09 00:56:19,108 - INFO - root - 成功加载PDF文件：demo.pdf
2025-11-09 00:56:19,108 - INFO - root - 跳过已处理论文 “Good Robot!”: Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer：d:\ChatPaper\myPapers\demo.pdf
2025-11-09 00:56:19,108 - INFO - root - summary time: 0.59 seconds
2025-11-09 00:56:38,605 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 00:56:38,606 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 00:56:38,607 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 00:56:38,610 - WARNING - root - 初始化 LLM 客户端失败: unexpected indent (llm_client.py, line 96)
2025-11-09 00:56:38,610 - INFO - root - === 运行配置 ===
2025-11-09 00:56:38,611 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 00:56:38,611 - INFO - root - PDF目录: d:\ChatPaper\myPapers
2025-11-09 00:56:38,611 - INFO - root - 最大处理数量: 2
2025-11-09 00:56:38,611 - INFO - root - 保存图片: 是
2025-11-09 00:56:38,611 - INFO - root - 输出语言: 中文
2025-11-09 00:56:38,611 - INFO - root - 强制重新处理: 否
2025-11-09 00:56:38,611 - INFO - root - ====================
2025-11-09 00:56:38,611 - INFO - root - 从本地目录读取PDF文件：d:\ChatPaper\myPapers
2025-11-09 00:56:39,256 - INFO - root - 成功加载PDF文件：demo.pdf
2025-11-09 00:56:39,257 - INFO - root - 跳过已处理论文 “Good Robot!”: Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer：d:\ChatPaper\myPapers\demo.pdf
2025-11-09 00:56:39,257 - INFO - root - summary time: 0.65 seconds
2025-11-09 00:57:37,925 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 00:57:37,926 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 00:57:37,928 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 00:57:39,179 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 00:57:44,038 - INFO - root - LLMClient: initialized model gemini-2.5-flash
2025-11-09 00:57:44,039 - INFO - root - 使用 LLM 模型: gemini-2.5-flash
2025-11-09 00:57:44,040 - INFO - root - === 运行配置 ===
2025-11-09 00:57:44,041 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 00:57:44,045 - INFO - root - PDF目录: d:\ChatPaper\myPapers
2025-11-09 00:57:44,049 - INFO - root - 最大处理数量: 2
2025-11-09 00:57:44,049 - INFO - root - 保存图片: 是
2025-11-09 00:57:44,051 - INFO - root - 输出语言: 中文
2025-11-09 00:57:44,052 - INFO - root - 强制重新处理: 否
2025-11-09 00:57:44,053 - INFO - root - ====================
2025-11-09 00:57:44,054 - INFO - root - 从本地目录读取PDF文件：d:\ChatPaper\myPapers
2025-11-09 00:57:45,100 - INFO - root - 成功加载PDF文件：demo.pdf
2025-11-09 00:57:45,101 - INFO - root - 跳过已处理论文 “Good Robot!”: Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer：d:\ChatPaper\myPapers\demo.pdf
2025-11-09 00:57:45,102 - INFO - root - summary time: 7.18 seconds
2025-11-09 00:57:54,670 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 00:57:54,670 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 00:57:54,670 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 00:57:55,831 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 00:57:58,715 - INFO - root - LLMClient: initialized model gemini-2.5-flash
2025-11-09 00:57:58,716 - INFO - root - 使用 LLM 模型: gemini-2.5-flash
2025-11-09 00:57:58,718 - INFO - root - === 运行配置 ===
2025-11-09 00:57:58,719 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 00:57:58,720 - INFO - root - PDF目录: d:\ChatPaper\myPapers
2025-11-09 00:57:58,721 - INFO - root - 最大处理数量: 2
2025-11-09 00:57:58,722 - INFO - root - 保存图片: 是
2025-11-09 00:57:58,723 - INFO - root - 输出语言: 中文
2025-11-09 00:57:58,724 - INFO - root - 强制重新处理: 否
2025-11-09 00:57:58,726 - INFO - root - ====================
2025-11-09 00:57:58,727 - INFO - root - 从本地目录读取PDF文件：d:\ChatPaper\myPapers
2025-11-09 00:57:59,367 - INFO - root - 成功加载PDF文件：demo.pdf
2025-11-09 00:57:59,367 - INFO - root - 跳过已处理论文 “Good Robot!”: Efﬁcient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer：d:\ChatPaper\myPapers\demo.pdf
2025-11-09 00:57:59,368 - INFO - root - summary time: 4.70 seconds
2025-11-09 00:58:27,401 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 00:58:27,401 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 00:58:27,405 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 00:58:28,300 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 00:58:33,329 - INFO - root - LLMClient: initialized model gemini-2.5-flash
2025-11-09 00:58:33,330 - INFO - root - 使用 LLM 模型: gemini-2.5-flash
2025-11-09 00:58:33,331 - INFO - root - === 运行配置 ===
2025-11-09 00:58:33,331 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 00:58:33,332 - INFO - root - 关键词: Quant
2025-11-09 00:58:33,332 - INFO - root - 查询: block float point
2025-11-09 00:58:33,332 - INFO - root - 排序: None
2025-11-09 00:58:33,332 - INFO - root - 最近天数: 180
2025-11-09 00:58:33,332 - INFO - root - 最大处理数量: 2
2025-11-09 00:58:33,333 - INFO - root - 保存图片: 是
2025-11-09 00:58:33,333 - INFO - root - 输出语言: 中文
2025-11-09 00:58:33,334 - INFO - root - 强制重新处理: 否
2025-11-09 00:58:33,335 - INFO - root - ====================
2025-11-09 00:58:33,335 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 00:59:13,415 - INFO - root - 正在总结论文 1/2: From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators
2025-11-09 00:59:51,953 - INFO - root - LLMClient: rate limit reached, sleeping 21.5s
2025-11-09 01:00:34,660 - INFO - root - 正在提取论文图片...
2025-11-09 01:14:08,444 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 01:14:08,444 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 01:14:08,449 - WARNING - root - LLMClient: Gemini API key not provided. LLM disabled.
2025-11-09 01:14:08,449 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 01:14:08,454 - INFO - root - 已创建目录：D:\ChatPaper\src\myPapers
2025-11-09 01:14:08,454 - INFO - root - === 运行配置 ===
2025-11-09 01:14:08,455 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 01:14:08,455 - INFO - root - 关键词: Quant
2025-11-09 01:14:08,456 - INFO - root - 查询: block float point
2025-11-09 01:14:08,456 - INFO - root - 排序: None
2025-11-09 01:14:08,457 - INFO - root - 最近天数: 180
2025-11-09 01:14:08,458 - INFO - root - 最大处理数量: 2
2025-11-09 01:14:08,458 - INFO - root - 保存图片: 是
2025-11-09 01:14:08,460 - INFO - root - 输出语言: 中文
2025-11-09 01:14:08,460 - INFO - root - 强制重新处理: 否
2025-11-09 01:14:08,461 - INFO - root - ====================
2025-11-09 01:14:08,461 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 01:14:18,810 - INFO - root - 正在总结论文 1/2: From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators
2025-11-09 01:14:18,810 - INFO - root - 正在提取论文图片...
2025-11-09 01:14:19,325 - INFO - root - 已保存图片 1/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 01:14:19,410 - INFO - root - 已保存图片 2/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 01:14:19,483 - INFO - root - 已保存图片 3/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 01:14:19,540 - INFO - root - 已保存图片 4/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 01:14:19,595 - INFO - root - 已保存图片 5/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 01:14:19,662 - INFO - root - 已保存图片 6/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 01:14:19,707 - INFO - root - 已保存图片 7/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 01:14:19,783 - INFO - root - 已保存图片 8/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 01:14:19,793 - INFO - root - 已保存图片 9/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 01:14:19,810 - INFO - root - 已保存图片 10/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 01:14:19,815 - INFO - root - 成功添加图片 1：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 01:14:19,815 - INFO - root - 成功添加图片 2：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 01:14:19,816 - INFO - root - 成功添加图片 3：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 01:14:19,818 - INFO - root - 成功添加图片 4：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 01:14:19,820 - INFO - root - 成功添加图片 5：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 01:14:19,820 - INFO - root - 成功添加图片 6：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 01:14:19,822 - INFO - root - 成功添加图片 7：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 01:14:19,825 - INFO - root - 成功添加图片 8：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 01:14:19,826 - INFO - root - 成功添加图片 9：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 01:14:19,827 - INFO - root - 成功添加图片 10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 01:14:19,829 - INFO - root - 论文《From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators》的分析已保存到 ./export\From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural.md
2025-11-09 01:14:19,836 - INFO - root - 正在总结论文 2/2: INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats
2025-11-09 01:14:19,845 - INFO - root - 正在提取论文图片...
2025-11-09 01:14:20,289 - INFO - root - 已保存图片 1/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 01:14:20,354 - INFO - root - 已保存图片 2/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 01:14:20,388 - INFO - root - 已保存图片 3/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 01:14:20,450 - INFO - root - 已保存图片 4/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 01:14:20,454 - INFO - root - 成功添加图片 1：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 01:14:20,454 - INFO - root - 成功添加图片 2：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 01:14:20,455 - INFO - root - 成功添加图片 3：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 01:14:20,455 - INFO - root - 成功添加图片 4：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 01:14:20,457 - INFO - root - 论文《INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats》的分析已保存到 ./export\INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats.md
2025-11-09 01:14:20,461 - INFO - root - summary time: 12.02 seconds
2025-11-09 01:17:35,499 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 01:17:35,499 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 01:17:35,499 - WARNING - root - LLMClient: Gemini API key not provided. LLM disabled.
2025-11-09 01:17:35,499 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 01:17:35,499 - INFO - root - === 运行配置 ===
2025-11-09 01:17:35,499 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 01:17:35,499 - INFO - root - 关键词: Quant
2025-11-09 01:17:35,499 - INFO - root - 查询: block float point
2025-11-09 01:17:35,499 - INFO - root - 排序: None
2025-11-09 01:17:35,499 - INFO - root - 最近天数: 180
2025-11-09 01:17:35,499 - INFO - root - 最大处理数量: 2
2025-11-09 01:17:35,499 - INFO - root - 保存图片: 是
2025-11-09 01:17:35,499 - INFO - root - 输出语言: 中文
2025-11-09 01:17:35,499 - INFO - root - 强制重新处理: 否
2025-11-09 01:17:35,499 - INFO - root - ====================
2025-11-09 01:17:35,499 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 01:17:59,345 - INFO - root - 正在总结论文 1/2: From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators
2025-11-09 01:17:59,346 - INFO - root - 正在提取论文图片...
2025-11-09 01:17:59,741 - INFO - root - 已保存图片 1/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 01:17:59,811 - INFO - root - 已保存图片 2/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 01:17:59,872 - INFO - root - 已保存图片 3/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 01:17:59,923 - INFO - root - 已保存图片 4/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 01:18:00,011 - INFO - root - 已保存图片 5/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 01:18:00,104 - INFO - root - 已保存图片 6/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 01:18:00,147 - INFO - root - 已保存图片 7/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 01:18:00,211 - INFO - root - 已保存图片 8/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 01:18:00,221 - INFO - root - 已保存图片 9/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 01:18:00,239 - INFO - root - 已保存图片 10/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 01:18:00,241 - INFO - root - 成功添加图片 1：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 01:18:00,246 - INFO - root - 成功添加图片 2：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 01:18:00,246 - INFO - root - 成功添加图片 3：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 01:18:00,247 - INFO - root - 成功添加图片 4：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 01:18:00,247 - INFO - root - 成功添加图片 5：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 01:18:00,249 - INFO - root - 成功添加图片 6：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 01:18:00,249 - INFO - root - 成功添加图片 7：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 01:18:00,249 - INFO - root - 成功添加图片 8：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 01:18:00,249 - INFO - root - 成功添加图片 9：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 01:18:00,250 - INFO - root - 成功添加图片 10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 01:18:00,253 - INFO - root - 论文《From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators》的分析已保存到 ./export\From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural-1.md
2025-11-09 01:18:00,268 - INFO - root - 正在总结论文 2/2: INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats
2025-11-09 01:18:00,272 - INFO - root - 正在提取论文图片...
2025-11-09 01:18:00,811 - INFO - root - 已保存图片 1/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 01:18:00,888 - INFO - root - 已保存图片 2/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 01:18:00,915 - INFO - root - 已保存图片 3/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 01:18:00,974 - INFO - root - 已保存图片 4/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 01:18:00,980 - INFO - root - 成功添加图片 1：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 01:18:00,981 - INFO - root - 成功添加图片 2：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 01:18:00,982 - INFO - root - 成功添加图片 3：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 01:18:00,983 - INFO - root - 成功添加图片 4：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 01:18:00,985 - INFO - root - 论文《INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats》的分析已保存到 ./export\INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats-1.md
2025-11-09 01:18:00,999 - INFO - root - summary time: 25.50 seconds
2025-11-09 01:18:24,953 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 01:18:24,953 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 01:18:24,953 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 01:18:26,047 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 01:18:30,078 - INFO - root - LLMClient: initialized model gemini-2.5-flash
2025-11-09 01:18:30,079 - INFO - root - 使用 LLM 模型: gemini-2.5-flash
2025-11-09 01:18:30,080 - INFO - root - === 运行配置 ===
2025-11-09 01:18:30,081 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 01:18:30,081 - INFO - root - 关键词: Quant
2025-11-09 01:18:30,082 - INFO - root - 查询: block float point
2025-11-09 01:18:30,082 - INFO - root - 排序: None
2025-11-09 01:18:30,082 - INFO - root - 最近天数: 180
2025-11-09 01:18:30,083 - INFO - root - 最大处理数量: 2
2025-11-09 01:18:30,083 - INFO - root - 保存图片: 是
2025-11-09 01:18:30,083 - INFO - root - 输出语言: 中文
2025-11-09 01:18:30,083 - INFO - root - 强制重新处理: 否
2025-11-09 01:18:30,084 - INFO - root - ====================
2025-11-09 01:18:30,084 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 01:18:42,682 - INFO - root - 正在总结论文 1/2: From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators
2025-11-09 01:19:21,628 - INFO - root - LLMClient: rate limit reached, sleeping 21.1s
2025-11-09 01:20:03,870 - INFO - root - 正在提取论文图片...
2025-11-09 01:20:04,695 - INFO - root - 已保存图片 1/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 01:20:04,870 - INFO - root - 已保存图片 2/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 01:20:05,078 - INFO - root - 已保存图片 3/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 01:20:05,256 - INFO - root - 已保存图片 4/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 01:20:05,391 - INFO - root - 已保存图片 5/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 01:20:05,493 - INFO - root - 已保存图片 6/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 01:20:05,603 - INFO - root - 已保存图片 7/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 01:20:05,681 - INFO - root - 已保存图片 8/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 01:20:05,703 - INFO - root - 已保存图片 9/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 01:20:05,726 - INFO - root - 已保存图片 10/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 01:20:05,732 - INFO - root - 成功添加图片 1：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 01:20:05,732 - INFO - root - 成功添加图片 2：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 01:20:05,734 - INFO - root - 成功添加图片 3：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 01:20:05,734 - INFO - root - 成功添加图片 4：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 01:20:05,734 - INFO - root - 成功添加图片 5：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 01:20:05,737 - INFO - root - 成功添加图片 6：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 01:20:05,737 - INFO - root - 成功添加图片 7：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 01:20:05,738 - INFO - root - 成功添加图片 8：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 01:20:05,739 - INFO - root - 成功添加图片 9：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 01:20:05,741 - INFO - root - 成功添加图片 10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 01:20:05,749 - INFO - root - 论文《From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators》的分析已保存到 ./export\From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural-2.md
2025-11-09 01:20:05,752 - INFO - root - 正在总结论文 2/2: INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats
2025-11-09 01:20:18,101 - INFO - root - LLMClient: rate limit reached, sleeping 24.6s
2025-11-09 01:21:36,061 - INFO - root - 正在提取论文图片...
2025-11-09 01:21:36,446 - INFO - root - 已保存图片 1/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 01:21:36,499 - INFO - root - 已保存图片 2/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 01:21:36,546 - INFO - root - 已保存图片 3/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 01:21:36,592 - INFO - root - 已保存图片 4/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 01:21:36,604 - INFO - root - 成功添加图片 1：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 01:21:36,604 - INFO - root - 成功添加图片 2：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 01:21:36,604 - INFO - root - 成功添加图片 3：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 01:21:36,604 - INFO - root - 成功添加图片 4：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 01:21:36,613 - INFO - root - 论文《INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats》的分析已保存到 ./export\INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats-2.md
2025-11-09 01:21:36,618 - INFO - root - summary time: 191.66 seconds
2025-11-09 01:26:11,351 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 01:26:11,351 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 01:26:11,351 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 01:26:12,198 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 01:26:14,752 - INFO - root - LLMClient: initialized model gemini-2.5-flash
2025-11-09 01:26:14,752 - INFO - root - 使用 LLM 模型: gemini-2.5-flash
2025-11-09 01:26:14,753 - INFO - root - === 运行配置 ===
2025-11-09 01:26:14,754 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 01:26:14,755 - INFO - root - 关键词: Quant
2025-11-09 01:26:14,756 - INFO - root - 查询: block float point
2025-11-09 01:26:14,757 - INFO - root - 排序: None
2025-11-09 01:26:14,757 - INFO - root - 最近天数: 180
2025-11-09 01:26:14,759 - INFO - root - 最大处理数量: 2
2025-11-09 01:26:14,760 - INFO - root - 保存图片: 是
2025-11-09 01:26:14,760 - INFO - root - 输出语言: 中文
2025-11-09 01:26:14,761 - INFO - root - 强制重新处理: 否
2025-11-09 01:26:14,762 - INFO - root - ====================
2025-11-09 01:26:14,763 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 01:26:31,982 - INFO - root - 正在总结论文 1/2: From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators
2025-11-09 01:27:10,578 - INFO - root - LLMClient: rate limit reached, sleeping 21.4s
2025-11-09 01:27:51,208 - INFO - root - 正在提取论文图片...
2025-11-09 01:27:51,587 - INFO - root - 已保存图片 1/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 01:27:51,674 - INFO - root - 已保存图片 2/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 01:27:51,738 - INFO - root - 已保存图片 3/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 01:27:51,809 - INFO - root - 已保存图片 4/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 01:27:51,866 - INFO - root - 已保存图片 5/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 01:27:51,923 - INFO - root - 已保存图片 6/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 01:27:51,979 - INFO - root - 已保存图片 7/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 01:27:52,046 - INFO - root - 已保存图片 8/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 01:27:52,057 - INFO - root - 已保存图片 9/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 01:27:52,073 - INFO - root - 已保存图片 10/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 01:27:52,081 - INFO - root - 成功添加图片 1：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 01:27:52,081 - INFO - root - 成功添加图片 2：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 01:27:52,083 - INFO - root - 成功添加图片 3：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 01:27:52,083 - INFO - root - 成功添加图片 4：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 01:27:52,083 - INFO - root - 成功添加图片 5：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 01:27:52,085 - INFO - root - 成功添加图片 6：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 01:27:52,086 - INFO - root - 成功添加图片 7：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 01:27:52,087 - INFO - root - 成功添加图片 8：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 01:27:52,088 - INFO - root - 成功添加图片 9：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 01:27:52,089 - INFO - root - 成功添加图片 10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 01:27:52,094 - INFO - root - 论文《From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators》的分析已保存到 ./export\From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural.md
2025-11-09 01:27:52,099 - INFO - root - 正在总结论文 2/2: INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats
2025-11-09 01:28:03,710 - INFO - root - LLMClient: rate limit reached, sleeping 28.3s
2025-11-09 01:29:19,818 - INFO - root - 正在提取论文图片...
2025-11-09 01:29:20,248 - INFO - root - 已保存图片 1/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 01:29:20,306 - INFO - root - 已保存图片 2/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 01:29:20,332 - INFO - root - 已保存图片 3/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 01:29:20,412 - INFO - root - 已保存图片 4/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 01:29:20,416 - INFO - root - 成功添加图片 1：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 01:29:20,417 - INFO - root - 成功添加图片 2：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 01:29:20,419 - INFO - root - 成功添加图片 3：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 01:29:20,419 - INFO - root - 成功添加图片 4：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 01:29:20,424 - INFO - root - 论文《INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats》的分析已保存到 ./export\INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats.md
2025-11-09 01:29:20,428 - INFO - root - summary time: 189.08 seconds
2025-11-09 01:32:09,191 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 01:32:09,191 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 01:32:09,191 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 01:32:10,153 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 01:32:14,357 - INFO - root - LLMClient: initialized model gemini-2.5-flash
2025-11-09 01:32:14,358 - INFO - root - 使用 LLM 模型: gemini-2.5-flash
2025-11-09 01:32:14,359 - INFO - root - === 运行配置 ===
2025-11-09 01:32:14,359 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 01:32:14,359 - INFO - root - 关键词: Quant
2025-11-09 01:32:14,360 - INFO - root - 查询: block float point quant
2025-11-09 01:32:14,361 - INFO - root - 排序: None
2025-11-09 01:32:14,361 - INFO - root - 最近天数: 180
2025-11-09 01:32:14,362 - INFO - root - 最大处理数量: 50
2025-11-09 01:32:14,363 - INFO - root - 保存图片: 是
2025-11-09 01:32:14,364 - INFO - root - 输出语言: 中文
2025-11-09 01:32:14,364 - INFO - root - 强制重新处理: 否
2025-11-09 01:32:14,365 - INFO - root - ====================
2025-11-09 01:32:14,366 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 01:32:15,526 - INFO - root - 没有找到要处理的论文，程序退出
2025-11-09 01:32:15,526 - INFO - root - summary time: 6.34 seconds
2025-11-09 01:33:18,608 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 01:33:18,608 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 01:33:18,608 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 01:33:19,613 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 01:33:22,894 - INFO - root - LLMClient: initialized model gemini-2.5-flash
2025-11-09 01:33:22,895 - INFO - root - 使用 LLM 模型: gemini-2.5-flash
2025-11-09 01:33:22,896 - INFO - root - === 运行配置 ===
2025-11-09 01:33:22,896 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 01:33:22,897 - INFO - root - 关键词: Quant
2025-11-09 01:33:22,898 - INFO - root - 查询: block float point
2025-11-09 01:33:22,899 - INFO - root - 排序: None
2025-11-09 01:33:22,899 - INFO - root - 最近天数: 180
2025-11-09 01:33:22,900 - INFO - root - 最大处理数量: 50
2025-11-09 01:33:22,901 - INFO - root - 保存图片: 是
2025-11-09 01:33:22,902 - INFO - root - 输出语言: 中文
2025-11-09 01:33:22,903 - INFO - root - 强制重新处理: 否
2025-11-09 01:33:22,904 - INFO - root - ====================
2025-11-09 01:33:22,904 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 01:35:28,889 - INFO - root - 正在总结论文 1/30: From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators
2025-11-09 01:36:07,843 - INFO - root - LLMClient: rate limit reached, sleeping 21.0s
2025-11-09 01:36:46,512 - INFO - root - 正在提取论文图片...
2025-11-09 01:36:46,941 - INFO - root - 已保存图片 1/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 01:36:47,012 - INFO - root - 已保存图片 2/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 01:36:47,088 - INFO - root - 已保存图片 3/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 01:36:47,156 - INFO - root - 已保存图片 4/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 01:36:47,208 - INFO - root - 已保存图片 5/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 01:36:47,265 - INFO - root - 已保存图片 6/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 01:36:47,312 - INFO - root - 已保存图片 7/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 01:36:47,369 - INFO - root - 已保存图片 8/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 01:36:47,382 - INFO - root - 已保存图片 9/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 01:36:47,403 - INFO - root - 已保存图片 10/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 01:36:47,408 - INFO - root - 成功添加图片 1：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 01:36:47,408 - INFO - root - 成功添加图片 2：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 01:36:47,410 - INFO - root - 成功添加图片 3：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 01:36:47,410 - INFO - root - 成功添加图片 4：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 01:36:47,410 - INFO - root - 成功添加图片 5：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 01:36:47,411 - INFO - root - 成功添加图片 6：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 01:36:47,411 - INFO - root - 成功添加图片 7：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 01:36:47,411 - INFO - root - 成功添加图片 8：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 01:36:47,411 - INFO - root - 成功添加图片 9：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 01:36:47,412 - INFO - root - 成功添加图片 10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 01:36:47,418 - INFO - root - 论文《From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators》的分析已保存到 ./export\From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural.md
2025-11-09 01:36:47,423 - INFO - root - 正在总结论文 2/30: INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats
2025-11-09 01:36:58,155 - INFO - root - LLMClient: rate limit reached, sleeping 30.7s
2025-11-09 01:38:16,887 - INFO - root - 正在提取论文图片...
2025-11-09 01:38:17,284 - INFO - root - 已保存图片 1/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 01:38:17,342 - INFO - root - 已保存图片 2/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 01:38:17,375 - INFO - root - 已保存图片 3/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 01:38:17,449 - INFO - root - 已保存图片 4/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 01:38:17,451 - INFO - root - 成功添加图片 1：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 01:38:17,452 - INFO - root - 成功添加图片 2：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 01:38:17,452 - INFO - root - 成功添加图片 3：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 01:38:17,453 - INFO - root - 成功添加图片 4：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 01:38:17,459 - INFO - root - 论文《INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats》的分析已保存到 ./export\INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats.md
2025-11-09 01:38:17,460 - INFO - root - 正在总结论文 3/30: MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving
2025-11-09 01:38:17,464 - INFO - root - LLMClient: rate limit reached, sleeping 11.4s
2025-11-09 01:38:39,386 - INFO - root - LLMClient: rate limit reached, sleeping 17.3s
2025-11-09 01:39:27,594 - INFO - root - LLMClient: rate limit reached, sleeping 1.3s
2025-11-09 01:39:49,790 - INFO - root - 正在提取论文图片...
2025-11-09 01:39:49,913 - INFO - root - 已保存图片 1/10：./export\images_MX+_ Pushing the Limits of Microscaling Formats for Efficient Large Language Mod\figure_1_page3.png
2025-11-09 01:39:49,913 - INFO - root - 成功添加图片 1：./export\images_MX+_ Pushing the Limits of Microscaling Formats for Efficient Large Language Mod\figure_1_page3.png
2025-11-09 01:39:49,919 - INFO - root - 论文《MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving》的分析已保存到 ./export\MX+_ Pushing the Limits of Microscaling Formats for Efficient Large Language Mod.md
2025-11-09 01:39:49,920 - INFO - root - 正在总结论文 4/30: F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs
2025-11-09 01:39:49,924 - INFO - root - LLMClient: rate limit reached, sleeping 6.8s
2025-11-09 01:40:08,222 - INFO - root - LLMClient: rate limit reached, sleeping 20.7s
2025-11-09 01:41:47,120 - INFO - root - 正在提取论文图片...
2025-11-09 01:41:47,964 - INFO - root - 已保存图片 1/10：./export\images_F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs\figure_1_page4.png
2025-11-09 01:41:48,120 - INFO - root - 已保存图片 2/10：./export\images_F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs\figure_2_page3.png
2025-11-09 01:41:48,183 - INFO - root - 已保存图片 3/10：./export\images_F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs\figure_3_page2.png
2025-11-09 01:41:48,190 - INFO - root - 成功添加图片 1：./export\images_F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs\figure_1_page4.png
2025-11-09 01:41:48,190 - INFO - root - 成功添加图片 2：./export\images_F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs\figure_2_page3.png
2025-11-09 01:41:48,191 - INFO - root - 成功添加图片 3：./export\images_F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs\figure_3_page2.png
2025-11-09 01:41:48,197 - INFO - root - 论文《F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs》的分析已保存到 ./export\F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs.md
2025-11-09 01:41:48,204 - INFO - root - 正在总结论文 5/30: Computationally Efficient Neural Receivers via Axial Self-Attention
2025-11-09 01:41:56,029 - INFO - root - LLMClient: rate limit reached, sleeping 30.6s
2025-11-09 01:43:18,247 - INFO - root - 正在提取论文图片...
2025-11-09 01:43:19,213 - INFO - root - 已保存图片 1/10：./export\images_Computationally Efficient Neural Receivers via Axial Self-Attention\figure_1_page5.png
2025-11-09 01:43:19,215 - INFO - root - 成功添加图片 1：./export\images_Computationally Efficient Neural Receivers via Axial Self-Attention\figure_1_page5.png
2025-11-09 01:43:19,222 - INFO - root - 论文《Computationally Efficient Neural Receivers via Axial Self-Attention》的分析已保存到 ./export\Computationally Efficient Neural Receivers via Axial Self-Attention.md
2025-11-09 01:43:19,222 - INFO - root - 正在总结论文 6/30: Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs
2025-11-09 01:43:19,229 - INFO - root - LLMClient: rate limit reached, sleeping 7.4s
2025-11-09 01:43:37,171 - INFO - root - LLMClient: rate limit reached, sleeping 22.5s
2025-11-09 01:44:49,455 - INFO - root - 正在提取论文图片...
2025-11-09 01:44:49,522 - INFO - root - 已保存图片 1/10：./export\images_Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs\figure_1_page3.png
2025-11-09 01:44:49,560 - INFO - root - 已保存图片 2/10：./export\images_Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs\figure_2_page3.png
2025-11-09 01:44:49,671 - INFO - root - 已保存图片 3/10：./export\images_Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs\figure_3_page3.png
2025-11-09 01:44:49,671 - INFO - root - 成功添加图片 1：./export\images_Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs\figure_1_page3.png
2025-11-09 01:44:49,671 - INFO - root - 成功添加图片 2：./export\images_Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs\figure_2_page3.png
2025-11-09 01:44:49,671 - INFO - root - 成功添加图片 3：./export\images_Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs\figure_3_page3.png
2025-11-09 01:44:49,671 - INFO - root - 论文《Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs》的分析已保存到 ./export\Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs.md
2025-11-09 01:44:49,684 - INFO - root - 正在总结论文 7/30: Dissecting Transformers: A CLEAR Perspective towards Green AI
2025-11-09 01:44:49,685 - INFO - root - LLMClient: rate limit reached, sleeping 10.0s
2025-11-09 01:45:09,772 - INFO - root - LLMClient: rate limit reached, sleeping 19.1s
2025-11-09 09:31:54,038 - ERROR - root - LLMClient: generation error: 504 Deadline Exceeded
Traceback (most recent call last):
  File "D:\ChatPaper\llm_client.py", line 117, in generate
    resp = self.model.generate_content(prompt, safety_settings=self.safety_settings)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\generativeai\generative_models.py", line 331, in generate_content
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 835, in generate_content
    response = rpc(
               ^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
                 ^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
             ^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\grpc_helpers.py", line 77, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.DeadlineExceeded: 504 Deadline Exceeded
2025-11-09 09:32:55,151 - INFO - root - LLMClient: retry attempt 2 for generation
2025-11-09 09:33:40,009 - INFO - root - 正在提取论文图片...
2025-11-09 09:33:40,070 - WARNING - root - 处理页面 19 的图片 1 时出错：Image size (360000000 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.
2025-11-09 09:33:40,180 - INFO - root - 已保存图片 1/10：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_1_page3.png
2025-11-09 09:33:40,254 - INFO - root - 已保存图片 2/10：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_2_page3.png
2025-11-09 09:33:40,317 - INFO - root - 已保存图片 3/10：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_3_page3.png
2025-11-09 09:33:40,398 - INFO - root - 已保存图片 4/10：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_4_page3.png
2025-11-09 09:33:40,475 - INFO - root - 已保存图片 5/10：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_5_page3.png
2025-11-09 09:33:40,475 - INFO - root - 成功添加图片 1：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_1_page3.png
2025-11-09 09:33:40,475 - INFO - root - 成功添加图片 2：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_2_page3.png
2025-11-09 09:33:40,475 - INFO - root - 成功添加图片 3：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_3_page3.png
2025-11-09 09:33:40,475 - INFO - root - 成功添加图片 4：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_4_page3.png
2025-11-09 09:33:40,475 - INFO - root - 成功添加图片 5：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_5_page3.png
2025-11-09 09:33:40,482 - INFO - root - 论文《Dissecting Transformers: A CLEAR Perspective towards Green AI》的分析已保存到 ./export\Dissecting Transformers_ A CLEAR Perspective towards Green AI.md
2025-11-09 09:33:40,490 - INFO - root - 正在总结论文 8/30: Microscaling Floating Point Formats for Large Language Models
2025-11-09 09:33:40,490 - INFO - root - LLMClient: rate limit reached, sleeping 14.7s
2025-11-09 09:34:05,948 - INFO - root - LLMClient: rate limit reached, sleeping 16.8s
2025-11-09 09:34:49,182 - INFO - root - LLMClient: rate limit reached, sleeping 6.0s
2025-11-09 09:35:12,373 - INFO - root - 正在提取论文图片...
2025-11-09 09:35:12,402 - INFO - root - 论文《Microscaling Floating Point Formats for Large Language Models》的分析已保存到 ./export\Microscaling Floating Point Formats for Large Language Models.md
2025-11-09 09:35:12,406 - INFO - root - 正在总结论文 9/30: Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework
2025-11-09 09:35:12,414 - INFO - root - LLMClient: rate limit reached, sleeping 10.4s
2025-11-09 09:35:31,867 - INFO - root - LLMClient: rate limit reached, sleeping 23.3s
2025-11-09 09:36:20,645 - INFO - root - LLMClient: rate limit reached, sleeping 2.1s
2025-11-09 09:36:42,880 - INFO - root - 正在提取论文图片...
2025-11-09 09:37:17,872 - INFO - root - 已保存图片 1/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_1_page8.jpeg
2025-11-09 09:37:18,126 - INFO - root - 已保存图片 2/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_2_page5.jpeg
2025-11-09 09:37:18,414 - INFO - root - 已保存图片 3/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_3_page9.jpeg
2025-11-09 09:37:18,633 - INFO - root - 已保存图片 4/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_4_page9.jpeg
2025-11-09 09:37:18,850 - INFO - root - 已保存图片 5/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_5_page9.jpeg
2025-11-09 09:37:19,059 - INFO - root - 已保存图片 6/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_6_page9.jpeg
2025-11-09 09:37:19,336 - INFO - root - 已保存图片 7/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_7_page9.jpeg
2025-11-09 09:37:19,590 - INFO - root - 已保存图片 8/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_8_page9.jpeg
2025-11-09 09:37:19,794 - INFO - root - 已保存图片 9/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_9_page9.jpeg
2025-11-09 09:37:20,022 - INFO - root - 已保存图片 10/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_10_page9.jpeg
2025-11-09 09:37:20,055 - INFO - root - 成功添加图片 1：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_1_page8.jpeg
2025-11-09 09:37:20,055 - INFO - root - 成功添加图片 2：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_2_page5.jpeg
2025-11-09 09:37:20,055 - INFO - root - 成功添加图片 3：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_3_page9.jpeg
2025-11-09 09:37:20,055 - INFO - root - 成功添加图片 4：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_4_page9.jpeg
2025-11-09 09:37:20,055 - INFO - root - 成功添加图片 5：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_5_page9.jpeg
2025-11-09 09:37:20,055 - INFO - root - 成功添加图片 6：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_6_page9.jpeg
2025-11-09 09:37:20,055 - INFO - root - 成功添加图片 7：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_7_page9.jpeg
2025-11-09 09:37:20,055 - INFO - root - 成功添加图片 8：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_8_page9.jpeg
2025-11-09 09:37:20,055 - INFO - root - 成功添加图片 9：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_9_page9.jpeg
2025-11-09 09:37:20,055 - INFO - root - 成功添加图片 10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_10_page9.jpeg
2025-11-09 09:37:20,063 - INFO - root - 论文《Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework》的分析已保存到 ./export\Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via.md
2025-11-09 09:37:20,067 - INFO - root - 正在总结论文 10/30: AMLA: MUL by ADD in FlashAttention Rescaling
2025-11-09 09:38:03,534 - INFO - root - LLMClient: rate limit reached, sleeping 16.5s
2025-11-09 09:38:39,999 - INFO - root - 正在提取论文图片...
2025-11-09 09:38:40,977 - INFO - root - 已保存图片 1/10：./export\images_AMLA_ MUL by ADD in FlashAttention Rescaling\figure_1_page15.png
2025-11-09 09:38:41,030 - INFO - root - 已保存图片 2/10：./export\images_AMLA_ MUL by ADD in FlashAttention Rescaling\figure_2_page4.jpeg
2025-11-09 09:38:41,059 - INFO - root - 已保存图片 3/10：./export\images_AMLA_ MUL by ADD in FlashAttention Rescaling\figure_3_page8.jpeg
2025-11-09 09:38:41,077 - INFO - root - 成功添加图片 1：./export\images_AMLA_ MUL by ADD in FlashAttention Rescaling\figure_1_page15.png
2025-11-09 09:38:41,077 - INFO - root - 成功添加图片 2：./export\images_AMLA_ MUL by ADD in FlashAttention Rescaling\figure_2_page4.jpeg
2025-11-09 09:38:41,078 - INFO - root - 成功添加图片 3：./export\images_AMLA_ MUL by ADD in FlashAttention Rescaling\figure_3_page8.jpeg
2025-11-09 09:38:41,082 - INFO - root - 论文《AMLA: MUL by ADD in FlashAttention Rescaling》的分析已保存到 ./export\AMLA_ MUL by ADD in FlashAttention Rescaling.md
2025-11-09 09:38:41,086 - INFO - root - 正在总结论文 11/30: Pretraining Large Language Models with NVFP4
2025-11-09 09:38:51,473 - INFO - root - LLMClient: rate limit reached, sleeping 28.6s
2025-11-09 09:40:00,161 - INFO - root - 正在提取论文图片...
2025-11-09 09:40:00,308 - INFO - root - 已保存图片 1/10：./export\images_Pretraining Large Language Models with NVFP4\figure_1_page7.png
2025-11-09 09:40:00,337 - INFO - root - 已保存图片 2/10：./export\images_Pretraining Large Language Models with NVFP4\figure_2_page7.jpeg
2025-11-09 09:40:00,356 - INFO - root - 已保存图片 3/10：./export\images_Pretraining Large Language Models with NVFP4\figure_3_page1.png
2025-11-09 09:40:00,378 - INFO - root - 已保存图片 4/10：./export\images_Pretraining Large Language Models with NVFP4\figure_4_page7.jpeg
2025-11-09 09:40:00,398 - INFO - root - 已保存图片 5/10：./export\images_Pretraining Large Language Models with NVFP4\figure_5_page7.jpeg
2025-11-09 09:40:00,422 - INFO - root - 已保存图片 6/10：./export\images_Pretraining Large Language Models with NVFP4\figure_6_page7.jpeg
2025-11-09 09:40:00,440 - INFO - root - 已保存图片 7/10：./export\images_Pretraining Large Language Models with NVFP4\figure_7_page7.png
2025-11-09 09:40:00,472 - INFO - root - 已保存图片 8/10：./export\images_Pretraining Large Language Models with NVFP4\figure_8_page7.jpeg
2025-11-09 09:40:00,487 - INFO - root - 已保存图片 9/10：./export\images_Pretraining Large Language Models with NVFP4\figure_9_page7.jpeg
2025-11-09 09:40:00,532 - INFO - root - 已保存图片 10/10：./export\images_Pretraining Large Language Models with NVFP4\figure_10_page7.jpeg
2025-11-09 09:40:00,535 - INFO - root - 成功添加图片 1：./export\images_Pretraining Large Language Models with NVFP4\figure_1_page7.png
2025-11-09 09:40:00,535 - INFO - root - 成功添加图片 2：./export\images_Pretraining Large Language Models with NVFP4\figure_2_page7.jpeg
2025-11-09 09:40:00,535 - INFO - root - 成功添加图片 3：./export\images_Pretraining Large Language Models with NVFP4\figure_3_page1.png
2025-11-09 09:40:00,535 - INFO - root - 成功添加图片 4：./export\images_Pretraining Large Language Models with NVFP4\figure_4_page7.jpeg
2025-11-09 09:40:00,535 - INFO - root - 成功添加图片 5：./export\images_Pretraining Large Language Models with NVFP4\figure_5_page7.jpeg
2025-11-09 09:40:00,535 - INFO - root - 成功添加图片 6：./export\images_Pretraining Large Language Models with NVFP4\figure_6_page7.jpeg
2025-11-09 09:40:00,535 - INFO - root - 成功添加图片 7：./export\images_Pretraining Large Language Models with NVFP4\figure_7_page7.png
2025-11-09 09:40:00,535 - INFO - root - 成功添加图片 8：./export\images_Pretraining Large Language Models with NVFP4\figure_8_page7.jpeg
2025-11-09 09:40:00,535 - INFO - root - 成功添加图片 9：./export\images_Pretraining Large Language Models with NVFP4\figure_9_page7.jpeg
2025-11-09 09:40:00,535 - INFO - root - 成功添加图片 10：./export\images_Pretraining Large Language Models with NVFP4\figure_10_page7.jpeg
2025-11-09 09:40:00,543 - INFO - root - 论文《Pretraining Large Language Models with NVFP4》的分析已保存到 ./export\Pretraining Large Language Models with NVFP4.md
2025-11-09 09:40:00,543 - INFO - root - 正在总结论文 12/30: Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization
2025-11-09 09:40:00,543 - INFO - root - LLMClient: rate limit reached, sleeping 19.5s
2025-11-09 09:40:28,694 - INFO - root - LLMClient: rate limit reached, sleeping 13.9s
2025-11-09 09:41:10,576 - INFO - root - LLMClient: rate limit reached, sleeping 9.5s
2025-11-09 09:41:40,505 - INFO - root - 正在提取论文图片...
2025-11-09 09:41:40,613 - INFO - root - 已保存图片 1/10：./export\images_Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantizati\figure_1_page25.png
2025-11-09 09:41:40,615 - INFO - root - 成功添加图片 1：./export\images_Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantizati\figure_1_page25.png
2025-11-09 09:41:40,619 - INFO - root - 论文《Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization》的分析已保存到 ./export\Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantizati.md
2025-11-09 09:41:40,631 - INFO - root - 正在总结论文 13/30: Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs
2025-11-09 09:41:40,635 - INFO - root - LLMClient: rate limit reached, sleeping 1.9s
2025-11-09 09:41:51,136 - INFO - root - LLMClient: rate limit reached, sleeping 28.9s
2025-11-09 09:43:15,225 - INFO - root - 正在提取论文图片...
2025-11-09 09:43:15,235 - INFO - root - 论文《Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs》的分析已保存到 ./export\Towards Verified Compilation of Floating-point Optimization in Scientific Comput.md
2025-11-09 09:43:15,242 - INFO - root - 正在总结论文 14/30: Green Learning for STAR-RIS mmWave Systems with Implicit CSI
2025-11-09 09:43:15,246 - INFO - root - LLMClient: rate limit reached, sleeping 4.8s
2025-11-09 09:43:29,726 - INFO - root - LLMClient: rate limit reached, sleeping 25.1s
2025-11-09 09:44:20,033 - INFO - root - LLMClient: rate limit reached, sleeping 0.0s
2025-11-09 09:44:39,745 - INFO - root - 正在提取论文图片...
2025-11-09 09:44:39,853 - INFO - root - 已保存图片 1/10：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_1_page2.jpeg
2025-11-09 09:44:39,931 - INFO - root - 已保存图片 2/10：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_2_page2.png
2025-11-09 09:44:39,987 - INFO - root - 已保存图片 3/10：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_3_page2.png
2025-11-09 09:44:40,042 - INFO - root - 已保存图片 4/10：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_4_page2.png
2025-11-09 09:44:40,044 - INFO - root - 成功添加图片 1：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_1_page2.jpeg
2025-11-09 09:44:40,044 - INFO - root - 成功添加图片 2：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_2_page2.png
2025-11-09 09:44:40,045 - INFO - root - 成功添加图片 3：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_3_page2.png
2025-11-09 09:44:40,045 - INFO - root - 成功添加图片 4：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_4_page2.png
2025-11-09 09:44:40,048 - INFO - root - 论文《Green Learning for STAR-RIS mmWave Systems with Implicit CSI》的分析已保存到 ./export\Green Learning for STAR-RIS mmWave Systems with Implicit CSI.md
2025-11-09 09:44:40,055 - INFO - root - 正在总结论文 15/30: A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN
2025-11-09 09:44:40,055 - INFO - root - LLMClient: rate limit reached, sleeping 14.7s
2025-11-09 09:45:05,784 - INFO - root - LLMClient: rate limit reached, sleeping 14.3s
2025-11-09 09:45:47,148 - INFO - root - LLMClient: rate limit reached, sleeping 7.6s
2025-11-09 09:46:13,321 - INFO - root - 正在提取论文图片...
2025-11-09 09:46:14,102 - INFO - root - 已保存图片 1/10：./export\images_A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted\figure_1_page6.png
2025-11-09 09:46:14,216 - INFO - root - 已保存图片 2/10：./export\images_A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted\figure_2_page6.png
2025-11-09 09:46:14,299 - INFO - root - 已保存图片 3/10：./export\images_A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted\figure_3_page2.png
2025-11-09 09:46:14,351 - INFO - root - 已保存图片 4/10：./export\images_A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted\figure_4_page2.png
2025-11-09 09:46:14,398 - INFO - root - 已保存图片 5/10：./export\images_A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted\figure_5_page3.png
2025-11-09 09:46:14,401 - INFO - root - 成功添加图片 1：./export\images_A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted\figure_1_page6.png
2025-11-09 09:46:14,401 - INFO - root - 成功添加图片 2：./export\images_A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted\figure_2_page6.png
2025-11-09 09:46:14,401 - INFO - root - 成功添加图片 3：./export\images_A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted\figure_3_page2.png
2025-11-09 09:46:14,401 - INFO - root - 成功添加图片 4：./export\images_A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted\figure_4_page2.png
2025-11-09 09:46:14,401 - INFO - root - 成功添加图片 5：./export\images_A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted\figure_5_page3.png
2025-11-09 09:46:14,401 - INFO - root - 论文《A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN》的分析已保存到 ./export\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.md
2025-11-09 09:46:14,413 - INFO - root - 正在总结论文 16/30: SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration
2025-11-09 09:46:14,414 - INFO - root - LLMClient: rate limit reached, sleeping 5.7s
2025-11-09 09:46:28,571 - INFO - root - LLMClient: rate limit reached, sleeping 26.2s
2025-11-09 09:47:42,531 - INFO - root - 正在提取论文图片...
2025-11-09 09:47:47,298 - INFO - root - 已保存图片 1/10：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_1_page8.jpeg
2025-11-09 09:47:47,342 - INFO - root - 已保存图片 2/10：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_2_page7.jpeg
2025-11-09 09:47:47,388 - INFO - root - 已保存图片 3/10：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_3_page7.jpeg
2025-11-09 09:47:47,418 - INFO - root - 已保存图片 4/10：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_4_page7.jpeg
2025-11-09 09:47:47,447 - INFO - root - 已保存图片 5/10：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_5_page7.jpeg
2025-11-09 09:47:47,576 - INFO - root - 已保存图片 6/10：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_6_page8.png
2025-11-09 09:47:47,711 - INFO - root - 已保存图片 7/10：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_7_page8.png
2025-11-09 09:47:47,837 - INFO - root - 已保存图片 8/10：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_8_page8.png
2025-11-09 09:47:47,970 - INFO - root - 已保存图片 9/10：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_9_page8.png
2025-11-09 09:47:48,087 - INFO - root - 已保存图片 10/10：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_10_page8.png
2025-11-09 09:47:48,104 - INFO - root - 成功添加图片 1：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_1_page8.jpeg
2025-11-09 09:47:48,104 - INFO - root - 成功添加图片 2：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_2_page7.jpeg
2025-11-09 09:47:48,104 - INFO - root - 成功添加图片 3：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_3_page7.jpeg
2025-11-09 09:47:48,104 - INFO - root - 成功添加图片 4：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_4_page7.jpeg
2025-11-09 09:47:48,104 - INFO - root - 成功添加图片 5：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_5_page7.jpeg
2025-11-09 09:47:48,104 - INFO - root - 成功添加图片 6：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_6_page8.png
2025-11-09 09:47:48,104 - INFO - root - 成功添加图片 7：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_7_page8.png
2025-11-09 09:47:48,104 - INFO - root - 成功添加图片 8：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_8_page8.png
2025-11-09 09:47:48,104 - INFO - root - 成功添加图片 9：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_9_page8.png
2025-11-09 09:47:48,104 - INFO - root - 成功添加图片 10：./export\images_SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration\figure_10_page8.png
2025-11-09 09:47:48,112 - INFO - root - 论文《SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration》的分析已保存到 ./export\SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration.md
2025-11-09 09:47:48,121 - INFO - root - 正在总结论文 17/30: SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration
2025-11-09 09:47:48,121 - INFO - root - LLMClient: rate limit reached, sleeping 6.7s
2025-11-09 09:48:05,407 - INFO - root - LLMClient: rate limit reached, sleeping 17.7s
2025-11-09 09:48:48,016 - INFO - root - LLMClient: rate limit reached, sleeping 6.8s
2025-11-09 09:49:14,621 - INFO - root - 正在提取论文图片...
2025-11-09 09:49:15,026 - INFO - root - 已保存图片 1/10：./export\images_SpikeSTAG_ Spatial-Temporal Forecasting via GNN-SNN Collaboration\figure_1_page4.png
2025-11-09 09:49:15,087 - INFO - root - 已保存图片 2/10：./export\images_SpikeSTAG_ Spatial-Temporal Forecasting via GNN-SNN Collaboration\figure_2_page2.png
2025-11-09 09:49:15,163 - INFO - root - 已保存图片 3/10：./export\images_SpikeSTAG_ Spatial-Temporal Forecasting via GNN-SNN Collaboration\figure_3_page7.png
2025-11-09 09:49:15,233 - INFO - root - 已保存图片 4/10：./export\images_SpikeSTAG_ Spatial-Temporal Forecasting via GNN-SNN Collaboration\figure_4_page4.png
2025-11-09 09:49:15,237 - INFO - root - 成功添加图片 1：./export\images_SpikeSTAG_ Spatial-Temporal Forecasting via GNN-SNN Collaboration\figure_1_page4.png
2025-11-09 09:49:15,237 - INFO - root - 成功添加图片 2：./export\images_SpikeSTAG_ Spatial-Temporal Forecasting via GNN-SNN Collaboration\figure_2_page2.png
2025-11-09 09:49:15,237 - INFO - root - 成功添加图片 3：./export\images_SpikeSTAG_ Spatial-Temporal Forecasting via GNN-SNN Collaboration\figure_3_page7.png
2025-11-09 09:49:15,237 - INFO - root - 成功添加图片 4：./export\images_SpikeSTAG_ Spatial-Temporal Forecasting via GNN-SNN Collaboration\figure_4_page4.png
2025-11-09 09:49:15,243 - INFO - root - 论文《SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration》的分析已保存到 ./export\SpikeSTAG_ Spatial-Temporal Forecasting via GNN-SNN Collaboration.md
2025-11-09 09:49:15,254 - INFO - root - 正在总结论文 18/30: DGEMM without FP64 Arithmetic - Using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme
2025-11-09 09:49:15,257 - INFO - root - LLMClient: rate limit reached, sleeping 7.8s
2025-11-09 09:49:32,140 - INFO - root - LLMClient: rate limit reached, sleeping 22.6s
2025-11-09 09:50:22,400 - INFO - root - LLMClient: rate limit reached, sleeping 0.7s
2025-11-09 09:50:42,963 - INFO - root - 正在提取论文图片...
2025-11-09 09:50:42,976 - INFO - root - 论文《DGEMM without FP64 Arithmetic - Using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme》的分析已保存到 ./export\DGEMM without FP64 Arithmetic - Using FP64 Emulation and FP8 Tensor Cores with O.md
2025-11-09 09:50:42,979 - INFO - root - 正在总结论文 19/30: Scaling Probabilistic Circuits via Monarch Matrices
2025-11-09 09:50:42,985 - INFO - root - LLMClient: rate limit reached, sleeping 11.8s
2025-11-09 09:51:03,291 - INFO - root - LLMClient: rate limit reached, sleeping 19.8s
2025-11-09 09:51:48,985 - INFO - root - LLMClient: rate limit reached, sleeping 5.8s
2025-11-09 09:52:13,915 - INFO - root - 正在提取论文图片...
2025-11-09 09:52:14,011 - INFO - root - 已保存图片 1/10：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_1_page5.png
2025-11-09 09:52:14,075 - INFO - root - 已保存图片 2/10：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_2_page5.png
2025-11-09 09:52:14,132 - INFO - root - 已保存图片 3/10：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_3_page5.png
2025-11-09 09:52:14,193 - INFO - root - 已保存图片 4/10：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_4_page5.png
2025-11-09 09:52:14,253 - INFO - root - 已保存图片 5/10：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_5_page13.png
2025-11-09 09:52:14,319 - INFO - root - 已保存图片 6/10：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_6_page13.png
2025-11-09 09:52:14,379 - INFO - root - 已保存图片 7/10：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_7_page13.png
2025-11-09 09:52:14,429 - INFO - root - 已保存图片 8/10：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_8_page13.png
2025-11-09 09:52:14,429 - INFO - root - 成功添加图片 1：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_1_page5.png
2025-11-09 09:52:14,429 - INFO - root - 成功添加图片 2：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_2_page5.png
2025-11-09 09:52:14,429 - INFO - root - 成功添加图片 3：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_3_page5.png
2025-11-09 09:52:14,429 - INFO - root - 成功添加图片 4：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_4_page5.png
2025-11-09 09:52:14,442 - INFO - root - 成功添加图片 5：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_5_page13.png
2025-11-09 09:52:14,442 - INFO - root - 成功添加图片 6：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_6_page13.png
2025-11-09 09:52:14,442 - INFO - root - 成功添加图片 7：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_7_page13.png
2025-11-09 09:52:14,442 - INFO - root - 成功添加图片 8：./export\images_Scaling Probabilistic Circuits via Monarch Matrices\figure_8_page13.png
2025-11-09 09:52:14,447 - INFO - root - 论文《Scaling Probabilistic Circuits via Monarch Matrices》的分析已保存到 ./export\Scaling Probabilistic Circuits via Monarch Matrices.md
2025-11-09 09:52:14,458 - INFO - root - 正在总结论文 20/30: Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization
2025-11-09 09:52:14,461 - INFO - root - LLMClient: rate limit reached, sleeping 8.6s
2025-11-09 09:52:32,733 - INFO - root - LLMClient: rate limit reached, sleeping 22.1s
2025-11-09 09:53:43,563 - INFO - root - 正在提取论文图片...
2025-11-09 09:53:44,074 - INFO - root - 已保存图片 1/10：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_1_page10.png
2025-11-09 09:53:44,111 - INFO - root - 已保存图片 2/10：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_2_page5.png
2025-11-09 09:53:44,215 - INFO - root - 已保存图片 3/10：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_3_page7.png
2025-11-09 09:53:44,295 - INFO - root - 已保存图片 4/10：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_4_page7.png
2025-11-09 09:53:44,362 - INFO - root - 已保存图片 5/10：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_5_page7.png
2025-11-09 09:53:44,433 - INFO - root - 已保存图片 6/10：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_6_page7.png
2025-11-09 09:53:44,500 - INFO - root - 已保存图片 7/10：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_7_page9.png
2025-11-09 09:53:44,567 - INFO - root - 已保存图片 8/10：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_8_page9.png
2025-11-09 09:53:44,634 - INFO - root - 已保存图片 9/10：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_9_page8.png
2025-11-09 09:53:44,701 - INFO - root - 已保存图片 10/10：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_10_page8.png
2025-11-09 09:53:44,703 - INFO - root - 成功添加图片 1：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_1_page10.png
2025-11-09 09:53:44,703 - INFO - root - 成功添加图片 2：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_2_page5.png
2025-11-09 09:53:44,703 - INFO - root - 成功添加图片 3：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_3_page7.png
2025-11-09 09:53:44,703 - INFO - root - 成功添加图片 4：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_4_page7.png
2025-11-09 09:53:44,703 - INFO - root - 成功添加图片 5：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_5_page7.png
2025-11-09 09:53:44,703 - INFO - root - 成功添加图片 6：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_6_page7.png
2025-11-09 09:53:44,703 - INFO - root - 成功添加图片 7：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_7_page9.png
2025-11-09 09:53:44,703 - INFO - root - 成功添加图片 8：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_8_page9.png
2025-11-09 09:53:44,703 - INFO - root - 成功添加图片 9：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_9_page8.png
2025-11-09 09:53:44,703 - INFO - root - 成功添加图片 10：./export\images_Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne\figure_10_page8.png
2025-11-09 09:53:44,710 - INFO - root - 论文《Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization》的分析已保存到 ./export\Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne.md
2025-11-09 09:53:44,717 - INFO - root - 正在总结论文 21/30: Recipes for Pre-training LLMs with MXFP8
2025-11-09 09:53:44,719 - INFO - root - LLMClient: rate limit reached, sleeping 10.1s
2025-11-09 09:54:05,661 - INFO - root - LLMClient: rate limit reached, sleeping 18.6s
2025-11-09 09:54:50,756 - INFO - root - LLMClient: rate limit reached, sleeping 4.0s
2025-11-09 09:55:13,138 - INFO - root - 正在提取论文图片...
2025-11-09 09:55:13,159 - INFO - root - 论文《Recipes for Pre-training LLMs with MXFP8》的分析已保存到 ./export\Recipes for Pre-training LLMs with MXFP8.md
2025-11-09 09:55:13,166 - INFO - root - 正在总结论文 22/30: FP4 All the Way: Fully Quantized Training of LLMs
2025-11-09 09:55:13,166 - INFO - root - LLMClient: rate limit reached, sleeping 11.1s
2025-11-09 09:55:32,581 - INFO - root - LLMClient: rate limit reached, sleeping 22.2s
2025-11-09 09:56:22,886 - INFO - root - LLMClient: rate limit reached, sleeping 1.3s
2025-11-09 09:56:41,959 - INFO - root - 正在提取论文图片...
2025-11-09 09:56:41,981 - INFO - root - 论文《FP4 All the Way: Fully Quantized Training of LLMs》的分析已保存到 ./export\FP4 All the Way_ Fully Quantized Training of LLMs.md
2025-11-09 09:56:41,981 - INFO - root - 正在总结论文 23/30: Automatic Verification of Floating-Point Accumulation Networks
2025-11-09 09:56:41,981 - INFO - root - LLMClient: rate limit reached, sleeping 12.8s
2025-11-09 09:57:05,462 - INFO - root - LLMClient: rate limit reached, sleeping 18.8s
2025-11-09 09:57:52,739 - INFO - root - LLMClient: rate limit reached, sleeping 2.1s
2025-11-09 09:58:13,141 - INFO - root - 正在提取论文图片...
2025-11-09 09:58:13,169 - INFO - root - 论文《Automatic Verification of Floating-Point Accumulation Networks》的分析已保存到 ./export\Automatic Verification of Floating-Point Accumulation Networks.md
2025-11-09 09:58:13,174 - INFO - root - 正在总结论文 24/30: MXDOTP: A RISC-V ISA Extension for Enabling Microscaling (MX) Floating-Point Dot Products
2025-11-09 09:58:13,176 - INFO - root - LLMClient: rate limit reached, sleeping 11.0s
2025-11-09 09:58:33,881 - INFO - root - LLMClient: rate limit reached, sleeping 20.9s
2025-11-09 09:59:21,837 - INFO - root - LLMClient: rate limit reached, sleeping 2.4s
2025-11-09 09:59:44,166 - INFO - root - 正在提取论文图片...
2025-11-09 09:59:44,188 - INFO - root - 论文《MXDOTP: A RISC-V ISA Extension for Enabling Microscaling (MX) Floating-Point Dot Products》的分析已保存到 ./export\MXDOTP_ A RISC-V ISA Extension for Enabling Microscaling (MX) Floating-Point Dot.md
2025-11-09 09:59:44,195 - INFO - root - 正在总结论文 25/30: Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors
2025-11-09 09:59:44,197 - INFO - root - LLMClient: rate limit reached, sleeping 10.6s
2025-11-09 10:00:04,187 - INFO - root - LLMClient: rate limit reached, sleeping 20.1s
2025-11-09 10:01:54,954 - INFO - root - 正在提取论文图片...
2025-11-09 10:01:54,964 - INFO - root - 论文《Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors》的分析已保存到 ./export\Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors.md
2025-11-09 10:01:54,969 - INFO - root - 正在总结论文 26/30: Silenzio: Secure Non-Interactive Outsourced MLP Training
2025-11-09 10:02:07,552 - INFO - root - LLMClient: rate limit reached, sleeping 29.5s
2025-11-09 10:03:31,905 - INFO - root - 正在提取论文图片...
2025-11-09 10:03:32,162 - INFO - root - 已保存图片 1/10：./export\images_Silenzio_ Secure Non-Interactive Outsourced MLP Training\figure_1_page11.png
2025-11-09 10:03:32,162 - INFO - root - 成功添加图片 1：./export\images_Silenzio_ Secure Non-Interactive Outsourced MLP Training\figure_1_page11.png
2025-11-09 10:03:32,167 - INFO - root - 论文《Silenzio: Secure Non-Interactive Outsourced MLP Training》的分析已保存到 ./export\Silenzio_ Secure Non-Interactive Outsourced MLP Training.md
2025-11-09 10:03:32,175 - INFO - root - 正在总结论文 27/30: Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution
2025-11-09 10:03:32,175 - INFO - root - LLMClient: rate limit reached, sleeping 4.8s
2025-11-09 10:03:48,557 - INFO - root - LLMClient: rate limit reached, sleeping 22.9s
2025-11-09 10:04:58,538 - INFO - root - 正在提取论文图片...
2025-11-09 10:04:58,608 - INFO - root - 已保存图片 1/10：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_1_page4.jpeg
2025-11-09 10:04:58,624 - INFO - root - 已保存图片 2/10：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_2_page3.jpeg
2025-11-09 10:04:58,688 - INFO - root - 已保存图片 3/10：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_3_page1.png
2025-11-09 10:04:58,764 - INFO - root - 已保存图片 4/10：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_4_page5.png
2025-11-09 10:04:58,797 - INFO - root - 已保存图片 5/10：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_5_page7.jpeg
2025-11-09 10:04:58,842 - INFO - root - 已保存图片 6/10：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_6_page7.jpeg
2025-11-09 10:04:58,876 - INFO - root - 已保存图片 7/10：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_7_page7.jpeg
2025-11-09 10:04:58,912 - INFO - root - 已保存图片 8/10：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_8_page7.jpeg
2025-11-09 10:04:58,940 - INFO - root - 已保存图片 9/10：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_9_page7.jpeg
2025-11-09 10:04:58,977 - INFO - root - 已保存图片 10/10：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_10_page7.jpeg
2025-11-09 10:04:58,983 - INFO - root - 成功添加图片 1：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_1_page4.jpeg
2025-11-09 10:04:58,984 - INFO - root - 成功添加图片 2：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_2_page3.jpeg
2025-11-09 10:04:58,984 - INFO - root - 成功添加图片 3：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_3_page1.png
2025-11-09 10:04:58,985 - INFO - root - 成功添加图片 4：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_4_page5.png
2025-11-09 10:04:58,985 - INFO - root - 成功添加图片 5：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_5_page7.jpeg
2025-11-09 10:04:58,985 - INFO - root - 成功添加图片 6：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_6_page7.jpeg
2025-11-09 10:04:58,986 - INFO - root - 成功添加图片 7：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_7_page7.jpeg
2025-11-09 10:04:58,986 - INFO - root - 成功添加图片 8：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_8_page7.jpeg
2025-11-09 10:04:58,987 - INFO - root - 成功添加图片 9：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_9_page7.jpeg
2025-11-09 10:04:58,987 - INFO - root - 成功添加图片 10：./export\images_Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup\figure_10_page7.jpeg
2025-11-09 10:04:58,994 - INFO - root - 论文《Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution》的分析已保存到 ./export\Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup.md
2025-11-09 10:04:59,001 - INFO - root - 正在总结论文 28/30: Flopping for FLOPs: Leveraging equivariance for computational efficiency
2025-11-09 10:04:59,002 - INFO - root - LLMClient: rate limit reached, sleeping 12.5s
2025-11-09 10:05:22,017 - INFO - root - LLMClient: rate limit reached, sleeping 18.0s
2025-11-09 10:06:03,248 - INFO - root - LLMClient: rate limit reached, sleeping 8.2s
2025-11-09 10:06:27,546 - INFO - root - 正在提取论文图片...
2025-11-09 10:06:27,990 - INFO - root - 已保存图片 1/10：./export\images_Flopping for FLOPs_ Leveraging equivariance for computational efficiency\figure_1_page3.png
2025-11-09 10:06:28,075 - INFO - root - 已保存图片 2/10：./export\images_Flopping for FLOPs_ Leveraging equivariance for computational efficiency\figure_2_page5.png
2025-11-09 10:06:28,170 - INFO - root - 已保存图片 3/10：./export\images_Flopping for FLOPs_ Leveraging equivariance for computational efficiency\figure_3_page1.png
2025-11-09 10:06:28,170 - INFO - root - 成功添加图片 1：./export\images_Flopping for FLOPs_ Leveraging equivariance for computational efficiency\figure_1_page3.png
2025-11-09 10:06:28,170 - INFO - root - 成功添加图片 2：./export\images_Flopping for FLOPs_ Leveraging equivariance for computational efficiency\figure_2_page5.png
2025-11-09 10:06:28,170 - INFO - root - 成功添加图片 3：./export\images_Flopping for FLOPs_ Leveraging equivariance for computational efficiency\figure_3_page1.png
2025-11-09 10:06:28,185 - INFO - root - 论文《Flopping for FLOPs: Leveraging equivariance for computational efficiency》的分析已保存到 ./export\Flopping for FLOPs_ Leveraging equivariance for computational efficiency.md
2025-11-09 10:06:28,191 - INFO - root - 正在总结论文 29/30: An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in Medical Image
2025-11-09 10:06:28,191 - INFO - root - LLMClient: rate limit reached, sleeping 11.8s
2025-11-09 10:06:49,350 - INFO - root - LLMClient: rate limit reached, sleeping 22.1s
2025-11-09 10:07:35,800 - INFO - root - LLMClient: rate limit reached, sleeping 4.2s
2025-11-09 10:07:58,300 - INFO - root - 正在提取论文图片...
2025-11-09 10:07:58,489 - INFO - root - 已保存图片 1/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_1_page14.png
2025-11-09 10:07:58,531 - INFO - root - 已保存图片 2/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_2_page19.jpeg
2025-11-09 10:07:58,577 - INFO - root - 已保存图片 3/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_3_page19.jpeg
2025-11-09 10:07:58,672 - INFO - root - 已保存图片 4/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_4_page19.jpeg
2025-11-09 10:07:58,764 - INFO - root - 已保存图片 5/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_5_page19.jpeg
2025-11-09 10:07:58,822 - INFO - root - 已保存图片 6/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_6_page19.jpeg
2025-11-09 10:07:58,889 - INFO - root - 已保存图片 7/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_7_page19.jpeg
2025-11-09 10:07:58,992 - INFO - root - 已保存图片 8/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_8_page19.jpeg
2025-11-09 10:07:59,073 - INFO - root - 已保存图片 9/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_9_page19.jpeg
2025-11-09 10:07:59,146 - INFO - root - 已保存图片 10/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_10_page19.jpeg
2025-11-09 10:07:59,241 - INFO - root - 成功添加图片 1：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_1_page14.png
2025-11-09 10:07:59,253 - INFO - root - 成功添加图片 2：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_2_page19.jpeg
2025-11-09 10:07:59,255 - INFO - root - 成功添加图片 3：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_3_page19.jpeg
2025-11-09 10:07:59,258 - INFO - root - 成功添加图片 4：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_4_page19.jpeg
2025-11-09 10:07:59,263 - INFO - root - 成功添加图片 5：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_5_page19.jpeg
2025-11-09 10:07:59,285 - INFO - root - 成功添加图片 6：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_6_page19.jpeg
2025-11-09 10:07:59,297 - INFO - root - 成功添加图片 7：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_7_page19.jpeg
2025-11-09 10:07:59,321 - INFO - root - 成功添加图片 8：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_8_page19.jpeg
2025-11-09 10:07:59,321 - INFO - root - 成功添加图片 9：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_9_page19.jpeg
2025-11-09 10:07:59,323 - INFO - root - 成功添加图片 10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in\figure_10_page19.jpeg
2025-11-09 10:07:59,333 - INFO - root - 论文《An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in Medical Image》的分析已保存到 ./export\An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in.md
2025-11-09 10:07:59,343 - INFO - root - 正在总结论文 30/30: TwinLiteNet+: An Enhanced Multi-Task Segmentation Model for Autonomous Driving
2025-11-09 10:07:59,345 - INFO - root - LLMClient: rate limit reached, sleeping 12.1s
2025-11-09 10:08:22,814 - INFO - root - LLMClient: rate limit reached, sleeping 17.2s
2025-11-09 10:09:04,202 - INFO - root - LLMClient: rate limit reached, sleeping 7.3s
2025-11-09 10:09:11,822 - ERROR - root - LLMClient: generation error: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250
Please retry in 47.09277372s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-flash"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 250
}
, retry_delay {
  seconds: 47
}
]
Traceback (most recent call last):
  File "D:\ChatPaper\llm_client.py", line 117, in generate
    resp = self.model.generate_content(prompt, safety_settings=self.safety_settings)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\generativeai\generative_models.py", line 331, in generate_content
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 835, in generate_content
    response = rpc(
               ^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
                 ^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
             ^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\grpc_helpers.py", line 77, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250
Please retry in 47.09277372s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-flash"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 250
}
, retry_delay {
  seconds: 47
}
]
2025-11-09 10:09:11,825 - WARNING - root - LLMClient: quota/rate detected, sleeping 60 s then retrying
2025-11-09 10:10:11,826 - INFO - root - LLMClient: retry attempt 2 for generation
2025-11-09 10:10:12,768 - ERROR - root - LLMClient: generation error: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250
Please retry in 46.154652662s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-flash"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 250
}
, retry_delay {
  seconds: 46
}
]
Traceback (most recent call last):
  File "D:\ChatPaper\llm_client.py", line 117, in generate
    resp = self.model.generate_content(prompt, safety_settings=self.safety_settings)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\generativeai\generative_models.py", line 331, in generate_content
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 835, in generate_content
    response = rpc(
               ^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
                 ^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
             ^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\grpc_helpers.py", line 77, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250
Please retry in 46.154652662s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-flash"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 250
}
, retry_delay {
  seconds: 46
}
]
2025-11-09 10:10:12,770 - WARNING - root - LLMClient: quota/rate detected, sleeping 60 s then retrying
2025-11-09 10:11:12,771 - INFO - root - LLMClient: retry attempt 3 for generation
2025-11-09 10:11:14,264 - ERROR - root - LLMClient: generation error: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250
Please retry in 44.666304166s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-flash"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 250
}
, retry_delay {
  seconds: 44
}
]
Traceback (most recent call last):
  File "D:\ChatPaper\llm_client.py", line 117, in generate
    resp = self.model.generate_content(prompt, safety_settings=self.safety_settings)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\generativeai\generative_models.py", line 331, in generate_content
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 835, in generate_content
    response = rpc(
               ^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
                 ^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
             ^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\grpc_helpers.py", line 77, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250
Please retry in 44.666304166s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-flash"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 250
}
, retry_delay {
  seconds: 44
}
]
2025-11-09 10:11:14,267 - INFO - root - 正在提取论文图片...
2025-11-09 10:11:14,617 - INFO - root - 已保存图片 1/10：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_1_page23.jpeg
2025-11-09 10:11:14,653 - INFO - root - 已保存图片 2/10：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_2_page6.jpeg
2025-11-09 10:11:14,696 - INFO - root - 已保存图片 3/10：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_3_page6.png
2025-11-09 10:11:14,750 - INFO - root - 已保存图片 4/10：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_4_page6.png
2025-11-09 10:11:14,794 - INFO - root - 已保存图片 5/10：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_5_page8.jpeg
2025-11-09 10:11:14,835 - INFO - root - 已保存图片 6/10：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_6_page17.jpeg
2025-11-09 10:11:14,883 - INFO - root - 已保存图片 7/10：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_7_page17.jpeg
2025-11-09 10:11:14,919 - INFO - root - 已保存图片 8/10：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_8_page17.jpeg
2025-11-09 10:11:14,961 - INFO - root - 已保存图片 9/10：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_9_page17.jpeg
2025-11-09 10:11:15,005 - INFO - root - 已保存图片 10/10：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_10_page17.jpeg
2025-11-09 10:11:15,011 - INFO - root - 成功添加图片 1：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_1_page23.jpeg
2025-11-09 10:11:15,011 - INFO - root - 成功添加图片 2：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_2_page6.jpeg
2025-11-09 10:11:15,011 - INFO - root - 成功添加图片 3：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_3_page6.png
2025-11-09 10:11:15,011 - INFO - root - 成功添加图片 4：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_4_page6.png
2025-11-09 10:11:15,011 - INFO - root - 成功添加图片 5：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_5_page8.jpeg
2025-11-09 10:11:15,011 - INFO - root - 成功添加图片 6：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_6_page17.jpeg
2025-11-09 10:11:15,017 - INFO - root - 成功添加图片 7：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_7_page17.jpeg
2025-11-09 10:11:15,017 - INFO - root - 成功添加图片 8：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_8_page17.jpeg
2025-11-09 10:11:15,017 - INFO - root - 成功添加图片 9：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_9_page17.jpeg
2025-11-09 10:11:15,017 - INFO - root - 成功添加图片 10：./export\images_TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving\figure_10_page17.jpeg
2025-11-09 10:11:15,028 - INFO - root - 论文《TwinLiteNet+: An Enhanced Multi-Task Segmentation Model for Autonomous Driving》的分析已保存到 ./export\TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving.md
2025-11-09 10:11:15,034 - INFO - root - summary time: 31076.43 seconds
2025-11-09 10:23:21,960 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 10:23:21,960 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 10:23:21,960 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 10:23:22,718 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 10:23:23,714 - INFO - root - LLMClient: trying model gemini-2.5-pro
2025-11-09 10:23:34,497 - INFO - root - LLMClient: initialized model gemini-2.5-pro
2025-11-09 10:23:34,497 - INFO - root - 使用 LLM 模型: gemini-2.5-pro
2025-11-09 10:23:34,498 - INFO - root - === 运行配置 ===
2025-11-09 10:23:34,498 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 10:23:34,499 - INFO - root - 关键词: Quant
2025-11-09 10:23:34,499 - INFO - root - 查询: block float point
2025-11-09 10:23:34,499 - INFO - root - 排序: None
2025-11-09 10:23:34,500 - INFO - root - 最近天数: 180
2025-11-09 10:23:34,500 - INFO - root - 最大处理数量: 50
2025-11-09 10:23:34,500 - INFO - root - 保存图片: 是
2025-11-09 10:23:34,501 - INFO - root - 输出语言: 中文
2025-11-09 10:23:34,501 - INFO - root - 强制重新处理: 否
2025-11-09 10:23:34,501 - INFO - root - ====================
2025-11-09 10:23:34,501 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 10:24:22,578 - INFO - root - 正在总结论文 1/30: From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators
2025-11-09 10:24:22,751 - ERROR - root - LLMClient: generation error: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50
Please retry in 35.947403049s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 50
}
, retry_delay {
  seconds: 35
}
]
Traceback (most recent call last):
  File "D:\ChatPaper\llm_client.py", line 117, in generate
    resp = self.model.generate_content(prompt, safety_settings=self.safety_settings)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\generativeai\generative_models.py", line 331, in generate_content
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 835, in generate_content
    response = rpc(
               ^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
                 ^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
             ^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\grpc_helpers.py", line 77, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50
Please retry in 35.947403049s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 50
}
, retry_delay {
  seconds: 35
}
]
2025-11-09 10:24:22,758 - WARNING - root - LLMClient: quota/rate detected, sleeping 60 s then retrying
2025-11-09 10:25:22,762 - INFO - root - LLMClient: retry attempt 2 for generation
2025-11-09 10:25:23,508 - ERROR - root - LLMClient: generation error: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50
Please retry in 35.205752668s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 50
}
, retry_delay {
  seconds: 35
}
]
Traceback (most recent call last):
  File "D:\ChatPaper\llm_client.py", line 117, in generate
    resp = self.model.generate_content(prompt, safety_settings=self.safety_settings)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\generativeai\generative_models.py", line 331, in generate_content
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 835, in generate_content
    response = rpc(
               ^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
                 ^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
             ^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\grpc_helpers.py", line 77, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50
Please retry in 35.205752668s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 50
}
, retry_delay {
  seconds: 35
}
]
2025-11-09 10:25:23,511 - WARNING - root - LLMClient: quota/rate detected, sleeping 60 s then retrying
2025-11-09 10:26:23,511 - INFO - root - LLMClient: retry attempt 3 for generation
2025-11-09 10:26:24,694 - ERROR - root - LLMClient: generation error: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50
Please retry in 34.01837253s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 50
}
, retry_delay {
  seconds: 34
}
]
Traceback (most recent call last):
  File "D:\ChatPaper\llm_client.py", line 117, in generate
    resp = self.model.generate_content(prompt, safety_settings=self.safety_settings)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\generativeai\generative_models.py", line 331, in generate_content
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 835, in generate_content
    response = rpc(
               ^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
                 ^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
             ^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\grpc_helpers.py", line 77, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50
Please retry in 34.01837253s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 50
}
, retry_delay {
  seconds: 34
}
]
2025-11-09 10:26:24,828 - ERROR - root - LLMClient: generation error: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50
Please retry in 33.879301367s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 50
}
, retry_delay {
  seconds: 33
}
]
Traceback (most recent call last):
  File "D:\ChatPaper\llm_client.py", line 117, in generate
    resp = self.model.generate_content(prompt, safety_settings=self.safety_settings)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\generativeai\generative_models.py", line 331, in generate_content
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 835, in generate_content
    response = rpc(
               ^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
                 ^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
             ^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\grpc_helpers.py", line 77, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50
Please retry in 33.879301367s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 50
}
, retry_delay {
  seconds: 33
}
]
2025-11-09 10:26:24,830 - WARNING - root - LLMClient: quota/rate detected, sleeping 60 s then retrying
2025-11-09 10:27:24,831 - INFO - root - LLMClient: retry attempt 2 for generation
2025-11-09 10:27:25,705 - ERROR - root - LLMClient: generation error: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50
Please retry in 32.998956656s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 50
}
, retry_delay {
  seconds: 32
}
]
Traceback (most recent call last):
  File "D:\ChatPaper\llm_client.py", line 117, in generate
    resp = self.model.generate_content(prompt, safety_settings=self.safety_settings)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\generativeai\generative_models.py", line 331, in generate_content
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 835, in generate_content
    response = rpc(
               ^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
                 ^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
             ^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\grpc_helpers.py", line 77, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50
Please retry in 32.998956656s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 50
}
, retry_delay {
  seconds: 32
}
]
2025-11-09 10:27:25,707 - WARNING - root - LLMClient: quota/rate detected, sleeping 60 s then retrying
2025-11-09 10:28:25,708 - INFO - root - LLMClient: retry attempt 3 for generation
2025-11-09 10:28:26,444 - ERROR - root - LLMClient: generation error: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50
Please retry in 32.258710193s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 50
}
, retry_delay {
  seconds: 32
}
]
Traceback (most recent call last):
  File "D:\ChatPaper\llm_client.py", line 117, in generate
    resp = self.model.generate_content(prompt, safety_settings=self.safety_settings)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\generativeai\generative_models.py", line 331, in generate_content
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 835, in generate_content
    response = rpc(
               ^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
                 ^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
             ^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\grpc_helpers.py", line 77, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50
Please retry in 32.258710193s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 50
}
, retry_delay {
  seconds: 32
}
]
2025-11-09 10:28:26,558 - ERROR - root - LLMClient: generation error: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50
Please retry in 32.138240581s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 50
}
, retry_delay {
  seconds: 32
}
]
Traceback (most recent call last):
  File "D:\ChatPaper\llm_client.py", line 117, in generate
    resp = self.model.generate_content(prompt, safety_settings=self.safety_settings)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\generativeai\generative_models.py", line 331, in generate_content
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 835, in generate_content
    response = rpc(
               ^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
                 ^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
             ^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\google\api_core\grpc_helpers.py", line 77, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50
Please retry in 32.138240581s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 50
}
, retry_delay {
  seconds: 32
}
]
2025-11-09 10:28:26,561 - WARNING - root - LLMClient: quota/rate detected, sleeping 60 s then retrying
2025-11-09 10:34:14,529 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 10:34:14,529 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 10:34:14,529 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 10:34:15,175 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 10:34:16,017 - INFO - root - LLMClient: trying model gemini-2.5-pro
2025-11-09 10:34:16,375 - WARNING - root - LLMClient: no usable model found, LLM disabled
2025-11-09 10:34:16,375 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 10:34:16,375 - INFO - root - === 运行配置 ===
2025-11-09 10:34:16,376 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 10:34:16,376 - INFO - root - 关键词: Quant
2025-11-09 10:34:16,376 - INFO - root - 查询: block float point
2025-11-09 10:34:16,376 - INFO - root - 排序: None
2025-11-09 10:34:16,377 - INFO - root - 最近天数: 180
2025-11-09 10:34:16,377 - INFO - root - 最大处理数量: 50
2025-11-09 10:34:16,377 - INFO - root - 保存图片: 是
2025-11-09 10:34:16,378 - INFO - root - 输出语言: 中文
2025-11-09 10:34:16,378 - INFO - root - 强制重新处理: 否
2025-11-09 10:34:16,379 - INFO - root - ====================
2025-11-09 10:34:16,379 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 10:34:16,380 - ERROR - root - 从 chat_arxiv 获取论文列表失败: name 'get_all_titles_from_web' is not defined
2025-11-09 10:34:16,384 - INFO - root - 没有找到要处理的论文，程序退出
2025-11-09 10:34:16,384 - INFO - root - summary time: 1.85 seconds
2025-11-09 10:40:38,168 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 10:40:38,176 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 10:40:38,176 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 10:40:38,993 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 10:40:39,838 - INFO - root - LLMClient: trying model gemini-2.5-pro
2025-11-09 10:40:40,189 - WARNING - root - LLMClient: no usable model found, LLM disabled
2025-11-09 10:40:40,190 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 10:40:40,190 - INFO - root - === 运行配置 ===
2025-11-09 10:40:40,190 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 10:40:40,190 - INFO - root - 关键词: Quant
2025-11-09 10:40:40,191 - INFO - root - 查询: block float point
2025-11-09 10:40:40,191 - INFO - root - 排序: None
2025-11-09 10:40:40,191 - INFO - root - 最近天数: 180
2025-11-09 10:40:40,192 - INFO - root - 最大处理数量: 50
2025-11-09 10:40:40,192 - INFO - root - 保存图片: 是
2025-11-09 10:40:40,192 - INFO - root - 输出语言: 中文
2025-11-09 10:40:40,193 - INFO - root - 强制重新处理: 否
2025-11-09 10:40:40,193 - INFO - root - ====================
2025-11-09 10:40:40,193 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 10:40:40,193 - ERROR - root - 从 chat_arxiv 获取论文列表失败: get_all_titles_from_web() missing 1 required positional argument: 'keyword'
2025-11-09 10:40:40,195 - INFO - root - 没有找到要处理的论文，程序退出
2025-11-09 10:40:40,195 - INFO - root - summary time: 2.03 seconds
2025-11-09 10:43:10,381 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 10:43:10,381 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 10:43:10,381 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 10:43:10,983 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 10:43:11,896 - INFO - root - LLMClient: trying model gemini-2.5-pro
2025-11-09 10:43:12,334 - WARNING - root - LLMClient: no usable model found, LLM disabled
2025-11-09 10:43:12,335 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 10:43:12,335 - INFO - root - === 运行配置 ===
2025-11-09 10:43:12,335 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 10:43:12,335 - INFO - root - 关键词: Quant
2025-11-09 10:43:12,336 - INFO - root - 查询: block float point
2025-11-09 10:43:12,336 - INFO - root - 排序: None
2025-11-09 10:43:12,336 - INFO - root - 最近天数: 180
2025-11-09 10:43:12,336 - INFO - root - 最大处理数量: 50
2025-11-09 10:43:12,337 - INFO - root - 保存图片: 是
2025-11-09 10:43:12,337 - INFO - root - 输出语言: 中文
2025-11-09 10:43:12,338 - INFO - root - 强制重新处理: 否
2025-11-09 10:43:12,338 - INFO - root - ====================
2025-11-09 10:43:12,338 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 10:43:12,338 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 10:43:14,471 - INFO - root - get_all_titles_from_web 
2025-11-09 10:43:14,471 - INFO - root - Page:0, Index:0, From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators, https://arxiv.org/pdf/2511.00032, 2025-11-04
2025-11-09 10:43:14,471 - INFO - root - Page:0, Index:1, INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats, https://arxiv.org/pdf/2510.25602, 2025-10-29
2025-11-09 10:43:14,471 - INFO - root - Page:0, Index:2, MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving, https://arxiv.org/pdf/2510.14557, 2025-10-16
2025-11-09 10:43:14,471 - INFO - root - Page:0, Index:3, F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs, https://arxiv.org/pdf/2510.13401, 2025-10-15
2025-11-09 10:43:14,471 - INFO - root - Page:0, Index:4, Computationally Efficient Neural Receivers via Axial Self-Attention, https://arxiv.org/pdf/2510.12941, 2025-10-14
2025-11-09 10:43:14,471 - INFO - root - Page:0, Index:5, Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs, https://arxiv.org/pdf/2510.11192, 2025-10-13
2025-11-09 10:43:14,471 - INFO - root - Page:0, Index:6, Dissecting Transformers: A CLEAR Perspective towards Green AI, https://arxiv.org/pdf/2510.02810, 2025-10-03
2025-11-09 10:43:14,471 - INFO - root - Page:0, Index:7, Microscaling Floating Point Formats for Large Language Models, https://arxiv.org/pdf/2510.01863, 2025-10-02
2025-11-09 10:43:14,471 - INFO - root - Page:0, Index:8, Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework, https://arxiv.org/pdf/2509.26548, 2025-09-30
2025-11-09 10:43:14,471 - INFO - root - Page:0, Index:9, AMLA: MUL by ADD in FlashAttention Rescaling, https://arxiv.org/pdf/2509.25224, 2025-09-24
2025-11-09 10:43:14,471 - INFO - root - Page:0, Index:10, Pretraining Large Language Models with NVFP4, https://arxiv.org/pdf/2509.25149, 2025-09-29
2025-11-09 10:43:14,471 - INFO - root - Page:0, Index:11, Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization, https://arxiv.org/pdf/2509.23202, 2025-10-16
2025-11-09 10:43:14,471 - INFO - root - Page:0, Index:12, Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs, https://arxiv.org/pdf/2509.09019, 2025-09-10
2025-11-09 10:43:14,471 - INFO - root - Page:0, Index:13, Green Learning for STAR-RIS mmWave Systems with Implicit CSI, https://arxiv.org/pdf/2509.06820, 2025-09-08
2025-11-09 10:43:14,476 - INFO - root - Page:0, Index:14, A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN, https://arxiv.org/pdf/2508.12892, 2025-10-22
2025-11-09 10:43:14,476 - INFO - root - Page:0, Index:15, SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration, https://arxiv.org/pdf/2508.12271, 2025-08-17
2025-11-09 10:43:14,476 - INFO - root - Page:0, Index:16, SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration, https://arxiv.org/pdf/2508.02069, 2025-08-18
2025-11-09 10:43:14,476 - INFO - root - Page:0, Index:17, DGEMM without FP64 Arithmetic - Using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme, https://arxiv.org/pdf/2508.00441, 2025-09-25
2025-11-09 10:43:14,477 - INFO - root - Page:0, Index:18, Scaling Probabilistic Circuits via Monarch Matrices, https://arxiv.org/pdf/2506.12383, 2025-06-14
2025-11-09 10:43:14,477 - INFO - root - Page:0, Index:19, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 10:43:14,477 - INFO - root - Page:0, Index:20, Recipes for Pre-training LLMs with MXFP8, https://arxiv.org/pdf/2506.08027, 2025-08-18
2025-11-09 10:43:14,477 - INFO - root - Page:0, Index:21, FP4 All the Way: Fully Quantized Training of LLMs, https://arxiv.org/pdf/2505.19115, 2025-08-10
2025-11-09 10:43:14,478 - INFO - root - Page:0, Index:22, Automatic Verification of Floating-Point Accumulation Networks, https://arxiv.org/pdf/2505.18791, 2025-05-24
2025-11-09 10:43:14,478 - INFO - root - Page:0, Index:23, MXDOTP: A RISC-V ISA Extension for Enabling Microscaling (MX) Floating-Point Dot Products, https://arxiv.org/pdf/2505.13159, 2025-05-19
2025-11-09 10:43:14,478 - INFO - root - Page:0, Index:24, Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors, https://arxiv.org/pdf/2505.00580, 2025-07-15
2025-11-09 10:43:14,480 - INFO - root - Page:0, Index:25, Silenzio: Secure Non-Interactive Outsourced MLP Training, https://arxiv.org/pdf/2504.17785, 2025-09-18
2025-11-09 10:43:14,480 - INFO - root - Page:0, Index:26, Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution, https://arxiv.org/pdf/2503.14779, 2025-09-08
2025-11-09 10:43:14,480 - INFO - root - Page:0, Index:27, Flopping for FLOPs: Leveraging equivariance for computational efficiency, https://arxiv.org/pdf/2502.05169, 2025-06-24
2025-11-09 10:43:14,481 - INFO - root - Page:0, Index:28, An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in Medical Image, https://arxiv.org/pdf/2409.05324, 2025-07-26
2025-11-09 10:43:14,481 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 10:43:15,874 - INFO - root - get_all_titles_from_web 
2025-11-09 10:43:15,874 - INFO - root - Page:1, Index:0, TwinLiteNet+: An Enhanced Multi-Task Segmentation Model for Autonomous Driving, https://arxiv.org/pdf/2403.16958, 2025-08-30
2025-11-09 10:43:15,874 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 10:43:17,159 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 10:43:57,319 - INFO - root - 正在总结论文 1/30: From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators
2025-11-09 10:43:57,319 - INFO - root - 正在提取论文图片...
2025-11-09 10:43:57,752 - INFO - root - 已保存图片 1/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 10:43:57,814 - INFO - root - 已保存图片 2/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 10:43:57,914 - INFO - root - 已保存图片 3/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 10:43:57,978 - INFO - root - 已保存图片 4/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 10:43:58,046 - INFO - root - 已保存图片 5/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 10:43:58,121 - INFO - root - 已保存图片 6/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 10:43:58,206 - INFO - root - 已保存图片 7/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 10:43:58,277 - INFO - root - 已保存图片 8/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 10:43:58,296 - INFO - root - 已保存图片 9/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 10:43:58,307 - INFO - root - 已保存图片 10/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 10:43:58,313 - INFO - root - 成功添加图片 1：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 10:43:58,313 - INFO - root - 成功添加图片 2：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 10:43:58,313 - INFO - root - 成功添加图片 3：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 10:43:58,313 - INFO - root - 成功添加图片 4：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 10:43:58,315 - INFO - root - 成功添加图片 5：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 10:43:58,315 - INFO - root - 成功添加图片 6：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 10:43:58,315 - INFO - root - 成功添加图片 7：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 10:43:58,317 - INFO - root - 成功添加图片 8：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 10:43:58,317 - INFO - root - 成功添加图片 9：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 10:43:58,317 - INFO - root - 成功添加图片 10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 10:43:58,321 - INFO - root - 论文《From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators》的分析已保存到 ./export\From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural-1.md
2025-11-09 10:43:58,335 - INFO - root - 正在总结论文 2/30: INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats
2025-11-09 10:43:58,336 - INFO - root - 正在提取论文图片...
2025-11-09 10:43:58,817 - INFO - root - 已保存图片 1/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 10:43:58,884 - INFO - root - 已保存图片 2/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 10:43:58,918 - INFO - root - 已保存图片 3/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 10:43:58,989 - INFO - root - 已保存图片 4/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 10:43:58,995 - INFO - root - 成功添加图片 1：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 10:43:58,995 - INFO - root - 成功添加图片 2：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 10:43:58,997 - INFO - root - 成功添加图片 3：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 10:43:58,997 - INFO - root - 成功添加图片 4：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 10:43:59,009 - INFO - root - 论文《INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats》的分析已保存到 ./export\INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats-1.md
2025-11-09 10:43:59,018 - INFO - root - 正在总结论文 3/30: MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving
2025-11-09 10:43:59,020 - INFO - root - 正在提取论文图片...
2025-11-09 10:43:59,133 - INFO - root - 已保存图片 1/10：./export\images_MX+_ Pushing the Limits of Microscaling Formats for Efficient Large Language Mod\figure_1_page3.png
2025-11-09 10:43:59,133 - INFO - root - 成功添加图片 1：./export\images_MX+_ Pushing the Limits of Microscaling Formats for Efficient Large Language Mod\figure_1_page3.png
2025-11-09 10:43:59,137 - INFO - root - 论文《MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving》的分析已保存到 ./export\MX+_ Pushing the Limits of Microscaling Formats for Efficient Large Language Mod-1.md
2025-11-09 10:43:59,150 - INFO - root - 正在总结论文 4/30: F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs
2025-11-09 10:43:59,151 - INFO - root - 正在提取论文图片...
2025-11-09 10:44:00,097 - INFO - root - 已保存图片 1/10：./export\images_F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs\figure_1_page4.png
2025-11-09 10:44:00,242 - INFO - root - 已保存图片 2/10：./export\images_F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs\figure_2_page3.png
2025-11-09 10:44:00,286 - INFO - root - 已保存图片 3/10：./export\images_F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs\figure_3_page2.png
2025-11-09 10:44:00,298 - INFO - root - 成功添加图片 1：./export\images_F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs\figure_1_page4.png
2025-11-09 10:44:00,298 - INFO - root - 成功添加图片 2：./export\images_F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs\figure_2_page3.png
2025-11-09 10:44:00,299 - INFO - root - 成功添加图片 3：./export\images_F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs\figure_3_page2.png
2025-11-09 10:44:00,304 - INFO - root - 论文《F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs》的分析已保存到 ./export\F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs-1.md
2025-11-09 10:44:00,310 - INFO - root - 正在总结论文 5/30: Computationally Efficient Neural Receivers via Axial Self-Attention
2025-11-09 10:44:00,314 - INFO - root - 正在提取论文图片...
2025-11-09 10:44:01,593 - INFO - root - 已保存图片 1/10：./export\images_Computationally Efficient Neural Receivers via Axial Self-Attention\figure_1_page5.png
2025-11-09 10:44:01,601 - INFO - root - 成功添加图片 1：./export\images_Computationally Efficient Neural Receivers via Axial Self-Attention\figure_1_page5.png
2025-11-09 10:44:01,608 - INFO - root - 论文《Computationally Efficient Neural Receivers via Axial Self-Attention》的分析已保存到 ./export\Computationally Efficient Neural Receivers via Axial Self-Attention-1.md
2025-11-09 10:44:01,617 - INFO - root - 正在总结论文 6/30: Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs
2025-11-09 10:44:01,619 - INFO - root - 正在提取论文图片...
2025-11-09 10:44:01,703 - INFO - root - 已保存图片 1/10：./export\images_Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs\figure_1_page3.png
2025-11-09 10:44:01,769 - INFO - root - 已保存图片 2/10：./export\images_Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs\figure_2_page3.png
2025-11-09 10:44:01,909 - INFO - root - 已保存图片 3/10：./export\images_Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs\figure_3_page3.png
2025-11-09 10:44:01,910 - INFO - root - 成功添加图片 1：./export\images_Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs\figure_1_page3.png
2025-11-09 10:44:01,910 - INFO - root - 成功添加图片 2：./export\images_Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs\figure_2_page3.png
2025-11-09 10:44:01,912 - INFO - root - 成功添加图片 3：./export\images_Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs\figure_3_page3.png
2025-11-09 10:44:01,921 - INFO - root - 论文《Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs》的分析已保存到 ./export\Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs-1.md
2025-11-09 10:44:01,932 - INFO - root - 正在总结论文 7/30: Dissecting Transformers: A CLEAR Perspective towards Green AI
2025-11-09 10:44:01,934 - INFO - root - 正在提取论文图片...
2025-11-09 10:44:02,051 - WARNING - root - 处理页面 19 的图片 1 时出错：Image size (360000000 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.
2025-11-09 10:44:02,223 - INFO - root - 已保存图片 1/10：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_1_page3.png
2025-11-09 10:44:02,317 - INFO - root - 已保存图片 2/10：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_2_page3.png
2025-11-09 10:44:02,443 - INFO - root - 已保存图片 3/10：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_3_page3.png
2025-11-09 10:44:02,555 - INFO - root - 已保存图片 4/10：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_4_page3.png
2025-11-09 10:44:02,691 - INFO - root - 已保存图片 5/10：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_5_page3.png
2025-11-09 10:44:02,691 - INFO - root - 成功添加图片 1：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_1_page3.png
2025-11-09 10:44:02,691 - INFO - root - 成功添加图片 2：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_2_page3.png
2025-11-09 10:44:02,691 - INFO - root - 成功添加图片 3：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_3_page3.png
2025-11-09 10:44:02,691 - INFO - root - 成功添加图片 4：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_4_page3.png
2025-11-09 10:44:02,691 - INFO - root - 成功添加图片 5：./export\images_Dissecting Transformers_ A CLEAR Perspective towards Green AI\figure_5_page3.png
2025-11-09 10:44:02,699 - INFO - root - 论文《Dissecting Transformers: A CLEAR Perspective towards Green AI》的分析已保存到 ./export\Dissecting Transformers_ A CLEAR Perspective towards Green AI-1.md
2025-11-09 10:44:02,711 - INFO - root - 正在总结论文 8/30: Microscaling Floating Point Formats for Large Language Models
2025-11-09 10:44:02,718 - INFO - root - 正在提取论文图片...
2025-11-09 10:44:02,737 - INFO - root - 论文《Microscaling Floating Point Formats for Large Language Models》的分析已保存到 ./export\Microscaling Floating Point Formats for Large Language Models-1.md
2025-11-09 10:44:02,760 - INFO - root - 正在总结论文 9/30: Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework
2025-11-09 10:44:02,762 - INFO - root - 正在提取论文图片...
2025-11-09 10:44:32,563 - INFO - root - 已保存图片 1/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_1_page8.jpeg
2025-11-09 10:44:32,843 - INFO - root - 已保存图片 2/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_2_page5.jpeg
2025-11-09 10:44:33,156 - INFO - root - 已保存图片 3/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_3_page9.jpeg
2025-11-09 10:44:33,411 - INFO - root - 已保存图片 4/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_4_page9.jpeg
2025-11-09 10:44:33,641 - INFO - root - 已保存图片 5/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_5_page9.jpeg
2025-11-09 10:44:33,837 - INFO - root - 已保存图片 6/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_6_page9.jpeg
2025-11-09 10:44:34,057 - INFO - root - 已保存图片 7/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_7_page9.jpeg
2025-11-09 10:44:34,251 - INFO - root - 已保存图片 8/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_8_page9.jpeg
2025-11-09 10:44:34,453 - INFO - root - 已保存图片 9/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_9_page9.jpeg
2025-11-09 10:44:34,645 - INFO - root - 已保存图片 10/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_10_page9.jpeg
2025-11-09 10:44:34,678 - INFO - root - 成功添加图片 1：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_1_page8.jpeg
2025-11-09 10:44:34,679 - INFO - root - 成功添加图片 2：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_2_page5.jpeg
2025-11-09 10:44:34,680 - INFO - root - 成功添加图片 3：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_3_page9.jpeg
2025-11-09 10:44:34,680 - INFO - root - 成功添加图片 4：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_4_page9.jpeg
2025-11-09 10:44:34,680 - INFO - root - 成功添加图片 5：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_5_page9.jpeg
2025-11-09 10:44:34,680 - INFO - root - 成功添加图片 6：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_6_page9.jpeg
2025-11-09 10:44:34,680 - INFO - root - 成功添加图片 7：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_7_page9.jpeg
2025-11-09 10:44:34,680 - INFO - root - 成功添加图片 8：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_8_page9.jpeg
2025-11-09 10:44:34,680 - INFO - root - 成功添加图片 9：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_9_page9.jpeg
2025-11-09 10:44:34,680 - INFO - root - 成功添加图片 10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via\figure_10_page9.jpeg
2025-11-09 10:44:34,689 - INFO - root - 论文《Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework》的分析已保存到 ./export\Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via-1.md
2025-11-09 10:44:34,695 - INFO - root - 正在总结论文 10/30: AMLA: MUL by ADD in FlashAttention Rescaling
2025-11-09 10:44:34,698 - INFO - root - 正在提取论文图片...
2025-11-09 10:44:35,653 - INFO - root - 已保存图片 1/10：./export\images_AMLA_ MUL by ADD in FlashAttention Rescaling\figure_1_page15.png
2025-11-09 10:44:35,705 - INFO - root - 已保存图片 2/10：./export\images_AMLA_ MUL by ADD in FlashAttention Rescaling\figure_2_page4.jpeg
2025-11-09 10:44:35,752 - INFO - root - 已保存图片 3/10：./export\images_AMLA_ MUL by ADD in FlashAttention Rescaling\figure_3_page8.jpeg
2025-11-09 10:44:35,752 - INFO - root - 成功添加图片 1：./export\images_AMLA_ MUL by ADD in FlashAttention Rescaling\figure_1_page15.png
2025-11-09 10:44:35,752 - INFO - root - 成功添加图片 2：./export\images_AMLA_ MUL by ADD in FlashAttention Rescaling\figure_2_page4.jpeg
2025-11-09 10:44:35,752 - INFO - root - 成功添加图片 3：./export\images_AMLA_ MUL by ADD in FlashAttention Rescaling\figure_3_page8.jpeg
2025-11-09 10:44:35,766 - INFO - root - 论文《AMLA: MUL by ADD in FlashAttention Rescaling》的分析已保存到 ./export\AMLA_ MUL by ADD in FlashAttention Rescaling-1.md
2025-11-09 10:44:35,774 - INFO - root - 正在总结论文 11/30: Pretraining Large Language Models with NVFP4
2025-11-09 10:44:35,774 - INFO - root - 正在提取论文图片...
2025-11-09 10:44:35,975 - INFO - root - 已保存图片 1/10：./export\images_Pretraining Large Language Models with NVFP4\figure_1_page7.png
2025-11-09 10:44:36,036 - INFO - root - 已保存图片 2/10：./export\images_Pretraining Large Language Models with NVFP4\figure_2_page7.jpeg
2025-11-09 10:44:36,072 - INFO - root - 已保存图片 3/10：./export\images_Pretraining Large Language Models with NVFP4\figure_3_page1.png
2025-11-09 10:44:36,097 - INFO - root - 已保存图片 4/10：./export\images_Pretraining Large Language Models with NVFP4\figure_4_page7.jpeg
2025-11-09 10:44:36,128 - INFO - root - 已保存图片 5/10：./export\images_Pretraining Large Language Models with NVFP4\figure_5_page7.jpeg
2025-11-09 10:44:36,220 - INFO - root - 已保存图片 6/10：./export\images_Pretraining Large Language Models with NVFP4\figure_6_page7.jpeg
2025-11-09 10:44:36,277 - INFO - root - 已保存图片 7/10：./export\images_Pretraining Large Language Models with NVFP4\figure_7_page7.png
2025-11-09 10:44:36,307 - INFO - root - 已保存图片 8/10：./export\images_Pretraining Large Language Models with NVFP4\figure_8_page7.jpeg
2025-11-09 10:44:36,340 - INFO - root - 已保存图片 9/10：./export\images_Pretraining Large Language Models with NVFP4\figure_9_page7.jpeg
2025-11-09 10:44:36,387 - INFO - root - 已保存图片 10/10：./export\images_Pretraining Large Language Models with NVFP4\figure_10_page7.jpeg
2025-11-09 10:44:36,389 - INFO - root - 成功添加图片 1：./export\images_Pretraining Large Language Models with NVFP4\figure_1_page7.png
2025-11-09 10:44:36,389 - INFO - root - 成功添加图片 2：./export\images_Pretraining Large Language Models with NVFP4\figure_2_page7.jpeg
2025-11-09 10:44:36,389 - INFO - root - 成功添加图片 3：./export\images_Pretraining Large Language Models with NVFP4\figure_3_page1.png
2025-11-09 10:44:36,389 - INFO - root - 成功添加图片 4：./export\images_Pretraining Large Language Models with NVFP4\figure_4_page7.jpeg
2025-11-09 10:44:36,389 - INFO - root - 成功添加图片 5：./export\images_Pretraining Large Language Models with NVFP4\figure_5_page7.jpeg
2025-11-09 10:44:36,389 - INFO - root - 成功添加图片 6：./export\images_Pretraining Large Language Models with NVFP4\figure_6_page7.jpeg
2025-11-09 10:44:36,389 - INFO - root - 成功添加图片 7：./export\images_Pretraining Large Language Models with NVFP4\figure_7_page7.png
2025-11-09 10:44:36,389 - INFO - root - 成功添加图片 8：./export\images_Pretraining Large Language Models with NVFP4\figure_8_page7.jpeg
2025-11-09 10:44:36,397 - INFO - root - 成功添加图片 9：./export\images_Pretraining Large Language Models with NVFP4\figure_9_page7.jpeg
2025-11-09 10:44:36,397 - INFO - root - 成功添加图片 10：./export\images_Pretraining Large Language Models with NVFP4\figure_10_page7.jpeg
2025-11-09 10:44:36,406 - INFO - root - 论文《Pretraining Large Language Models with NVFP4》的分析已保存到 ./export\Pretraining Large Language Models with NVFP4-1.md
2025-11-09 10:44:36,416 - INFO - root - 正在总结论文 12/30: Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization
2025-11-09 10:44:36,418 - INFO - root - 正在提取论文图片...
2025-11-09 10:44:36,558 - INFO - root - 已保存图片 1/10：./export\images_Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantizati\figure_1_page25.png
2025-11-09 10:44:36,559 - INFO - root - 成功添加图片 1：./export\images_Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantizati\figure_1_page25.png
2025-11-09 10:44:36,577 - INFO - root - 论文《Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization》的分析已保存到 ./export\Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantizati-1.md
2025-11-09 10:44:36,589 - INFO - root - 正在总结论文 13/30: Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs
2025-11-09 10:44:36,591 - INFO - root - 正在提取论文图片...
2025-11-09 10:44:36,597 - INFO - root - 论文《Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs》的分析已保存到 ./export\Towards Verified Compilation of Floating-point Optimization in Scientific Comput-1.md
2025-11-09 10:44:36,608 - INFO - root - 正在总结论文 14/30: Green Learning for STAR-RIS mmWave Systems with Implicit CSI
2025-11-09 10:44:36,609 - INFO - root - 正在提取论文图片...
2025-11-09 10:44:36,795 - INFO - root - 已保存图片 1/10：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_1_page2.jpeg
2025-11-09 10:44:36,926 - INFO - root - 已保存图片 2/10：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_2_page2.png
2025-11-09 10:44:36,977 - INFO - root - 已保存图片 3/10：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_3_page2.png
2025-11-09 10:44:37,035 - INFO - root - 已保存图片 4/10：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_4_page2.png
2025-11-09 10:44:37,037 - INFO - root - 成功添加图片 1：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_1_page2.jpeg
2025-11-09 10:44:37,039 - INFO - root - 成功添加图片 2：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_2_page2.png
2025-11-09 10:44:37,039 - INFO - root - 成功添加图片 3：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_3_page2.png
2025-11-09 10:44:37,041 - INFO - root - 成功添加图片 4：./export\images_Green Learning for STAR-RIS mmWave Systems with Implicit CSI\figure_4_page2.png
2025-11-09 10:44:37,055 - INFO - root - 论文《Green Learning for STAR-RIS mmWave Systems with Implicit CSI》的分析已保存到 ./export\Green Learning for STAR-RIS mmWave Systems with Implicit CSI-1.md
2025-11-09 10:44:37,064 - INFO - root - 正在总结论文 15/30: A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN
2025-11-09 10:44:37,065 - INFO - root - 正在提取论文图片...
2025-11-09 10:44:37,775 - INFO - root - 已保存图片 1/10：./export\images_A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted\figure_1_page6.png
2025-11-09 10:44:37,872 - INFO - root - 已保存图片 2/10：./export\images_A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted\figure_2_page6.png
2025-11-09 10:44:37,969 - INFO - root - 已保存图片 3/10：./export\images_A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted\figure_3_page2.png
2025-11-09 10:54:15,017 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 10:54:15,033 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 10:54:15,033 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 10:55:08,572 - ERROR - root - LLMClient: error during initialization: Timeout of 60.0s exceeded, last exception: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:142.251.34.202:443: socket is null
2025-11-09 10:55:08,574 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 10:55:08,574 - INFO - root - === 运行配置 ===
2025-11-09 10:55:08,575 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 10:55:08,576 - INFO - root - 关键词: Quant
2025-11-09 10:55:08,576 - INFO - root - 查询: block float point
2025-11-09 10:55:08,578 - INFO - root - 排序: None
2025-11-09 10:55:08,578 - INFO - root - 最近天数: 180
2025-11-09 10:55:08,578 - INFO - root - 最大处理数量: 50
2025-11-09 10:55:08,578 - INFO - root - 保存图片: 是
2025-11-09 10:55:08,578 - INFO - root - 输出语言: 中文
2025-11-09 10:55:08,578 - INFO - root - 强制重新处理: 否
2025-11-09 10:55:08,578 - INFO - root - ====================
2025-11-09 10:55:08,590 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 10:55:08,590 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 10:55:39,943 - INFO - root - get_all_titles_from_web 
2025-11-09 10:55:39,943 - INFO - root - Page:0, Index:0, From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators, https://arxiv.org/pdf/2511.00032, 2025-11-04
2025-11-09 10:55:39,943 - INFO - root - Page:0, Index:1, INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats, https://arxiv.org/pdf/2510.25602, 2025-10-29
2025-11-09 10:55:39,943 - INFO - root - Page:0, Index:2, MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving, https://arxiv.org/pdf/2510.14557, 2025-10-16
2025-11-09 10:55:39,943 - INFO - root - Page:0, Index:3, F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs, https://arxiv.org/pdf/2510.13401, 2025-10-15
2025-11-09 10:55:39,943 - INFO - root - Page:0, Index:4, Computationally Efficient Neural Receivers via Axial Self-Attention, https://arxiv.org/pdf/2510.12941, 2025-10-14
2025-11-09 10:55:39,943 - INFO - root - Page:0, Index:5, Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs, https://arxiv.org/pdf/2510.11192, 2025-10-13
2025-11-09 10:55:39,943 - INFO - root - Page:0, Index:6, Dissecting Transformers: A CLEAR Perspective towards Green AI, https://arxiv.org/pdf/2510.02810, 2025-10-03
2025-11-09 10:55:39,943 - INFO - root - Page:0, Index:7, Microscaling Floating Point Formats for Large Language Models, https://arxiv.org/pdf/2510.01863, 2025-10-02
2025-11-09 10:55:39,943 - INFO - root - Page:0, Index:8, Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework, https://arxiv.org/pdf/2509.26548, 2025-09-30
2025-11-09 10:55:39,943 - INFO - root - Page:0, Index:9, AMLA: MUL by ADD in FlashAttention Rescaling, https://arxiv.org/pdf/2509.25224, 2025-09-24
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:10, Pretraining Large Language Models with NVFP4, https://arxiv.org/pdf/2509.25149, 2025-09-29
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:11, Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization, https://arxiv.org/pdf/2509.23202, 2025-10-16
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:12, Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs, https://arxiv.org/pdf/2509.09019, 2025-09-10
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:13, Green Learning for STAR-RIS mmWave Systems with Implicit CSI, https://arxiv.org/pdf/2509.06820, 2025-09-08
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:14, A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN, https://arxiv.org/pdf/2508.12892, 2025-10-22
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:15, SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration, https://arxiv.org/pdf/2508.12271, 2025-08-17
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:16, SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration, https://arxiv.org/pdf/2508.02069, 2025-08-18
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:17, DGEMM without FP64 Arithmetic - Using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme, https://arxiv.org/pdf/2508.00441, 2025-09-25
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:18, Scaling Probabilistic Circuits via Monarch Matrices, https://arxiv.org/pdf/2506.12383, 2025-06-14
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:19, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:20, Recipes for Pre-training LLMs with MXFP8, https://arxiv.org/pdf/2506.08027, 2025-08-18
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:21, FP4 All the Way: Fully Quantized Training of LLMs, https://arxiv.org/pdf/2505.19115, 2025-08-10
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:22, Automatic Verification of Floating-Point Accumulation Networks, https://arxiv.org/pdf/2505.18791, 2025-05-24
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:23, MXDOTP: A RISC-V ISA Extension for Enabling Microscaling (MX) Floating-Point Dot Products, https://arxiv.org/pdf/2505.13159, 2025-05-19
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:24, Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors, https://arxiv.org/pdf/2505.00580, 2025-07-15
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:25, Silenzio: Secure Non-Interactive Outsourced MLP Training, https://arxiv.org/pdf/2504.17785, 2025-09-18
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:26, Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution, https://arxiv.org/pdf/2503.14779, 2025-09-08
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:27, Flopping for FLOPs: Leveraging equivariance for computational efficiency, https://arxiv.org/pdf/2502.05169, 2025-06-24
2025-11-09 10:55:39,949 - INFO - root - Page:0, Index:28, An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in Medical Image, https://arxiv.org/pdf/2409.05324, 2025-07-26
2025-11-09 10:55:39,949 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 10:59:20,251 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 10:59:20,252 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 10:59:20,254 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 10:59:21,780 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 10:59:22,916 - INFO - root - LLMClient: trying model gemini-2.5-pro
2025-11-09 10:59:23,284 - WARNING - root - LLMClient: no usable model found, LLM disabled
2025-11-09 10:59:23,284 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 10:59:23,285 - INFO - root - === 运行配置 ===
2025-11-09 10:59:23,285 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 10:59:23,286 - INFO - root - 关键词: Quant
2025-11-09 10:59:23,286 - INFO - root - 查询: block float point
2025-11-09 10:59:23,286 - INFO - root - 排序: None
2025-11-09 10:59:23,287 - INFO - root - 最近天数: 180
2025-11-09 10:59:23,287 - INFO - root - 最大处理数量: 50
2025-11-09 10:59:23,288 - INFO - root - 保存图片: 是
2025-11-09 10:59:23,288 - INFO - root - 输出语言: 中文
2025-11-09 10:59:23,288 - INFO - root - 强制重新处理: 否
2025-11-09 10:59:23,288 - INFO - root - ====================
2025-11-09 10:59:23,289 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 10:59:23,290 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 10:59:29,441 - INFO - root - get_all_titles_from_web 
2025-11-09 10:59:29,441 - INFO - root - Page:0, Index:0, From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators, https://arxiv.org/pdf/2511.00032, 2025-11-04
2025-11-09 10:59:29,442 - INFO - root - Page:0, Index:1, INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats, https://arxiv.org/pdf/2510.25602, 2025-10-29
2025-11-09 10:59:29,442 - INFO - root - Page:0, Index:2, MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving, https://arxiv.org/pdf/2510.14557, 2025-10-16
2025-11-09 10:59:29,442 - INFO - root - Page:0, Index:3, F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs, https://arxiv.org/pdf/2510.13401, 2025-10-15
2025-11-09 10:59:29,442 - INFO - root - Page:0, Index:4, Computationally Efficient Neural Receivers via Axial Self-Attention, https://arxiv.org/pdf/2510.12941, 2025-10-14
2025-11-09 10:59:29,444 - INFO - root - Page:0, Index:5, Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs, https://arxiv.org/pdf/2510.11192, 2025-10-13
2025-11-09 10:59:29,444 - INFO - root - Page:0, Index:6, Dissecting Transformers: A CLEAR Perspective towards Green AI, https://arxiv.org/pdf/2510.02810, 2025-10-03
2025-11-09 10:59:29,444 - INFO - root - Page:0, Index:7, Microscaling Floating Point Formats for Large Language Models, https://arxiv.org/pdf/2510.01863, 2025-10-02
2025-11-09 10:59:29,444 - INFO - root - Page:0, Index:8, Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework, https://arxiv.org/pdf/2509.26548, 2025-09-30
2025-11-09 10:59:29,445 - INFO - root - Page:0, Index:9, AMLA: MUL by ADD in FlashAttention Rescaling, https://arxiv.org/pdf/2509.25224, 2025-09-24
2025-11-09 10:59:29,445 - INFO - root - Page:0, Index:10, Pretraining Large Language Models with NVFP4, https://arxiv.org/pdf/2509.25149, 2025-09-29
2025-11-09 10:59:29,445 - INFO - root - Page:0, Index:11, Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization, https://arxiv.org/pdf/2509.23202, 2025-10-16
2025-11-09 10:59:29,446 - INFO - root - Page:0, Index:12, Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs, https://arxiv.org/pdf/2509.09019, 2025-09-10
2025-11-09 10:59:29,446 - INFO - root - Page:0, Index:13, Green Learning for STAR-RIS mmWave Systems with Implicit CSI, https://arxiv.org/pdf/2509.06820, 2025-09-08
2025-11-09 10:59:29,446 - INFO - root - Page:0, Index:14, A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN, https://arxiv.org/pdf/2508.12892, 2025-10-22
2025-11-09 10:59:29,447 - INFO - root - Page:0, Index:15, SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration, https://arxiv.org/pdf/2508.12271, 2025-08-17
2025-11-09 10:59:29,447 - INFO - root - Page:0, Index:16, SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration, https://arxiv.org/pdf/2508.02069, 2025-08-18
2025-11-09 10:59:29,450 - INFO - root - Page:0, Index:17, DGEMM without FP64 Arithmetic - Using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme, https://arxiv.org/pdf/2508.00441, 2025-09-25
2025-11-09 10:59:29,451 - INFO - root - Page:0, Index:18, Scaling Probabilistic Circuits via Monarch Matrices, https://arxiv.org/pdf/2506.12383, 2025-06-14
2025-11-09 10:59:29,452 - INFO - root - Page:0, Index:19, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 10:59:29,452 - INFO - root - Page:0, Index:20, Recipes for Pre-training LLMs with MXFP8, https://arxiv.org/pdf/2506.08027, 2025-08-18
2025-11-09 10:59:29,452 - INFO - root - Page:0, Index:21, FP4 All the Way: Fully Quantized Training of LLMs, https://arxiv.org/pdf/2505.19115, 2025-08-10
2025-11-09 10:59:29,453 - INFO - root - Page:0, Index:22, Automatic Verification of Floating-Point Accumulation Networks, https://arxiv.org/pdf/2505.18791, 2025-05-24
2025-11-09 10:59:29,454 - INFO - root - Page:0, Index:23, MXDOTP: A RISC-V ISA Extension for Enabling Microscaling (MX) Floating-Point Dot Products, https://arxiv.org/pdf/2505.13159, 2025-05-19
2025-11-09 10:59:29,455 - INFO - root - Page:0, Index:24, Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors, https://arxiv.org/pdf/2505.00580, 2025-07-15
2025-11-09 10:59:29,455 - INFO - root - Page:0, Index:25, Silenzio: Secure Non-Interactive Outsourced MLP Training, https://arxiv.org/pdf/2504.17785, 2025-09-18
2025-11-09 10:59:29,455 - INFO - root - Page:0, Index:26, Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution, https://arxiv.org/pdf/2503.14779, 2025-09-08
2025-11-09 10:59:29,456 - INFO - root - Page:0, Index:27, Flopping for FLOPs: Leveraging equivariance for computational efficiency, https://arxiv.org/pdf/2502.05169, 2025-06-24
2025-11-09 10:59:29,457 - INFO - root - Page:0, Index:28, An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in Medical Image, https://arxiv.org/pdf/2409.05324, 2025-07-26
2025-11-09 10:59:29,458 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 10:59:35,891 - INFO - root - get_all_titles_from_web 
2025-11-09 10:59:35,892 - INFO - root - Page:1, Index:0, TwinLiteNet+: An Enhanced Multi-Task Segmentation Model for Autonomous Driving, https://arxiv.org/pdf/2403.16958, 2025-08-30
2025-11-09 10:59:35,892 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 10:59:42,083 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 11:02:42,840 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 11:02:42,841 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 11:02:42,846 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 11:02:43,393 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 11:02:44,233 - INFO - root - LLMClient: trying model gemini-2.5-pro
2025-11-09 11:02:44,596 - WARNING - root - LLMClient: no usable model found, LLM disabled
2025-11-09 11:02:44,596 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 11:02:44,596 - INFO - root - === 运行配置 ===
2025-11-09 11:02:44,597 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 11:02:44,597 - INFO - root - 关键词: Quant
2025-11-09 11:02:44,598 - INFO - root - 查询: block float point
2025-11-09 11:02:44,598 - INFO - root - 排序: None
2025-11-09 11:02:44,598 - INFO - root - 最近天数: 180
2025-11-09 11:02:44,599 - INFO - root - 最大处理数量: 50
2025-11-09 11:02:44,599 - INFO - root - 保存图片: 是
2025-11-09 11:02:44,600 - INFO - root - 输出语言: 中文
2025-11-09 11:02:44,600 - INFO - root - 强制重新处理: 否
2025-11-09 11:02:44,600 - INFO - root - ====================
2025-11-09 11:02:44,600 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 11:02:44,601 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 11:02:50,748 - INFO - root - get_all_titles_from_web 
2025-11-09 11:02:50,750 - INFO - root - Page:0, Index:0, From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators, https://arxiv.org/pdf/2511.00032, 2025-11-04
2025-11-09 11:02:50,750 - INFO - root - Page:0, Index:1, INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats, https://arxiv.org/pdf/2510.25602, 2025-10-29
2025-11-09 11:02:50,751 - INFO - root - Page:0, Index:2, MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving, https://arxiv.org/pdf/2510.14557, 2025-10-16
2025-11-09 11:02:50,751 - INFO - root - Page:0, Index:3, F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs, https://arxiv.org/pdf/2510.13401, 2025-10-15
2025-11-09 11:02:50,751 - INFO - root - Page:0, Index:4, Computationally Efficient Neural Receivers via Axial Self-Attention, https://arxiv.org/pdf/2510.12941, 2025-10-14
2025-11-09 11:02:50,751 - INFO - root - Page:0, Index:5, Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs, https://arxiv.org/pdf/2510.11192, 2025-10-13
2025-11-09 11:02:50,751 - INFO - root - Page:0, Index:6, Dissecting Transformers: A CLEAR Perspective towards Green AI, https://arxiv.org/pdf/2510.02810, 2025-10-03
2025-11-09 11:02:50,751 - INFO - root - Page:0, Index:7, Microscaling Floating Point Formats for Large Language Models, https://arxiv.org/pdf/2510.01863, 2025-10-02
2025-11-09 11:02:50,751 - INFO - root - Page:0, Index:8, Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework, https://arxiv.org/pdf/2509.26548, 2025-09-30
2025-11-09 11:02:50,751 - INFO - root - Page:0, Index:9, AMLA: MUL by ADD in FlashAttention Rescaling, https://arxiv.org/pdf/2509.25224, 2025-09-24
2025-11-09 11:02:50,751 - INFO - root - Page:0, Index:10, Pretraining Large Language Models with NVFP4, https://arxiv.org/pdf/2509.25149, 2025-09-29
2025-11-09 11:02:50,751 - INFO - root - Page:0, Index:11, Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization, https://arxiv.org/pdf/2509.23202, 2025-10-16
2025-11-09 11:02:50,755 - INFO - root - Page:0, Index:12, Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs, https://arxiv.org/pdf/2509.09019, 2025-09-10
2025-11-09 11:02:50,755 - INFO - root - Page:0, Index:13, Green Learning for STAR-RIS mmWave Systems with Implicit CSI, https://arxiv.org/pdf/2509.06820, 2025-09-08
2025-11-09 11:02:50,755 - INFO - root - Page:0, Index:14, A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN, https://arxiv.org/pdf/2508.12892, 2025-10-22
2025-11-09 11:02:50,756 - INFO - root - Page:0, Index:15, SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration, https://arxiv.org/pdf/2508.12271, 2025-08-17
2025-11-09 11:02:50,758 - INFO - root - Page:0, Index:16, SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration, https://arxiv.org/pdf/2508.02069, 2025-08-18
2025-11-09 11:02:50,758 - INFO - root - Page:0, Index:17, DGEMM without FP64 Arithmetic - Using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme, https://arxiv.org/pdf/2508.00441, 2025-09-25
2025-11-09 11:02:50,758 - INFO - root - Page:0, Index:18, Scaling Probabilistic Circuits via Monarch Matrices, https://arxiv.org/pdf/2506.12383, 2025-06-14
2025-11-09 11:02:50,758 - INFO - root - Page:0, Index:19, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 11:02:50,758 - INFO - root - Page:0, Index:20, Recipes for Pre-training LLMs with MXFP8, https://arxiv.org/pdf/2506.08027, 2025-08-18
2025-11-09 11:02:50,758 - INFO - root - Page:0, Index:21, FP4 All the Way: Fully Quantized Training of LLMs, https://arxiv.org/pdf/2505.19115, 2025-08-10
2025-11-09 11:02:50,759 - INFO - root - Page:0, Index:22, Automatic Verification of Floating-Point Accumulation Networks, https://arxiv.org/pdf/2505.18791, 2025-05-24
2025-11-09 11:02:50,759 - INFO - root - Page:0, Index:23, MXDOTP: A RISC-V ISA Extension for Enabling Microscaling (MX) Floating-Point Dot Products, https://arxiv.org/pdf/2505.13159, 2025-05-19
2025-11-09 11:02:50,760 - INFO - root - Page:0, Index:24, Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors, https://arxiv.org/pdf/2505.00580, 2025-07-15
2025-11-09 11:02:50,760 - INFO - root - Page:0, Index:25, Silenzio: Secure Non-Interactive Outsourced MLP Training, https://arxiv.org/pdf/2504.17785, 2025-09-18
2025-11-09 11:02:50,760 - INFO - root - Page:0, Index:26, Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution, https://arxiv.org/pdf/2503.14779, 2025-09-08
2025-11-09 11:02:50,762 - INFO - root - Page:0, Index:27, Flopping for FLOPs: Leveraging equivariance for computational efficiency, https://arxiv.org/pdf/2502.05169, 2025-06-24
2025-11-09 11:02:50,762 - INFO - root - Page:0, Index:28, An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in Medical Image, https://arxiv.org/pdf/2409.05324, 2025-07-26
2025-11-09 11:02:50,762 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 11:14:07,152 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 11:14:07,154 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 11:14:07,155 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 11:14:07,953 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 11:14:08,826 - INFO - root - LLMClient: trying model gemini-2.5-pro
2025-11-09 11:14:09,206 - WARNING - root - LLMClient: no usable model found, LLM disabled
2025-11-09 11:14:09,206 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 11:14:09,207 - INFO - root - === 运行配置 ===
2025-11-09 11:14:09,207 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 11:14:09,207 - INFO - root - 关键词: Quant
2025-11-09 11:14:09,207 - INFO - root - 查询: block float point
2025-11-09 11:14:09,208 - INFO - root - 排序: None
2025-11-09 11:14:09,208 - INFO - root - 最近天数: 180
2025-11-09 11:14:09,208 - INFO - root - 最大处理数量: 50
2025-11-09 11:14:09,209 - INFO - root - 保存图片: 是
2025-11-09 11:14:09,209 - INFO - root - 输出语言: 中文
2025-11-09 11:14:09,209 - INFO - root - 强制重新处理: 否
2025-11-09 11:14:09,209 - INFO - root - ====================
2025-11-09 11:14:09,210 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 11:14:09,210 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 11:14:15,463 - INFO - root - get_all_titles_from_web 
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:0, From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators, https://arxiv.org/pdf/2511.00032, 2025-11-04
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:1, INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats, https://arxiv.org/pdf/2510.25602, 2025-10-29
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:2, MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving, https://arxiv.org/pdf/2510.14557, 2025-10-16
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:3, F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs, https://arxiv.org/pdf/2510.13401, 2025-10-15
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:4, Computationally Efficient Neural Receivers via Axial Self-Attention, https://arxiv.org/pdf/2510.12941, 2025-10-14
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:5, Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs, https://arxiv.org/pdf/2510.11192, 2025-10-13
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:6, Dissecting Transformers: A CLEAR Perspective towards Green AI, https://arxiv.org/pdf/2510.02810, 2025-10-03
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:7, Microscaling Floating Point Formats for Large Language Models, https://arxiv.org/pdf/2510.01863, 2025-10-02
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:8, Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework, https://arxiv.org/pdf/2509.26548, 2025-09-30
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:9, AMLA: MUL by ADD in FlashAttention Rescaling, https://arxiv.org/pdf/2509.25224, 2025-09-24
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:10, Pretraining Large Language Models with NVFP4, https://arxiv.org/pdf/2509.25149, 2025-09-29
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:11, Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization, https://arxiv.org/pdf/2509.23202, 2025-10-16
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:12, Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs, https://arxiv.org/pdf/2509.09019, 2025-09-10
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:13, Green Learning for STAR-RIS mmWave Systems with Implicit CSI, https://arxiv.org/pdf/2509.06820, 2025-09-08
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:14, A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN, https://arxiv.org/pdf/2508.12892, 2025-10-22
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:15, SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration, https://arxiv.org/pdf/2508.12271, 2025-08-17
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:16, SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration, https://arxiv.org/pdf/2508.02069, 2025-08-18
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:17, DGEMM without FP64 Arithmetic - Using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme, https://arxiv.org/pdf/2508.00441, 2025-09-25
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:18, Scaling Probabilistic Circuits via Monarch Matrices, https://arxiv.org/pdf/2506.12383, 2025-06-14
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:19, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:20, Recipes for Pre-training LLMs with MXFP8, https://arxiv.org/pdf/2506.08027, 2025-08-18
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:21, FP4 All the Way: Fully Quantized Training of LLMs, https://arxiv.org/pdf/2505.19115, 2025-08-10
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:22, Automatic Verification of Floating-Point Accumulation Networks, https://arxiv.org/pdf/2505.18791, 2025-05-24
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:23, MXDOTP: A RISC-V ISA Extension for Enabling Microscaling (MX) Floating-Point Dot Products, https://arxiv.org/pdf/2505.13159, 2025-05-19
2025-11-09 11:14:15,463 - INFO - root - Page:0, Index:24, Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors, https://arxiv.org/pdf/2505.00580, 2025-07-15
2025-11-09 11:14:15,473 - INFO - root - Page:0, Index:25, Silenzio: Secure Non-Interactive Outsourced MLP Training, https://arxiv.org/pdf/2504.17785, 2025-09-18
2025-11-09 11:14:15,475 - INFO - root - Page:0, Index:26, Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution, https://arxiv.org/pdf/2503.14779, 2025-09-08
2025-11-09 11:14:15,475 - INFO - root - Page:0, Index:27, Flopping for FLOPs: Leveraging equivariance for computational efficiency, https://arxiv.org/pdf/2502.05169, 2025-06-24
2025-11-09 11:14:15,477 - INFO - root - Page:0, Index:28, An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in Medical Image, https://arxiv.org/pdf/2409.05324, 2025-07-26
2025-11-09 11:14:15,477 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 11:14:21,869 - INFO - root - get_all_titles_from_web 
2025-11-09 11:14:21,869 - INFO - root - Page:1, Index:0, TwinLiteNet+: An Enhanced Multi-Task Segmentation Model for Autonomous Driving, https://arxiv.org/pdf/2403.16958, 2025-08-30
2025-11-09 11:14:21,869 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 11:14:28,257 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 11:18:03,978 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 11:18:03,979 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 11:18:03,981 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 11:18:04,669 - INFO - root - LLMClient: trying model gemini-2.5-flash
2025-11-09 11:18:05,549 - INFO - root - LLMClient: trying model gemini-2.5-pro
2025-11-09 11:18:05,899 - WARNING - root - LLMClient: no usable model found, LLM disabled
2025-11-09 11:18:05,899 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 11:18:05,900 - INFO - root - === 运行配置 ===
2025-11-09 11:18:05,901 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 11:18:05,901 - INFO - root - 关键词: Quant
2025-11-09 11:18:05,901 - INFO - root - 查询: block float point
2025-11-09 11:18:05,901 - INFO - root - 排序: None
2025-11-09 11:18:05,902 - INFO - root - 最近天数: 180
2025-11-09 11:18:05,902 - INFO - root - 最大处理数量: 2
2025-11-09 11:18:05,903 - INFO - root - 保存图片: 是
2025-11-09 11:18:05,903 - INFO - root - 输出语言: 中文
2025-11-09 11:18:05,903 - INFO - root - 强制重新处理: 否
2025-11-09 11:18:05,903 - INFO - root - ====================
2025-11-09 11:18:05,904 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 11:18:05,904 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 11:18:12,193 - INFO - root - get_all_titles_from_web 
2025-11-09 11:18:12,193 - INFO - root - Page:0, Index:0, From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators, https://arxiv.org/pdf/2511.00032, 2025-11-04
2025-11-09 11:18:12,194 - INFO - root - Page:0, Index:1, INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats, https://arxiv.org/pdf/2510.25602, 2025-10-29
2025-11-09 11:18:12,194 - INFO - root - Page:0, Index:2, MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving, https://arxiv.org/pdf/2510.14557, 2025-10-16
2025-11-09 11:18:12,194 - INFO - root - Page:0, Index:3, F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs, https://arxiv.org/pdf/2510.13401, 2025-10-15
2025-11-09 11:18:12,194 - INFO - root - Page:0, Index:4, Computationally Efficient Neural Receivers via Axial Self-Attention, https://arxiv.org/pdf/2510.12941, 2025-10-14
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:5, Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs, https://arxiv.org/pdf/2510.11192, 2025-10-13
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:6, Dissecting Transformers: A CLEAR Perspective towards Green AI, https://arxiv.org/pdf/2510.02810, 2025-10-03
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:7, Microscaling Floating Point Formats for Large Language Models, https://arxiv.org/pdf/2510.01863, 2025-10-02
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:8, Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework, https://arxiv.org/pdf/2509.26548, 2025-09-30
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:9, AMLA: MUL by ADD in FlashAttention Rescaling, https://arxiv.org/pdf/2509.25224, 2025-09-24
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:10, Pretraining Large Language Models with NVFP4, https://arxiv.org/pdf/2509.25149, 2025-09-29
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:11, Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization, https://arxiv.org/pdf/2509.23202, 2025-10-16
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:12, Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs, https://arxiv.org/pdf/2509.09019, 2025-09-10
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:13, Green Learning for STAR-RIS mmWave Systems with Implicit CSI, https://arxiv.org/pdf/2509.06820, 2025-09-08
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:14, A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN, https://arxiv.org/pdf/2508.12892, 2025-10-22
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:15, SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration, https://arxiv.org/pdf/2508.12271, 2025-08-17
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:16, SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration, https://arxiv.org/pdf/2508.02069, 2025-08-18
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:17, DGEMM without FP64 Arithmetic - Using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme, https://arxiv.org/pdf/2508.00441, 2025-09-25
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:18, Scaling Probabilistic Circuits via Monarch Matrices, https://arxiv.org/pdf/2506.12383, 2025-06-14
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:19, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:20, Recipes for Pre-training LLMs with MXFP8, https://arxiv.org/pdf/2506.08027, 2025-08-18
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:21, FP4 All the Way: Fully Quantized Training of LLMs, https://arxiv.org/pdf/2505.19115, 2025-08-10
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:22, Automatic Verification of Floating-Point Accumulation Networks, https://arxiv.org/pdf/2505.18791, 2025-05-24
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:23, MXDOTP: A RISC-V ISA Extension for Enabling Microscaling (MX) Floating-Point Dot Products, https://arxiv.org/pdf/2505.13159, 2025-05-19
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:24, Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors, https://arxiv.org/pdf/2505.00580, 2025-07-15
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:25, Silenzio: Secure Non-Interactive Outsourced MLP Training, https://arxiv.org/pdf/2504.17785, 2025-09-18
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:26, Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution, https://arxiv.org/pdf/2503.14779, 2025-09-08
2025-11-09 11:18:12,196 - INFO - root - Page:0, Index:27, Flopping for FLOPs: Leveraging equivariance for computational efficiency, https://arxiv.org/pdf/2502.05169, 2025-06-24
2025-11-09 11:18:12,204 - INFO - root - Page:0, Index:28, An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in Medical Image, https://arxiv.org/pdf/2409.05324, 2025-07-26
2025-11-09 11:18:12,204 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 11:18:18,346 - INFO - root - get_all_titles_from_web 
2025-11-09 11:18:18,347 - INFO - root - Page:1, Index:0, TwinLiteNet+: An Enhanced Multi-Task Segmentation Model for Autonomous Driving, https://arxiv.org/pdf/2403.16958, 2025-08-30
2025-11-09 11:18:18,347 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 11:18:24,765 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 11:18:36,427 - INFO - root - 正在总结论文 1/2: From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators
2025-11-09 11:18:36,429 - INFO - root - 正在提取论文图片...
2025-11-09 11:18:36,905 - INFO - root - 已保存图片 1/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 11:18:36,981 - INFO - root - 已保存图片 2/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 11:18:37,084 - INFO - root - 已保存图片 3/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 11:18:37,161 - INFO - root - 已保存图片 4/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 11:18:37,262 - INFO - root - 已保存图片 5/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 11:18:37,350 - INFO - root - 已保存图片 6/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 11:18:37,428 - INFO - root - 已保存图片 7/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 11:18:37,564 - INFO - root - 已保存图片 8/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 11:18:37,585 - INFO - root - 已保存图片 9/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 11:18:37,612 - INFO - root - 已保存图片 10/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 11:18:37,619 - INFO - root - 成功添加图片 1：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 11:18:37,621 - INFO - root - 成功添加图片 2：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 11:18:37,625 - INFO - root - 成功添加图片 3：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 11:18:37,626 - INFO - root - 成功添加图片 4：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 11:18:37,627 - INFO - root - 成功添加图片 5：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 11:18:37,627 - INFO - root - 成功添加图片 6：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 11:18:37,628 - INFO - root - 成功添加图片 7：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 11:18:37,629 - INFO - root - 成功添加图片 8：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 11:18:37,630 - INFO - root - 成功添加图片 9：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 11:18:37,631 - INFO - root - 成功添加图片 10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 11:18:37,634 - INFO - root - 论文《From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators》的分析已保存到 ./export\From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural-2.md
2025-11-09 11:18:37,645 - INFO - root - 正在总结论文 2/2: INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats
2025-11-09 11:18:37,646 - INFO - root - 正在提取论文图片...
2025-11-09 11:18:38,452 - INFO - root - 已保存图片 1/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 11:18:38,533 - INFO - root - 已保存图片 2/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 11:18:38,563 - INFO - root - 已保存图片 3/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 11:18:38,638 - INFO - root - 已保存图片 4/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 11:18:38,648 - INFO - root - 成功添加图片 1：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 11:18:38,648 - INFO - root - 成功添加图片 2：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 11:18:38,650 - INFO - root - 成功添加图片 3：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 11:18:38,650 - INFO - root - 成功添加图片 4：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 11:18:38,652 - INFO - root - 论文《INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats》的分析已保存到 ./export\INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats-2.md
2025-11-09 11:18:38,667 - INFO - root - summary time: 34.69 seconds
2025-11-09 11:29:36,716 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 11:29:36,721 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 11:29:36,737 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 11:30:37,916 - ERROR - root - LLMClient: error during initialization: Timeout of 60.0s exceeded, last exception: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.73.138:443: tcp handshaker shutdown
2025-11-09 11:30:37,917 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 11:30:37,917 - INFO - root - === 运行配置 ===
2025-11-09 11:30:37,918 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 11:30:37,918 - INFO - root - 关键词: Quant
2025-11-09 11:30:37,918 - INFO - root - 查询: block float point
2025-11-09 11:30:37,919 - INFO - root - 排序: None
2025-11-09 11:30:37,919 - INFO - root - 最近天数: 180
2025-11-09 11:30:37,919 - INFO - root - 最大处理数量: 2
2025-11-09 11:30:37,919 - INFO - root - 保存图片: 是
2025-11-09 11:30:37,920 - INFO - root - 输出语言: 中文
2025-11-09 11:30:37,920 - INFO - root - 强制重新处理: 否
2025-11-09 11:30:37,920 - INFO - root - ====================
2025-11-09 11:30:37,921 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 11:30:37,921 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 11:31:05,549 - INFO - root - get_all_titles_from_web 
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:0, From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators, https://arxiv.org/pdf/2511.00032, 2025-11-04
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:1, INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats, https://arxiv.org/pdf/2510.25602, 2025-10-29
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:2, MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving, https://arxiv.org/pdf/2510.14557, 2025-10-16
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:3, F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs, https://arxiv.org/pdf/2510.13401, 2025-10-15
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:4, Computationally Efficient Neural Receivers via Axial Self-Attention, https://arxiv.org/pdf/2510.12941, 2025-10-14
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:5, Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs, https://arxiv.org/pdf/2510.11192, 2025-10-13
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:6, Dissecting Transformers: A CLEAR Perspective towards Green AI, https://arxiv.org/pdf/2510.02810, 2025-10-03
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:7, Microscaling Floating Point Formats for Large Language Models, https://arxiv.org/pdf/2510.01863, 2025-10-02
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:8, Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework, https://arxiv.org/pdf/2509.26548, 2025-09-30
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:9, AMLA: MUL by ADD in FlashAttention Rescaling, https://arxiv.org/pdf/2509.25224, 2025-09-24
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:10, Pretraining Large Language Models with NVFP4, https://arxiv.org/pdf/2509.25149, 2025-09-29
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:11, Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization, https://arxiv.org/pdf/2509.23202, 2025-10-16
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:12, Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs, https://arxiv.org/pdf/2509.09019, 2025-09-10
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:13, Green Learning for STAR-RIS mmWave Systems with Implicit CSI, https://arxiv.org/pdf/2509.06820, 2025-09-08
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:14, A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN, https://arxiv.org/pdf/2508.12892, 2025-10-22
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:15, SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration, https://arxiv.org/pdf/2508.12271, 2025-08-17
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:16, SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration, https://arxiv.org/pdf/2508.02069, 2025-08-18
2025-11-09 11:31:05,549 - INFO - root - Page:0, Index:17, DGEMM without FP64 Arithmetic - Using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme, https://arxiv.org/pdf/2508.00441, 2025-09-25
2025-11-09 11:31:05,556 - INFO - root - Page:0, Index:18, Scaling Probabilistic Circuits via Monarch Matrices, https://arxiv.org/pdf/2506.12383, 2025-06-14
2025-11-09 11:31:05,557 - INFO - root - Page:0, Index:19, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 11:31:05,559 - INFO - root - Page:0, Index:20, Recipes for Pre-training LLMs with MXFP8, https://arxiv.org/pdf/2506.08027, 2025-08-18
2025-11-09 11:31:05,560 - INFO - root - Page:0, Index:21, FP4 All the Way: Fully Quantized Training of LLMs, https://arxiv.org/pdf/2505.19115, 2025-08-10
2025-11-09 11:31:05,560 - INFO - root - Page:0, Index:22, Automatic Verification of Floating-Point Accumulation Networks, https://arxiv.org/pdf/2505.18791, 2025-05-24
2025-11-09 11:31:05,560 - INFO - root - Page:0, Index:23, MXDOTP: A RISC-V ISA Extension for Enabling Microscaling (MX) Floating-Point Dot Products, https://arxiv.org/pdf/2505.13159, 2025-05-19
2025-11-09 11:31:05,561 - INFO - root - Page:0, Index:24, Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors, https://arxiv.org/pdf/2505.00580, 2025-07-15
2025-11-09 11:31:05,561 - INFO - root - Page:0, Index:25, Silenzio: Secure Non-Interactive Outsourced MLP Training, https://arxiv.org/pdf/2504.17785, 2025-09-18
2025-11-09 11:31:05,561 - INFO - root - Page:0, Index:26, Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution, https://arxiv.org/pdf/2503.14779, 2025-09-08
2025-11-09 11:31:05,562 - INFO - root - Page:0, Index:27, Flopping for FLOPs: Leveraging equivariance for computational efficiency, https://arxiv.org/pdf/2502.05169, 2025-06-24
2025-11-09 11:31:05,562 - INFO - root - Page:0, Index:28, An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in Medical Image, https://arxiv.org/pdf/2409.05324, 2025-07-26
2025-11-09 11:31:05,563 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 11:31:30,452 - INFO - root - get_all_titles_from_web 
2025-11-09 11:31:30,453 - INFO - root - Page:1, Index:0, TwinLiteNet+: An Enhanced Multi-Task Segmentation Model for Autonomous Driving, https://arxiv.org/pdf/2403.16958, 2025-08-30
2025-11-09 11:31:30,453 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 11:32:32,097 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 11:33:32,864 - INFO - root - 跳过已处理论文 From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators：D:\ChatPaper\academic Papers\block float point\From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural-1.pdf
2025-11-09 11:33:32,864 - INFO - root - 跳过已处理论文 INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats：D:\ChatPaper\academic Papers\block float point\INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats-1.pdf
2025-11-09 11:33:32,864 - INFO - root - summary time: 236.15 seconds
2025-11-09 12:02:26,323 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 12:02:26,323 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 12:02:26,323 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 12:02:27,086 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 12:02:27,922 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 12:02:28,280 - WARNING - root - GeminiClient: no usable model found
2025-11-09 12:02:28,280 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 12:02:28,281 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 12:02:28,281 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 12:02:28,281 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 12:02:28,281 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 12:02:28,282 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 12:02:28,282 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 12:02:28,282 - WARNING - root - DoubaoClient: API key not provided. LLM disabled.
2025-11-09 12:02:28,282 - WARNING - root - LLMClientManager: Doubao client initialization failed
2025-11-09 12:02:28,283 - WARNING - root - LLMClientManager: no LLM client available
2025-11-09 12:02:28,283 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 12:02:28,284 - INFO - root - === 运行配置 ===
2025-11-09 12:02:28,284 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 12:02:28,285 - INFO - root - 关键词: Quant
2025-11-09 12:02:28,285 - INFO - root - 查询: block float point
2025-11-09 12:02:28,287 - INFO - root - 排序: None
2025-11-09 12:02:28,287 - INFO - root - 最近天数: 180
2025-11-09 12:02:28,288 - INFO - root - 最大处理数量: 2
2025-11-09 12:02:28,289 - INFO - root - 保存图片: 是
2025-11-09 12:02:28,289 - INFO - root - 输出语言: 中文
2025-11-09 12:02:28,289 - INFO - root - 强制重新处理: 否
2025-11-09 12:02:28,290 - INFO - root - ====================
2025-11-09 12:02:28,290 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 12:02:28,290 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 12:02:34,854 - INFO - root - get_all_titles_from_web 
2025-11-09 12:02:34,854 - INFO - root - Page:0, Index:0, From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators, https://arxiv.org/pdf/2511.00032, 2025-11-04
2025-11-09 12:02:34,854 - INFO - root - Page:0, Index:1, INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats, https://arxiv.org/pdf/2510.25602, 2025-10-29
2025-11-09 12:02:34,854 - INFO - root - Page:0, Index:2, MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving, https://arxiv.org/pdf/2510.14557, 2025-10-16
2025-11-09 12:02:34,854 - INFO - root - Page:0, Index:3, F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs, https://arxiv.org/pdf/2510.13401, 2025-10-15
2025-11-09 12:02:34,854 - INFO - root - Page:0, Index:4, Computationally Efficient Neural Receivers via Axial Self-Attention, https://arxiv.org/pdf/2510.12941, 2025-10-14
2025-11-09 12:02:34,854 - INFO - root - Page:0, Index:5, Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs, https://arxiv.org/pdf/2510.11192, 2025-10-13
2025-11-09 12:02:34,854 - INFO - root - Page:0, Index:6, Dissecting Transformers: A CLEAR Perspective towards Green AI, https://arxiv.org/pdf/2510.02810, 2025-10-03
2025-11-09 12:02:34,857 - INFO - root - Page:0, Index:7, Microscaling Floating Point Formats for Large Language Models, https://arxiv.org/pdf/2510.01863, 2025-10-02
2025-11-09 12:02:34,857 - INFO - root - Page:0, Index:8, Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework, https://arxiv.org/pdf/2509.26548, 2025-09-30
2025-11-09 12:02:34,858 - INFO - root - Page:0, Index:9, AMLA: MUL by ADD in FlashAttention Rescaling, https://arxiv.org/pdf/2509.25224, 2025-09-24
2025-11-09 12:02:34,858 - INFO - root - Page:0, Index:10, Pretraining Large Language Models with NVFP4, https://arxiv.org/pdf/2509.25149, 2025-09-29
2025-11-09 12:02:34,858 - INFO - root - Page:0, Index:11, Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization, https://arxiv.org/pdf/2509.23202, 2025-10-16
2025-11-09 12:02:34,858 - INFO - root - Page:0, Index:12, Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs, https://arxiv.org/pdf/2509.09019, 2025-09-10
2025-11-09 12:02:34,859 - INFO - root - Page:0, Index:13, Green Learning for STAR-RIS mmWave Systems with Implicit CSI, https://arxiv.org/pdf/2509.06820, 2025-09-08
2025-11-09 12:02:34,859 - INFO - root - Page:0, Index:14, A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN, https://arxiv.org/pdf/2508.12892, 2025-10-22
2025-11-09 12:02:34,859 - INFO - root - Page:0, Index:15, SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration, https://arxiv.org/pdf/2508.12271, 2025-08-17
2025-11-09 12:02:34,860 - INFO - root - Page:0, Index:16, SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration, https://arxiv.org/pdf/2508.02069, 2025-08-18
2025-11-09 12:02:34,860 - INFO - root - Page:0, Index:17, DGEMM without FP64 Arithmetic - Using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme, https://arxiv.org/pdf/2508.00441, 2025-09-25
2025-11-09 12:02:34,860 - INFO - root - Page:0, Index:18, Scaling Probabilistic Circuits via Monarch Matrices, https://arxiv.org/pdf/2506.12383, 2025-06-14
2025-11-09 12:02:34,861 - INFO - root - Page:0, Index:19, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 12:02:34,861 - INFO - root - Page:0, Index:20, Recipes for Pre-training LLMs with MXFP8, https://arxiv.org/pdf/2506.08027, 2025-08-18
2025-11-09 12:02:34,862 - INFO - root - Page:0, Index:21, FP4 All the Way: Fully Quantized Training of LLMs, https://arxiv.org/pdf/2505.19115, 2025-08-10
2025-11-09 12:02:34,862 - INFO - root - Page:0, Index:22, Automatic Verification of Floating-Point Accumulation Networks, https://arxiv.org/pdf/2505.18791, 2025-05-24
2025-11-09 12:02:34,862 - INFO - root - Page:0, Index:23, MXDOTP: A RISC-V ISA Extension for Enabling Microscaling (MX) Floating-Point Dot Products, https://arxiv.org/pdf/2505.13159, 2025-05-19
2025-11-09 12:02:34,862 - INFO - root - Page:0, Index:24, Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors, https://arxiv.org/pdf/2505.00580, 2025-07-15
2025-11-09 12:02:34,863 - INFO - root - Page:0, Index:25, Silenzio: Secure Non-Interactive Outsourced MLP Training, https://arxiv.org/pdf/2504.17785, 2025-09-18
2025-11-09 12:02:34,863 - INFO - root - Page:0, Index:26, Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution, https://arxiv.org/pdf/2503.14779, 2025-09-08
2025-11-09 12:02:34,863 - INFO - root - Page:0, Index:27, Flopping for FLOPs: Leveraging equivariance for computational efficiency, https://arxiv.org/pdf/2502.05169, 2025-06-24
2025-11-09 12:02:34,863 - INFO - root - Page:0, Index:28, An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in Medical Image, https://arxiv.org/pdf/2409.05324, 2025-07-26
2025-11-09 12:02:34,864 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 12:02:41,042 - INFO - root - get_all_titles_from_web 
2025-11-09 12:02:41,042 - INFO - root - Page:1, Index:0, TwinLiteNet+: An Enhanced Multi-Task Segmentation Model for Autonomous Driving, https://arxiv.org/pdf/2403.16958, 2025-08-30
2025-11-09 12:02:41,043 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=block+float+point&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 12:02:47,533 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 12:02:59,831 - INFO - root - 正在总结论文 1/2: From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators
2025-11-09 12:02:59,835 - INFO - root - 正在提取论文图片...
2025-11-09 12:03:00,382 - INFO - root - 已保存图片 1/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_1_page3.png
2025-11-09 12:03:00,553 - INFO - root - 已保存图片 2/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_2_page8.png
2025-11-09 12:03:00,645 - INFO - root - 已保存图片 3/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_3_page8.png
2025-11-09 12:03:00,721 - INFO - root - 已保存图片 4/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_4_page9.png
2025-11-09 12:03:00,827 - INFO - root - 已保存图片 5/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_5_page9.png
2025-11-09 12:03:00,943 - INFO - root - 已保存图片 6/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_6_page4.png
2025-11-09 12:03:01,103 - INFO - root - 已保存图片 7/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_7_page4.png
2025-11-09 12:03:01,219 - INFO - root - 已保存图片 8/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_8_page16.png
2025-11-09 12:03:01,246 - INFO - root - 已保存图片 9/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_9_page4.png
2025-11-09 12:03:01,272 - INFO - root - 已保存图片 10/10：./export\images_From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural\figure_10_page4.png
2025-11-09 12:03:01,284 - INFO - root - 输出文件已存在，跳过论文 From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators 的处理
2025-11-09 12:03:01,286 - INFO - root - 正在总结论文 2/2: INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats
2025-11-09 12:03:01,287 - INFO - root - 正在提取论文图片...
2025-11-09 12:03:02,061 - INFO - root - 已保存图片 1/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_1_page9.png
2025-11-09 12:03:02,238 - INFO - root - 已保存图片 2/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_2_page1.png
2025-11-09 12:03:02,291 - INFO - root - 已保存图片 3/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_3_page1.jpeg
2025-11-09 12:03:02,361 - INFO - root - 已保存图片 4/10：./export\images_INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats\figure_4_page9.png
2025-11-09 12:03:02,364 - INFO - root - 输出文件已存在，跳过论文 INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats 的处理
2025-11-09 12:03:02,364 - INFO - root - summary time: 36.04 seconds
2025-11-09 12:40:27,534 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 12:40:27,541 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 12:40:27,546 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 12:40:28,502 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 12:40:29,436 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 12:40:29,790 - WARNING - root - GeminiClient: no usable model found
2025-11-09 12:40:29,791 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 12:40:29,791 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 12:40:29,791 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 12:40:29,791 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 12:40:29,792 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 12:40:29,792 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 12:40:29,792 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 12:40:29,792 - WARNING - root - DoubaoClient: API key not provided. LLM disabled.
2025-11-09 12:40:29,793 - WARNING - root - LLMClientManager: Doubao client initialization failed
2025-11-09 12:40:29,793 - WARNING - root - LLMClientManager: no LLM client available
2025-11-09 12:40:29,793 - WARNING - root - LLMClientManager: client Gemini not available
2025-11-09 12:40:29,794 - WARNING - root - 无法切换到指定的客户端 Gemini，将使用默认客户端
2025-11-09 12:40:29,794 - INFO - root - 可用客户端: []
2025-11-09 12:40:29,795 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 12:40:29,795 - INFO - root - === 运行配置 ===
2025-11-09 12:40:29,795 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 12:40:29,796 - INFO - root - PDF目录: ./myPapers
2025-11-09 12:40:29,796 - INFO - root - 最大处理数量: 2
2025-11-09 12:40:29,797 - INFO - root - 保存图片: 是
2025-11-09 12:40:29,799 - INFO - root - 输出语言: 中文
2025-11-09 12:40:29,799 - INFO - root - 强制重新处理: 否
2025-11-09 12:40:29,800 - INFO - root - ====================
2025-11-09 12:40:29,800 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 12:40:32,678 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 12:40:35,064 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 12:40:35,890 - INFO - root - 成功加载PDF文件：An Effective UNet Using Feature Interaction and Fusion for Organ Segmentation in.pdf
2025-11-09 12:42:15,377 - INFO - root - 成功加载PDF文件：Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via.pdf
2025-11-09 12:42:16,352 - INFO - root - 成功加载PDF文件：Automatic Verification of Floating-Point Accumulation Networks.pdf
2025-11-09 12:42:17,772 - INFO - root - 成功加载PDF文件：Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantizati.pdf
2025-11-09 12:42:20,130 - INFO - root - 成功加载PDF文件：Computationally Efficient Neural Receivers via Axial Self-Attention.pdf
2025-11-09 12:42:21,023 - INFO - root - 成功加载PDF文件：DGEMM without FP64 Arithmetic - Using FP64 Emulation and FP8 Tensor Cores with O.pdf
2025-11-09 12:42:23,369 - INFO - root - 成功加载PDF文件：Dissecting Transformers_ A CLEAR Perspective towards Green AI.pdf
2025-11-09 12:42:23,819 - INFO - root - 成功加载PDF文件：Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs.pdf
2025-11-09 12:42:26,826 - INFO - root - 成功加载PDF文件：F-BFQ_ Flexible Block Floating-Point Quantization Accelerator for LLMs.pdf
2025-11-09 12:42:28,662 - INFO - root - 成功加载PDF文件：Flopping for FLOPs_ Leveraging equivariance for computational efficiency.pdf
2025-11-09 12:42:29,354 - INFO - root - 成功加载PDF文件：FP4 All the Way_ Fully Quantized Training of LLMs.pdf
2025-11-09 12:42:30,794 - INFO - root - 成功加载PDF文件：From Uniform to Adaptive_ General Skip-Block Mechanisms for Efficient PDE Neural.pdf
2025-11-09 12:42:31,478 - INFO - root - 成功加载PDF文件：Green Learning for STAR-RIS mmWave Systems with Implicit CSI.pdf
2025-11-09 12:42:38,123 - INFO - root - 成功加载PDF文件：INT v.s. FP_ A Comprehensive Study of Fine-Grained Low-bit Quantization Formats.pdf
2025-11-09 12:42:38,686 - INFO - root - 成功加载PDF文件：Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Sup.pdf
2025-11-09 12:42:39,128 - INFO - root - 成功加载PDF文件：Microscaling Floating Point Formats for Large Language Models.pdf
2025-11-09 12:42:39,967 - INFO - root - 成功加载PDF文件：MX+_ Pushing the Limits of Microscaling Formats for Efficient Large Language Mod.pdf
2025-11-09 12:42:40,332 - INFO - root - 成功加载PDF文件：MXDOTP_ A RISC-V ISA Extension for Enabling Microscaling (MX) Floating-Point Dot.pdf
2025-11-09 12:42:40,665 - INFO - root - 成功加载PDF文件：Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors.pdf
2025-11-09 12:42:42,370 - INFO - root - 成功加载PDF文件：Pretraining Large Language Models with NVFP4.pdf
2025-11-09 12:42:42,773 - INFO - root - 成功加载PDF文件：Recipes for Pre-training LLMs with MXFP8.pdf
2025-11-09 12:42:44,763 - INFO - root - 成功加载PDF文件：Scaling Probabilistic Circuits via Monarch Matrices.pdf
2025-11-09 12:42:44,776 - ERROR - root - 处理PDF文件 Silenzio_ Secure Non-Interactive Outsourced MLP Training.pdf 时出错：no such file: './myPapers\Silenzio_ Secure Non-Interactive Outsourced MLP Training.pdf'
2025-11-09 12:42:44,778 - ERROR - root - 处理PDF文件 SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration.pdf 时出错：no such file: './myPapers\SNNSIR_ A Simple Spiking Neural Network for Stereo Image Restoration.pdf'
2025-11-09 12:42:44,909 - ERROR - root - 处理PDF文件 SpikeSTAG_ Spatial-Temporal Forecasting via GNN-SNN Collaboration.pdf 时出错：no such file: './myPapers\SpikeSTAG_ Spatial-Temporal Forecasting via GNN-SNN Collaboration.pdf'
2025-11-09 12:42:44,940 - ERROR - root - 处理PDF文件 Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne.pdf 时出错：no such file: './myPapers\Starting Positions Matter_ A Study on Better Weight Initialization for Neural Ne.pdf'
2025-11-09 12:42:44,984 - ERROR - root - 处理PDF文件 Towards Verified Compilation of Floating-point Optimization in Scientific Comput.pdf 时出错：no such file: './myPapers\Towards Verified Compilation of Floating-point Optimization in Scientific Comput.pdf'
2025-11-09 12:42:44,994 - ERROR - root - 处理PDF文件 TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving.pdf 时出错：no such file: './myPapers\TwinLiteNet+_ An Enhanced Multi-Task Segmentation Model for Autonomous Driving.pdf'
2025-11-09 12:42:45,008 - INFO - root - 正在总结论文 1/24: A Compute&Memory Efficient Model-Driven Neural
2025-11-09 12:42:45,010 - INFO - root - 正在提取论文图片...
2025-11-09 12:42:46,858 - INFO - root - 已保存图片 1/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_1_page6.png
2025-11-09 12:42:47,327 - INFO - root - 已保存图片 2/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_2_page6.png
2025-11-09 12:42:47,562 - INFO - root - 已保存图片 3/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_3_page2.png
2025-11-09 12:42:47,639 - INFO - root - 已保存图片 4/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_4_page2.png
2025-11-09 12:42:47,689 - INFO - root - 已保存图片 5/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_5_page3.png
2025-11-09 12:42:47,702 - INFO - root - 成功添加图片 1：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_1_page6.png
2025-11-09 12:42:47,702 - INFO - root - 成功添加图片 2：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_2_page6.png
2025-11-09 12:42:47,703 - INFO - root - 成功添加图片 3：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_3_page2.png
2025-11-09 12:42:47,703 - INFO - root - 成功添加图片 4：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_4_page2.png
2025-11-09 12:42:47,703 - INFO - root - 成功添加图片 5：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_5_page3.png
2025-11-09 12:42:47,711 - INFO - root - 论文《A Compute&Memory Efficient Model-Driven Neural》的分析已保存到 ./export\A Compute&Memory Efficient Model-Driven Neural.md
2025-11-09 12:42:47,729 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 12:42:47,730 - INFO - root - 正在总结论文 3/24: An Effective UNet Using Feature Interaction and Fusion for Organ
2025-11-09 12:42:47,737 - INFO - root - 正在提取论文图片...
2025-11-09 12:42:48,558 - INFO - root - 已保存图片 1/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_1_page14.png
2025-11-09 12:42:48,712 - INFO - root - 已保存图片 2/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_2_page19.jpeg
2025-11-09 12:42:48,812 - INFO - root - 已保存图片 3/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_3_page19.jpeg
2025-11-09 12:42:48,931 - INFO - root - 已保存图片 4/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_4_page19.jpeg
2025-11-09 12:42:49,105 - INFO - root - 已保存图片 5/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_5_page19.jpeg
2025-11-09 12:42:49,209 - INFO - root - 已保存图片 6/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_6_page19.jpeg
2025-11-09 12:42:49,478 - INFO - root - 已保存图片 7/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_7_page19.jpeg
2025-11-09 12:42:49,706 - INFO - root - 已保存图片 8/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_8_page19.jpeg
2025-11-09 12:42:49,794 - INFO - root - 已保存图片 9/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_9_page19.jpeg
2025-11-09 12:42:49,961 - INFO - root - 已保存图片 10/10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_10_page19.jpeg
2025-11-09 12:42:49,966 - INFO - root - 成功添加图片 1：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_1_page14.png
2025-11-09 12:42:49,967 - INFO - root - 成功添加图片 2：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_2_page19.jpeg
2025-11-09 12:42:49,969 - INFO - root - 成功添加图片 3：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_3_page19.jpeg
2025-11-09 12:42:49,977 - INFO - root - 成功添加图片 4：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_4_page19.jpeg
2025-11-09 12:42:49,979 - INFO - root - 成功添加图片 5：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_5_page19.jpeg
2025-11-09 12:42:49,982 - INFO - root - 成功添加图片 6：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_6_page19.jpeg
2025-11-09 12:42:49,985 - INFO - root - 成功添加图片 7：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_7_page19.jpeg
2025-11-09 12:42:49,993 - INFO - root - 成功添加图片 8：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_8_page19.jpeg
2025-11-09 12:42:49,996 - INFO - root - 成功添加图片 9：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_9_page19.jpeg
2025-11-09 12:42:50,011 - INFO - root - 成功添加图片 10：./export\images_An Effective UNet Using Feature Interaction and Fusion for Organ\figure_10_page19.jpeg
2025-11-09 12:42:50,030 - INFO - root - 论文《An Effective UNet Using Feature Interaction and Fusion for Organ》的分析已保存到 ./export\An Effective UNet Using Feature Interaction and Fusion for Organ.md
2025-11-09 12:42:50,076 - INFO - root - 正在总结论文 4/24: Automated and Scalable SEM Image Analysis of Perovskite Solar Cell
2025-11-09 12:42:50,095 - INFO - root - 正在提取论文图片...
2025-11-09 12:43:28,803 - INFO - root - 已保存图片 1/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_1_page8.jpeg
2025-11-09 12:43:29,124 - INFO - root - 已保存图片 2/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_2_page5.jpeg
2025-11-09 12:43:29,434 - INFO - root - 已保存图片 3/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_3_page9.jpeg
2025-11-09 12:43:29,639 - INFO - root - 已保存图片 4/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_4_page9.jpeg
2025-11-09 12:43:29,842 - INFO - root - 已保存图片 5/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_5_page9.jpeg
2025-11-09 12:43:30,072 - INFO - root - 已保存图片 6/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_6_page9.jpeg
2025-11-09 12:43:30,284 - INFO - root - 已保存图片 7/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_7_page9.jpeg
2025-11-09 12:43:30,485 - INFO - root - 已保存图片 8/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_8_page9.jpeg
2025-11-09 12:43:30,688 - INFO - root - 已保存图片 9/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_9_page9.jpeg
2025-11-09 12:43:30,896 - INFO - root - 已保存图片 10/10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_10_page9.jpeg
2025-11-09 12:43:30,919 - INFO - root - 成功添加图片 1：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_1_page8.jpeg
2025-11-09 12:43:30,920 - INFO - root - 成功添加图片 2：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_2_page5.jpeg
2025-11-09 12:43:30,921 - INFO - root - 成功添加图片 3：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_3_page9.jpeg
2025-11-09 12:43:30,921 - INFO - root - 成功添加图片 4：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_4_page9.jpeg
2025-11-09 12:43:30,922 - INFO - root - 成功添加图片 5：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_5_page9.jpeg
2025-11-09 12:43:30,922 - INFO - root - 成功添加图片 6：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_6_page9.jpeg
2025-11-09 12:43:30,922 - INFO - root - 成功添加图片 7：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_7_page9.jpeg
2025-11-09 12:43:30,923 - INFO - root - 成功添加图片 8：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_8_page9.jpeg
2025-11-09 12:43:30,923 - INFO - root - 成功添加图片 9：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_9_page9.jpeg
2025-11-09 12:43:30,925 - INFO - root - 成功添加图片 10：./export\images_Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\figure_10_page9.jpeg
2025-11-09 12:43:30,932 - INFO - root - 论文《Automated and Scalable SEM Image Analysis of Perovskite Solar Cell》的分析已保存到 ./export\Automated and Scalable SEM Image Analysis of Perovskite Solar Cell.md
2025-11-09 12:43:30,948 - INFO - root - 正在总结论文 5/24: Automatic Verification of
2025-11-09 12:43:30,950 - INFO - root - 正在提取论文图片...
2025-11-09 12:43:30,989 - INFO - root - 论文《Automatic Verification of》的分析已保存到 ./export\Automatic Verification of.md
2025-11-09 12:43:31,005 - INFO - root - 正在总结论文 6/24: 
2025-11-09 12:43:31,014 - INFO - root - 正在提取论文图片...
2025-11-09 12:43:31,371 - INFO - root - 已保存图片 1/10：./export\images_untitled\figure_1_page25.png
2025-11-09 12:43:31,371 - INFO - root - 成功添加图片 1：./export\images_untitled\figure_1_page25.png
2025-11-09 12:43:31,386 - INFO - root - 论文《》的分析已保存到 ./export\traffic flow prediction.md
2025-11-09 12:43:31,402 - INFO - root - 正在总结论文 7/24: Computationally Efficient Neural Receivers via
2025-11-09 12:43:31,409 - INFO - root - 正在提取论文图片...
2025-11-09 12:43:32,644 - INFO - root - 已保存图片 1/10：./export\images_Computationally Efficient Neural Receivers via\figure_1_page5.png
2025-11-09 12:43:32,650 - INFO - root - 成功添加图片 1：./export\images_Computationally Efficient Neural Receivers via\figure_1_page5.png
2025-11-09 12:43:32,655 - INFO - root - 论文《Computationally Efficient Neural Receivers via》的分析已保存到 ./export\Computationally Efficient Neural Receivers via.md
2025-11-09 12:43:32,660 - INFO - root - 正在总结论文 8/24: DGEMM without FP64 Arithmetic – Using FP64 Emulation and
2025-11-09 12:43:32,666 - INFO - root - 正在提取论文图片...
2025-11-09 12:45:56,822 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 12:45:56,824 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 12:45:56,826 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 12:45:57,544 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 12:45:58,503 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 12:45:58,863 - WARNING - root - GeminiClient: no usable model found
2025-11-09 12:45:58,863 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 12:45:58,864 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 12:45:58,864 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 12:45:58,864 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 12:45:58,865 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 12:45:58,865 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 12:45:58,866 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 12:45:58,866 - WARNING - root - DoubaoClient: API key not provided. LLM disabled.
2025-11-09 12:45:58,866 - WARNING - root - LLMClientManager: Doubao client initialization failed
2025-11-09 12:45:58,867 - WARNING - root - LLMClientManager: no LLM client available
2025-11-09 12:45:58,867 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 12:45:58,867 - INFO - root - === 运行配置 ===
2025-11-09 12:45:58,867 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 12:45:58,868 - INFO - root - PDF目录: ./myPapers
2025-11-09 12:45:58,869 - INFO - root - 最大处理数量: 2
2025-11-09 12:45:58,869 - INFO - root - 保存图片: 是
2025-11-09 12:45:58,869 - INFO - root - 输出语言: 中文
2025-11-09 12:45:58,870 - INFO - root - 强制重新处理: 否
2025-11-09 12:45:58,870 - INFO - root - ====================
2025-11-09 12:45:58,870 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 12:46:00,328 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 12:46:02,886 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 12:46:02,886 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 12:46:02,887 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 12:46:02,889 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 12:46:02,890 - INFO - root - summary time: 6.07 seconds
2025-11-09 12:46:38,191 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 12:46:38,192 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 12:46:38,194 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 12:46:38,778 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 12:46:39,436 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 12:46:39,558 - WARNING - root - GeminiClient: no usable model found
2025-11-09 12:46:39,560 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 12:46:39,560 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 12:46:39,561 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 12:46:39,562 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 12:46:39,562 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 12:46:39,563 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 12:46:39,563 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 12:46:40,213 - ERROR - root - DoubaoClient: API test failed with status 404
2025-11-09 12:46:40,215 - WARNING - root - LLMClientManager: Doubao client initialization failed
2025-11-09 12:46:40,215 - WARNING - root - LLMClientManager: no LLM client available
2025-11-09 12:46:40,215 - WARNING - root - LLMClientManager: client Doubao not available
2025-11-09 12:46:40,215 - WARNING - root - 无法切换到指定的客户端 Doubao，将使用默认客户端
2025-11-09 12:46:40,217 - INFO - root - 可用客户端: []
2025-11-09 12:46:40,217 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 12:46:40,218 - INFO - root - === 运行配置 ===
2025-11-09 12:46:40,218 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 12:46:40,221 - INFO - root - PDF目录: ./myPapers
2025-11-09 12:46:40,222 - INFO - root - 最大处理数量: 2
2025-11-09 12:46:40,223 - INFO - root - 保存图片: 是
2025-11-09 12:46:40,224 - INFO - root - 输出语言: 中文
2025-11-09 12:46:40,224 - INFO - root - 强制重新处理: 否
2025-11-09 12:46:40,224 - INFO - root - ====================
2025-11-09 12:46:40,226 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 12:46:41,906 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 12:46:45,400 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 12:46:45,400 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 12:46:45,401 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 12:46:45,402 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 12:46:45,402 - INFO - root - summary time: 7.21 seconds
2025-11-09 12:49:23,218 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 12:49:23,219 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 12:49:23,221 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 12:49:23,805 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 12:49:24,666 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 12:49:25,022 - WARNING - root - GeminiClient: no usable model found
2025-11-09 12:49:25,022 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 12:49:25,022 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 12:49:25,022 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 12:49:25,023 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 12:49:25,023 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 12:49:25,023 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 12:49:25,024 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 12:49:25,491 - ERROR - root - DoubaoClient: API test failed with status 404
2025-11-09 12:49:25,493 - WARNING - root - LLMClientManager: Doubao client initialization failed
2025-11-09 12:49:25,493 - WARNING - root - LLMClientManager: no LLM client available
2025-11-09 12:49:25,494 - WARNING - root - LLMClientManager: client Doubao not available
2025-11-09 12:49:25,494 - WARNING - root - 无法切换到指定的客户端 Doubao，将使用默认客户端
2025-11-09 12:49:25,494 - INFO - root - 可用客户端: []
2025-11-09 12:49:25,495 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 12:49:25,495 - INFO - root - === 运行配置 ===
2025-11-09 12:49:25,495 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 12:49:25,495 - INFO - root - PDF目录: ./myPapers
2025-11-09 12:49:25,495 - INFO - root - 最大处理数量: 2
2025-11-09 12:49:25,496 - INFO - root - 保存图片: 是
2025-11-09 12:49:25,497 - INFO - root - 输出语言: 中文
2025-11-09 12:49:25,497 - INFO - root - 强制重新处理: 否
2025-11-09 12:49:25,497 - INFO - root - ====================
2025-11-09 12:49:25,497 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 12:49:26,935 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 12:49:28,995 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 12:49:28,996 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 12:49:28,997 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 12:49:28,997 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 12:49:28,999 - INFO - root - summary time: 5.78 seconds
2025-11-09 12:58:12,607 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 12:58:12,611 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 12:58:12,613 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 12:58:13,285 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 12:58:14,129 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 12:58:14,477 - WARNING - root - GeminiClient: no usable model found
2025-11-09 12:58:14,478 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 12:58:14,478 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 12:58:14,478 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 12:58:14,479 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 12:58:14,479 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 12:58:14,480 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 12:58:14,480 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 12:58:15,026 - ERROR - root - DoubaoClient: API test failed with status 404
2025-11-09 12:58:15,030 - WARNING - root - LLMClientManager: Doubao client initialization failed
2025-11-09 12:58:15,031 - WARNING - root - LLMClientManager: no LLM client available
2025-11-09 12:58:15,031 - WARNING - root - LLMClientManager: client Doubao not available
2025-11-09 12:58:15,031 - WARNING - root - 无法切换到指定的客户端 Doubao，将使用默认客户端
2025-11-09 12:58:15,033 - INFO - root - 可用客户端: []
2025-11-09 12:58:15,033 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 12:58:15,034 - INFO - root - === 运行配置 ===
2025-11-09 12:58:15,034 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 12:58:15,034 - INFO - root - PDF目录: ./myPapers
2025-11-09 12:58:15,035 - INFO - root - 最大处理数量: 2
2025-11-09 12:58:15,035 - INFO - root - 保存图片: 是
2025-11-09 12:58:15,035 - INFO - root - 输出语言: 中文
2025-11-09 12:58:15,036 - INFO - root - 强制重新处理: 否
2025-11-09 12:58:15,036 - INFO - root - ====================
2025-11-09 12:58:15,036 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 12:58:16,829 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 12:58:19,740 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 12:58:19,741 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 12:58:19,747 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 12:58:19,747 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 12:58:19,748 - INFO - root - summary time: 7.14 seconds
2025-11-09 13:05:28,941 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 13:05:28,942 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 13:05:28,944 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 13:05:28,970 - INFO - root - LLMClientManager: 指定使用客户端: Doubao
2025-11-09 13:05:29,528 - ERROR - root - DoubaoClient: API test failed with status 404
2025-11-09 13:05:29,529 - ERROR - root - LLMClientManager: 指定的客户端 Doubao 初始化失败
2025-11-09 13:05:29,529 - WARNING - root - LLMClientManager: 指定的客户端 Doubao 不可用，将尝试其他客户端
2025-11-09 13:05:30,291 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 13:05:31,203 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 13:05:31,564 - WARNING - root - GeminiClient: no usable model found
2025-11-09 13:05:31,565 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 13:05:31,565 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 13:05:31,565 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 13:05:31,566 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 13:05:31,566 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 13:05:31,566 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 13:05:31,566 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 13:05:31,567 - WARNING - root - LLMClientManager: no LLM client available
2025-11-09 13:05:31,567 - WARNING - root - LLMClientManager: client Doubao not available
2025-11-09 13:05:31,568 - WARNING - root - 无法切换到指定的客户端 Doubao，将使用默认客户端
2025-11-09 13:05:31,568 - INFO - root - 可用客户端: []
2025-11-09 13:05:31,568 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 13:05:31,568 - INFO - root - === 运行配置 ===
2025-11-09 13:05:31,569 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 13:05:31,569 - INFO - root - PDF目录: ./myPapers
2025-11-09 13:05:31,569 - INFO - root - 最大处理数量: 2
2025-11-09 13:05:31,569 - INFO - root - 保存图片: 是
2025-11-09 13:05:31,570 - INFO - root - 输出语言: 中文
2025-11-09 13:05:31,570 - INFO - root - 强制重新处理: 否
2025-11-09 13:05:31,571 - INFO - root - ====================
2025-11-09 13:05:31,572 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 13:05:33,240 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:05:35,435 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:05:35,435 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 13:05:35,436 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:05:35,437 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:05:35,438 - INFO - root - summary time: 6.50 seconds
2025-11-09 13:09:04,133 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 13:09:04,152 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 13:09:04,154 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 13:09:04,321 - INFO - root - LLMClientManager: 指定使用客户端: Doubao
2025-11-09 13:09:06,618 - ERROR - root - DoubaoClient: API test failed with status 404
2025-11-09 13:09:06,631 - ERROR - root - LLMClientManager: 指定的客户端 Doubao 初始化失败
2025-11-09 13:09:06,644 - WARNING - root - LLMClientManager: 指定的客户端 Doubao 不可用，将尝试其他客户端
2025-11-09 13:09:08,688 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 13:09:09,712 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 13:09:10,067 - WARNING - root - GeminiClient: no usable model found
2025-11-09 13:09:10,075 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 13:09:10,076 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 13:09:10,077 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 13:09:10,079 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 13:09:10,080 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 13:09:10,080 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 13:09:10,081 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 13:09:10,082 - WARNING - root - LLMClientManager: no LLM client available
2025-11-09 13:09:10,082 - WARNING - root - LLMClientManager: client Doubao not available
2025-11-09 13:09:10,098 - WARNING - root - 无法切换到指定的客户端 Doubao，将使用默认客户端
2025-11-09 13:09:10,101 - INFO - root - 可用客户端: []
2025-11-09 13:09:10,107 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 13:09:10,117 - INFO - root - === 运行配置 ===
2025-11-09 13:09:10,123 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 13:09:10,126 - INFO - root - PDF目录: ./myPapers
2025-11-09 13:09:10,135 - INFO - root - 最大处理数量: 2
2025-11-09 13:09:10,143 - INFO - root - 保存图片: 是
2025-11-09 13:09:10,146 - INFO - root - 输出语言: 中文
2025-11-09 13:09:10,149 - INFO - root - 强制重新处理: 否
2025-11-09 13:09:10,151 - INFO - root - ====================
2025-11-09 13:09:10,158 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 13:09:15,684 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:09:19,567 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:09:19,568 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 13:09:19,572 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:09:19,573 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:09:19,574 - INFO - root - summary time: 15.44 seconds
2025-11-09 13:14:46,332 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 13:14:46,334 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 13:14:46,337 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 13:14:49,446 - INFO - root - LLMClientManager: 指定使用客户端: Doubao
2025-11-09 13:14:49,446 - WARNING - root - DoubaoClient: API key not provided. LLM disabled.
2025-11-09 13:14:49,446 - ERROR - root - LLMClientManager: 指定的客户端 Doubao 初始化失败
2025-11-09 13:14:49,446 - WARNING - root - LLMClientManager: 指定的客户端 Doubao 不可用，将尝试其他客户端
2025-11-09 13:14:50,249 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 13:14:51,097 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 13:14:51,453 - WARNING - root - GeminiClient: no usable model found
2025-11-09 13:14:51,454 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 13:14:51,455 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 13:14:51,455 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 13:14:51,456 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 13:14:51,456 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 13:14:51,456 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 13:14:51,457 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 13:14:51,457 - WARNING - root - LLMClientManager: no LLM client available
2025-11-09 13:14:51,458 - WARNING - root - LLMClientManager: client Doubao not available
2025-11-09 13:14:51,458 - WARNING - root - 无法切换到指定的客户端 Doubao，将使用默认客户端
2025-11-09 13:14:51,458 - INFO - root - 可用客户端: []
2025-11-09 13:14:51,459 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 13:14:51,459 - INFO - root - === 运行配置 ===
2025-11-09 13:14:51,460 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 13:14:51,461 - INFO - root - PDF目录: ./myPapers
2025-11-09 13:14:51,461 - INFO - root - 最大处理数量: 2
2025-11-09 13:14:51,463 - INFO - root - 保存图片: 是
2025-11-09 13:14:51,463 - INFO - root - 输出语言: 中文
2025-11-09 13:14:51,464 - INFO - root - 强制重新处理: 否
2025-11-09 13:14:51,465 - INFO - root - ====================
2025-11-09 13:14:51,466 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 13:14:53,801 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:14:57,267 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:14:57,268 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 13:14:57,277 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:14:57,279 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:14:57,279 - INFO - root - summary time: 10.95 seconds
2025-11-09 13:15:29,541 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 13:15:29,543 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 13:15:29,543 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 13:15:30,631 - INFO - root - LLMClientManager: 指定使用客户端: Doubao
2025-11-09 13:15:30,636 - WARNING - root - DoubaoClient: API key not provided. LLM disabled.
2025-11-09 13:15:30,636 - ERROR - root - LLMClientManager: 指定的客户端 Doubao 初始化失败
2025-11-09 13:15:30,636 - WARNING - root - LLMClientManager: 指定的客户端 Doubao 不可用，将尝试其他客户端
2025-11-09 13:15:31,714 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 13:15:32,394 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 13:15:32,518 - WARNING - root - GeminiClient: no usable model found
2025-11-09 13:15:32,519 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 13:15:32,520 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 13:15:32,521 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 13:15:32,522 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 13:15:32,522 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 13:15:32,523 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 13:15:32,523 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 13:15:32,523 - WARNING - root - LLMClientManager: no LLM client available
2025-11-09 13:15:32,524 - WARNING - root - LLMClientManager: client Doubao not available
2025-11-09 13:15:32,524 - WARNING - root - 无法切换到指定的客户端 Doubao，将使用默认客户端
2025-11-09 13:15:32,524 - INFO - root - 可用客户端: []
2025-11-09 13:15:32,525 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 13:15:32,525 - INFO - root - === 运行配置 ===
2025-11-09 13:15:32,525 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 13:15:32,526 - INFO - root - PDF目录: ./myPapers
2025-11-09 13:15:32,526 - INFO - root - 最大处理数量: 2
2025-11-09 13:15:32,527 - INFO - root - 保存图片: 是
2025-11-09 13:15:32,527 - INFO - root - 输出语言: 中文
2025-11-09 13:15:32,528 - INFO - root - 强制重新处理: 否
2025-11-09 13:15:32,528 - INFO - root - ====================
2025-11-09 13:15:32,529 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 13:15:34,692 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:15:37,219 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:15:37,219 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 13:15:37,220 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:15:37,220 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:15:37,222 - INFO - root - summary time: 7.68 seconds
2025-11-09 13:17:34,011 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 13:17:34,012 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 13:17:34,014 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 13:17:35,164 - INFO - root - LLMClientManager: 指定使用客户端: Doubao
2025-11-09 13:17:35,164 - ERROR - root - DoubaoClient: error reading config: RawConfigParser.get() takes 3 positional arguments but 4 were given
2025-11-09 13:17:35,165 - WARNING - root - DoubaoClient: API key not provided or using placeholder. LLM disabled.
2025-11-09 13:17:35,165 - ERROR - root - LLMClientManager: 指定的客户端 Doubao 初始化失败
2025-11-09 13:17:35,165 - WARNING - root - LLMClientManager: 指定的客户端 Doubao 不可用，将尝试其他客户端
2025-11-09 13:17:35,805 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 13:17:36,828 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 13:17:37,191 - WARNING - root - GeminiClient: no usable model found
2025-11-09 13:17:37,191 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 13:17:37,192 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 13:17:37,192 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 13:17:37,193 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 13:17:37,193 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 13:17:37,193 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 13:17:37,194 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 13:17:37,194 - WARNING - root - LLMClientManager: no LLM client available
2025-11-09 13:17:37,194 - WARNING - root - LLMClientManager: client Doubao not available
2025-11-09 13:17:37,195 - WARNING - root - 无法切换到指定的客户端 Doubao，将使用默认客户端
2025-11-09 13:17:37,195 - INFO - root - 可用客户端: []
2025-11-09 13:17:37,196 - INFO - root - LLM 未初始化或不可用，后续生成将返回备用消息
2025-11-09 13:17:37,196 - INFO - root - === 运行配置 ===
2025-11-09 13:17:37,203 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 13:17:37,213 - INFO - root - PDF目录: ./myPapers
2025-11-09 13:17:37,219 - INFO - root - 最大处理数量: 2
2025-11-09 13:17:37,220 - INFO - root - 保存图片: 是
2025-11-09 13:17:37,224 - INFO - root - 输出语言: 中文
2025-11-09 13:17:37,225 - INFO - root - 强制重新处理: 否
2025-11-09 13:17:37,228 - INFO - root - ====================
2025-11-09 13:17:37,230 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 13:17:39,662 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:17:42,426 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:17:42,426 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 13:17:42,426 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:17:42,426 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:17:42,426 - INFO - root - summary time: 8.42 seconds
2025-11-09 13:19:00,129 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 13:19:00,131 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 13:19:00,135 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 13:19:01,214 - INFO - root - LLMClientManager: 指定使用客户端: Doubao
2025-11-09 13:19:01,214 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 13:19:07,220 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 13:19:07,240 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 13:19:07,241 - INFO - root - LLMClientManager: Doubao 客户端初始化成功
2025-11-09 13:19:07,241 - INFO - root - LLMClientManager: switched to Doubao client
2025-11-09 13:19:07,241 - INFO - root - 已手动切换到 LLM 客户端: Doubao
2025-11-09 13:19:07,242 - INFO - root - 使用 LLM 模型: doubao-seed-1-6-lite-251015
2025-11-09 13:19:07,242 - INFO - root - 可用客户端: ['Doubao']
2025-11-09 13:19:07,242 - INFO - root - === 运行配置 ===
2025-11-09 13:19:07,243 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 13:19:07,245 - INFO - root - PDF目录: ./myPapers
2025-11-09 13:19:07,246 - INFO - root - 最大处理数量: 2
2025-11-09 13:19:07,249 - INFO - root - 保存图片: 是
2025-11-09 13:19:07,252 - INFO - root - 输出语言: 中文
2025-11-09 13:19:07,253 - INFO - root - 强制重新处理: 否
2025-11-09 13:19:07,260 - INFO - root - ====================
2025-11-09 13:19:07,263 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 13:19:09,141 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:19:11,769 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:19:11,769 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 13:19:11,772 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:19:11,772 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:19:11,773 - INFO - root - summary time: 11.65 seconds
2025-11-09 13:21:37,150 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 13:21:37,176 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 13:21:37,179 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 13:21:38,652 - INFO - root - LLMClientManager: 指定使用客户端: Doubao
2025-11-09 13:21:38,653 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 13:21:43,494 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 13:21:43,681 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 13:21:43,821 - INFO - root - LLMClientManager: Doubao 客户端初始化成功
2025-11-09 13:21:43,871 - INFO - root - LLMClientManager: switched to Doubao client
2025-11-09 13:21:43,897 - INFO - root - 已手动切换到 LLM 客户端: Doubao
2025-11-09 13:21:43,971 - INFO - root - 使用 LLM 模型: doubao-seed-1-6-lite-251015
2025-11-09 13:21:43,993 - INFO - root - 可用客户端: ['Doubao']
2025-11-09 13:21:44,032 - INFO - root - === 运行配置 ===
2025-11-09 13:21:44,100 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 13:21:44,117 - INFO - root - PDF目录: ./myPapers
2025-11-09 13:21:44,147 - INFO - root - 最大处理数量: 2
2025-11-09 13:21:44,186 - INFO - root - 保存图片: 是
2025-11-09 13:21:44,233 - INFO - root - 输出语言: 中文
2025-11-09 13:21:44,238 - INFO - root - 强制重新处理: 否
2025-11-09 13:21:44,244 - INFO - root - ====================
2025-11-09 13:21:44,250 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 13:21:46,096 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:21:49,176 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:21:49,178 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 13:21:49,179 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:21:49,179 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:21:49,179 - INFO - root - summary time: 12.03 seconds
2025-11-09 13:22:41,686 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 13:22:41,687 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 13:22:41,688 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 13:22:43,552 - INFO - root - LLMClientManager: 指定使用客户端: Doubao
2025-11-09 13:22:43,552 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 13:22:48,878 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 13:22:48,886 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 13:22:48,886 - INFO - root - LLMClientManager: Doubao 客户端初始化成功
2025-11-09 13:22:48,887 - INFO - root - LLMClientManager: switched to Doubao client
2025-11-09 13:22:48,887 - INFO - root - 已手动切换到 LLM 客户端: Doubao
2025-11-09 13:22:48,888 - INFO - root - 使用 LLM 模型: doubao-seed-1-6-lite-251015
2025-11-09 13:22:48,889 - INFO - root - 可用客户端: ['Doubao']
2025-11-09 13:22:48,889 - INFO - root - === 运行配置 ===
2025-11-09 13:22:48,890 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 13:22:48,891 - INFO - root - PDF目录: ./myPapers
2025-11-09 13:22:48,891 - INFO - root - 最大处理数量: 2
2025-11-09 13:22:48,892 - INFO - root - 保存图片: 是
2025-11-09 13:22:48,892 - INFO - root - 输出语言: 中文
2025-11-09 13:22:48,892 - INFO - root - 强制重新处理: 否
2025-11-09 13:22:48,892 - INFO - root - ====================
2025-11-09 13:22:48,894 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 13:22:50,256 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:22:52,443 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:22:52,443 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 13:22:52,443 - INFO - root - 正在总结论文 1/2: A Compute&Memory Efficient Model-Driven Neural
2025-11-09 13:23:18,467 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 13:24:47,003 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 13:25:29,784 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 13:25:29,788 - INFO - root - 正在提取论文图片...
2025-11-09 13:25:31,327 - INFO - root - 已保存图片 1/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_1_page6.png
2025-11-09 13:25:31,422 - INFO - root - 已保存图片 2/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_2_page6.png
2025-11-09 13:25:31,518 - INFO - root - 已保存图片 3/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_3_page2.png
2025-11-09 13:25:31,575 - INFO - root - 已保存图片 4/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_4_page2.png
2025-11-09 13:25:31,625 - INFO - root - 已保存图片 5/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_5_page3.png
2025-11-09 13:25:31,628 - INFO - root - 成功添加图片 1：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_1_page6.png
2025-11-09 13:25:31,628 - INFO - root - 成功添加图片 2：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_2_page6.png
2025-11-09 13:25:31,628 - INFO - root - 成功添加图片 3：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_3_page2.png
2025-11-09 13:25:31,629 - INFO - root - 成功添加图片 4：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_4_page2.png
2025-11-09 13:25:31,629 - INFO - root - 成功添加图片 5：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_5_page3.png
2025-11-09 13:25:31,631 - INFO - root - 论文《A Compute&Memory Efficient Model-Driven Neural》的分析已保存到 ./export\A Compute&Memory Efficient Model-Driven Neural.md
2025-11-09 13:25:31,639 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:25:31,644 - INFO - root - summary time: 169.96 seconds
2025-11-09 13:41:09,766 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 13:41:09,768 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 13:41:09,770 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 13:41:10,997 - INFO - root - LLMClientManager: 指定使用客户端: Doubao
2025-11-09 13:41:10,998 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 13:41:16,132 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 13:41:16,144 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 13:41:16,144 - INFO - root - LLMClientManager: Doubao 客户端初始化成功
2025-11-09 13:41:16,144 - INFO - root - LLMClientManager: switched to Doubao client
2025-11-09 13:41:16,145 - INFO - root - 已手动切换到 LLM 客户端: Doubao
2025-11-09 13:41:16,145 - INFO - root - 使用 LLM 模型: doubao-seed-1-6-lite-251015
2025-11-09 13:41:16,146 - INFO - root - 可用客户端: ['Doubao']
2025-11-09 13:41:16,146 - INFO - root - === 运行配置 ===
2025-11-09 13:41:16,147 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 13:41:16,147 - INFO - root - PDF目录: ./myPapers
2025-11-09 13:41:16,147 - INFO - root - 最大处理数量: 2
2025-11-09 13:41:16,148 - INFO - root - 保存图片: 是
2025-11-09 13:41:16,150 - INFO - root - 输出语言: 中文
2025-11-09 13:41:16,151 - INFO - root - 强制重新处理: 否
2025-11-09 13:41:16,152 - INFO - root - ====================
2025-11-09 13:41:16,153 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 13:41:18,769 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:41:22,435 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:41:22,435 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 13:41:22,437 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:41:22,437 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:41:22,437 - INFO - root - summary time: 12.67 seconds
2025-11-09 13:42:09,313 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 13:42:09,314 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 13:42:09,317 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 13:42:10,708 - INFO - root - LLMClientManager: 指定使用客户端: Doubao
2025-11-09 13:42:10,709 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 13:42:15,602 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 13:42:15,620 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 13:42:15,620 - INFO - root - LLMClientManager: Doubao 客户端初始化成功
2025-11-09 13:42:15,621 - INFO - root - LLMClientManager: switched to Doubao client
2025-11-09 13:42:15,623 - INFO - root - 已手动切换到 LLM 客户端: Doubao
2025-11-09 13:42:15,624 - INFO - root - 使用 LLM 模型: doubao-seed-1-6-lite-251015
2025-11-09 13:42:15,625 - INFO - root - 可用客户端: ['Doubao']
2025-11-09 13:42:15,627 - INFO - root - === 运行配置 ===
2025-11-09 13:42:15,628 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 13:42:15,629 - INFO - root - PDF目录: ./myPapers
2025-11-09 13:42:15,629 - INFO - root - 最大处理数量: 2
2025-11-09 13:42:15,630 - INFO - root - 保存图片: 是
2025-11-09 13:42:15,630 - INFO - root - 输出语言: 中文
2025-11-09 13:42:15,631 - INFO - root - 强制重新处理: 否
2025-11-09 13:42:15,631 - INFO - root - ====================
2025-11-09 13:42:15,631 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 13:42:17,181 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 13:42:19,842 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:42:19,843 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 13:42:19,844 - INFO - root - 正在总结论文 1/2: A Compute&Memory Efficient Model-Driven Neural
2025-11-09 13:42:52,518 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 13:44:31,832 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 13:45:09,321 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 13:45:09,322 - INFO - root - 正在提取论文图片...
2025-11-09 13:45:10,094 - INFO - root - 已保存图片 1/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_1_page6.png
2025-11-09 13:45:10,255 - INFO - root - 已保存图片 2/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_2_page6.png
2025-11-09 13:45:10,346 - INFO - root - 已保存图片 3/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_3_page2.png
2025-11-09 13:45:10,394 - INFO - root - 已保存图片 4/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_4_page2.png
2025-11-09 13:45:10,451 - INFO - root - 已保存图片 5/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_5_page3.png
2025-11-09 13:45:10,457 - INFO - root - 成功添加图片 1：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_1_page6.png
2025-11-09 13:45:10,457 - INFO - root - 成功添加图片 2：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_2_page6.png
2025-11-09 13:45:10,458 - INFO - root - 成功添加图片 3：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_3_page2.png
2025-11-09 13:45:10,458 - INFO - root - 成功添加图片 4：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_4_page2.png
2025-11-09 13:45:10,458 - INFO - root - 成功添加图片 5：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_5_page3.png
2025-11-09 13:45:10,460 - INFO - root - 论文《A Compute&Memory Efficient Model-Driven Neural》的分析已保存到 ./export\A Compute&Memory Efficient Model-Driven Neural.md
2025-11-09 13:45:10,473 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 13:45:10,479 - INFO - root - summary time: 181.17 seconds
2025-11-09 14:04:17,111 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 14:04:17,111 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 14:04:17,111 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 14:04:19,141 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 14:04:20,107 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 14:04:20,489 - WARNING - root - GeminiClient: no usable model found
2025-11-09 14:04:20,491 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 14:04:20,492 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 14:04:20,492 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 14:04:20,493 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 14:04:20,493 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 14:04:20,493 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 14:04:20,494 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 14:04:20,494 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 14:04:26,872 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 14:04:26,893 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 14:04:26,894 - INFO - root - LLMClientManager: Doubao client initialized successfully
2025-11-09 14:04:26,894 - INFO - root - LLMClientManager: using Doubao as default client
2025-11-09 14:04:26,895 - INFO - root - 使用 LLM 模型: doubao-seed-1-6-lite-251015
2025-11-09 14:04:26,896 - INFO - root - 可用客户端: ['Doubao']
2025-11-09 14:04:26,896 - INFO - root - === 运行配置 ===
2025-11-09 14:04:26,896 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 14:04:26,897 - INFO - root - PDF目录: ./myPapers
2025-11-09 14:04:26,897 - INFO - root - 最大处理数量: 2
2025-11-09 14:04:26,897 - INFO - root - 保存图片: 是
2025-11-09 14:04:26,898 - INFO - root - 输出语言: 中文
2025-11-09 14:04:26,898 - INFO - root - 强制重新处理: 否
2025-11-09 14:04:26,899 - INFO - root - ====================
2025-11-09 14:04:26,900 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 14:04:28,787 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 14:04:31,792 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 14:04:31,794 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 14:04:31,795 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 14:04:31,796 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 14:04:31,796 - INFO - root - summary time: 14.68 seconds
2025-11-09 14:12:56,035 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 14:12:56,035 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 14:12:56,049 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 14:12:57,528 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 14:12:58,374 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 14:12:58,745 - WARNING - root - GeminiClient: no usable model found
2025-11-09 14:12:58,746 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 14:12:58,746 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 14:12:58,746 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 14:12:58,747 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 14:12:58,748 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 14:12:58,748 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 14:12:58,748 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 14:12:58,748 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 14:13:02,872 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 14:13:02,877 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 14:13:02,877 - INFO - root - LLMClientManager: Doubao client initialized successfully
2025-11-09 14:13:02,877 - INFO - root - LLMClientManager: using Doubao as default client
2025-11-09 14:13:02,885 - INFO - root - 使用 LLM 模型: doubao-seed-1-6-lite-251015
2025-11-09 14:13:02,885 - INFO - root - 可用客户端: ['Doubao']
2025-11-09 14:13:02,886 - INFO - root - === 运行配置 ===
2025-11-09 14:13:02,886 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 14:13:02,887 - INFO - root - PDF目录: ./myPapers
2025-11-09 14:13:02,887 - INFO - root - 最大处理数量: 2
2025-11-09 14:13:02,890 - INFO - root - 保存图片: 是
2025-11-09 14:13:02,890 - INFO - root - 输出语言: 中文
2025-11-09 14:13:02,891 - INFO - root - 强制重新处理: 否
2025-11-09 14:13:02,891 - INFO - root - ====================
2025-11-09 14:13:02,891 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 14:13:04,457 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 14:13:06,450 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 14:13:06,450 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 14:13:06,450 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 14:13:06,450 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 14:13:06,450 - INFO - root - summary time: 10.42 seconds
2025-11-09 14:13:27,922 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 14:13:27,924 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 14:13:27,926 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 14:13:29,624 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 14:13:30,568 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 14:13:30,695 - WARNING - root - GeminiClient: no usable model found
2025-11-09 14:13:30,696 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 14:13:30,696 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 14:13:30,696 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 14:13:30,697 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 14:13:30,697 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 14:13:30,697 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 14:13:30,697 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 14:13:30,698 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 14:13:35,993 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 14:13:36,003 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 14:13:36,003 - INFO - root - LLMClientManager: Doubao client initialized successfully
2025-11-09 14:13:36,003 - INFO - root - LLMClientManager: using Doubao as default client
2025-11-09 14:13:36,003 - INFO - root - 使用 LLM 模型: doubao-seed-1-6-lite-251015
2025-11-09 14:13:36,004 - INFO - root - 可用客户端: ['Doubao']
2025-11-09 14:13:36,006 - INFO - root - === 运行配置 ===
2025-11-09 14:13:36,008 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 14:13:36,009 - INFO - root - PDF目录: ./myPapers
2025-11-09 14:13:36,010 - INFO - root - 最大处理数量: 2
2025-11-09 14:13:36,010 - INFO - root - 保存图片: 是
2025-11-09 14:13:36,011 - INFO - root - 输出语言: 中文
2025-11-09 14:13:36,011 - INFO - root - 强制重新处理: 否
2025-11-09 14:13:36,011 - INFO - root - ====================
2025-11-09 14:13:36,011 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 14:13:37,419 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 14:13:39,410 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 14:13:39,410 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 14:13:39,410 - INFO - root - 跳过已处理论文 A Compute&Memory Efficient Model-Driven Neural：./myPapers\A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 14:13:39,410 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 14:13:39,410 - INFO - root - summary time: 11.49 seconds
2025-11-09 14:14:39,048 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 14:14:39,052 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 14:14:39,054 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 14:14:40,444 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 14:14:41,291 - INFO - root - GeminiClient: trying model gemini-2.5-pro
2025-11-09 14:14:41,648 - WARNING - root - GeminiClient: no usable model found
2025-11-09 14:14:41,648 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 14:14:41,648 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 14:14:41,649 - WARNING - root - LLMClientManager: DeepSeek client initialization failed
2025-11-09 14:14:41,649 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 14:14:41,649 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 14:14:41,650 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 14:14:41,650 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 14:14:41,650 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 14:14:46,605 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 14:14:46,616 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 14:14:46,616 - INFO - root - LLMClientManager: Doubao client initialized successfully
2025-11-09 14:14:46,616 - INFO - root - LLMClientManager: using Doubao as default client
2025-11-09 14:14:46,616 - INFO - root - 使用 LLM 模型: doubao-seed-1-6-lite-251015
2025-11-09 14:14:46,616 - INFO - root - 可用客户端: ['Doubao']
2025-11-09 14:14:46,619 - INFO - root - === 运行配置 ===
2025-11-09 14:14:46,619 - INFO - root - 处理模式: 本地PDF文件
2025-11-09 14:14:46,620 - INFO - root - PDF目录: ./myPapers
2025-11-09 14:14:46,620 - INFO - root - 最大处理数量: 2
2025-11-09 14:14:46,620 - INFO - root - 保存图片: 是
2025-11-09 14:14:46,620 - INFO - root - 输出语言: 中文
2025-11-09 14:14:46,621 - INFO - root - 强制重新处理: 否
2025-11-09 14:14:46,622 - INFO - root - ====================
2025-11-09 14:14:46,622 - INFO - root - 从本地目录读取PDF文件：./myPapers
2025-11-09 14:14:47,983 - INFO - root - 成功加载PDF文件：A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted.pdf
2025-11-09 14:14:49,898 - INFO - root - 成功加载PDF文件：AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 14:14:49,898 - INFO - root - 已达到最大处理数量限制 (2)，停止加载更多PDF文件
2025-11-09 14:14:49,898 - INFO - root - 正在总结论文 1/2: A Compute&Memory Efficient Model-Driven Neural
2025-11-09 14:15:32,728 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 14:16:45,454 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 14:17:33,055 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 14:17:33,058 - INFO - root - 正在提取论文图片...
2025-11-09 14:17:33,812 - INFO - root - 已保存图片 1/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_1_page6.png
2025-11-09 14:17:33,915 - INFO - root - 已保存图片 2/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_2_page6.png
2025-11-09 14:17:34,007 - INFO - root - 已保存图片 3/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_3_page2.png
2025-11-09 14:17:34,056 - INFO - root - 已保存图片 4/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_4_page2.png
2025-11-09 14:17:34,127 - INFO - root - 已保存图片 5/10：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_5_page3.png
2025-11-09 14:17:34,132 - INFO - root - 成功添加图片 1：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_1_page6.png
2025-11-09 14:17:34,132 - INFO - root - 成功添加图片 2：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_2_page6.png
2025-11-09 14:17:34,132 - INFO - root - 成功添加图片 3：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_3_page2.png
2025-11-09 14:17:34,133 - INFO - root - 成功添加图片 4：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_4_page2.png
2025-11-09 14:17:34,133 - INFO - root - 成功添加图片 5：./export\images_A Compute&Memory Efficient Model-Driven Neural\figure_5_page3.png
2025-11-09 14:17:34,138 - INFO - root - 论文《A Compute&Memory Efficient Model-Driven Neural》的分析已保存到 ./export\A Compute&Memory Efficient Model-Driven Neural.md
2025-11-09 14:17:34,145 - INFO - root - 跳过已处理论文 AMLA: MUL by ADD in FlashAttention Rescaling：./myPapers\AMLA_ MUL by ADD in FlashAttention Rescaling.pdf
2025-11-09 14:17:34,153 - INFO - root - summary time: 175.10 seconds
2025-11-09 16:42:33,744 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=traffic+flow+prediction&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 16:42:35,196 - INFO - root - get_all_titles_from_web 
2025-11-09 16:42:35,196 - INFO - root - Page:0, Index:0, A Theoretical Framework for Environmental Similarity and Vessel Mobility as Coupled Predictors of Marine Invasive Species Pathways, https://arxiv.org/pdf/2511.03499, 2025-11-06
2025-11-09 16:42:35,196 - INFO - root - Page:0, Index:1, Towards Sub-millisecond Latency and Guaranteed Bit Rates in 5G User Plane, https://arxiv.org/pdf/2511.00196, 2025-10-31
2025-11-09 16:42:35,196 - INFO - root - Page:0, Index:2, A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration, https://arxiv.org/pdf/2510.27039, 2025-10-30
2025-11-09 16:42:35,196 - INFO - root - Page:0, Index:3, Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention, https://arxiv.org/pdf/2509.13361, 2025-11-04
2025-11-09 16:42:35,196 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=traffic+flow+prediction&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 16:42:36,837 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 16:42:46,124 - INFO - root - Downloaded 3 papers in 12.4s
2025-11-09 16:43:48,497 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 16:43:48,502 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 16:43:48,502 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 16:47:07,981 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 16:47:07,981 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 16:47:07,985 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 16:47:09,033 - INFO - root - LLMClientManager: 指定使用客户端: Deepseek
2025-11-09 16:47:09,034 - INFO - root - DeepSeekClient: API key found: your_deeps...
2025-11-09 16:47:09,034 - WARNING - root - DeepSeekClient: API key not provided or using placeholder. LLM disabled.
2025-11-09 16:47:09,034 - ERROR - root - LLMClientManager: 指定的客户端 DeepSeek 初始化失败
2025-11-09 16:47:09,034 - WARNING - root - LLMClientManager: 指定的客户端 Deepseek 不可用，将尝试其他客户端
2025-11-09 16:51:57,471 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 16:51:57,472 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 16:51:57,477 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 16:51:59,021 - INFO - root - LLMClientManager: 指定使用客户端: Deepseek
2025-11-09 16:51:59,021 - INFO - root - DeepSeekClient: API key found: 9f9e270e-b...
2025-11-09 16:52:01,342 - INFO - httpx - HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-09 16:52:01,345 - ERROR - root - DeepSeekClient: error during initialization: Error code: 401 - {'error': {'message': 'Authentication Fails, Your api key: ****24c9 is invalid', 'type': 'authentication_error', 'param': None, 'code': 'invalid_request_error'}}
2025-11-09 16:52:01,346 - ERROR - root - LLMClientManager: 指定的客户端 DeepSeek 初始化失败
2025-11-09 16:52:01,347 - WARNING - root - LLMClientManager: 指定的客户端 Deepseek 不可用，将尝试其他客户端
2025-11-09 16:53:00,258 - ERROR - root - GeminiClient: error during initialization: Timeout of 60.0s exceeded, last exception: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.73.138:443: socket is null
2025-11-09 16:53:00,260 - WARNING - root - LLMClientManager: Gemini client initialization failed
2025-11-09 16:53:00,261 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 16:53:00,261 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 16:53:00,261 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 16:53:00,263 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 16:53:00,263 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 16:53:04,463 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 16:53:04,488 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 16:53:04,489 - INFO - root - LLMClientManager: Doubao client initialized successfully
2025-11-09 16:53:04,491 - INFO - root - LLMClientManager: using Doubao as default client
2025-11-09 16:53:04,492 - WARNING - root - LLMClientManager: client Deepseek not available
2025-11-09 16:53:04,493 - WARNING - root - 无法切换到指定的客户端 Deepseek，将使用默认客户端
2025-11-09 16:53:04,497 - INFO - root - 可用客户端: ['Doubao']
2025-11-09 16:53:04,497 - INFO - root - 使用 LLM 模型: doubao-seed-1-6-lite-251015
2025-11-09 16:53:04,498 - INFO - root - 可用客户端: ['Doubao']
2025-11-09 16:53:04,501 - INFO - root - === 运行配置 ===
2025-11-09 16:53:04,502 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 16:53:04,503 - INFO - root - 关键词: Quant
2025-11-09 16:53:04,506 - INFO - root - 查询: Quantization-Aware-Training
2025-11-09 16:53:04,507 - INFO - root - 排序: None
2025-11-09 16:53:04,507 - INFO - root - 最近天数: 180
2025-11-09 16:53:04,544 - INFO - root - 最大处理数量: 10
2025-11-09 16:53:04,544 - INFO - root - 保存图片: 是
2025-11-09 16:53:04,546 - INFO - root - 输出语言: 中文
2025-11-09 16:53:04,547 - INFO - root - 强制重新处理: 否
2025-11-09 16:53:04,551 - INFO - root - ====================
2025-11-09 16:53:04,557 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 16:53:04,557 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 16:53:06,532 - INFO - root - get_all_titles_from_web 
2025-11-09 16:53:06,533 - INFO - root - Page:0, Index:0, A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies, https://arxiv.org/pdf/2511.03201, 2025-11-05
2025-11-09 16:53:06,533 - INFO - root - Page:0, Index:1, FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error, https://arxiv.org/pdf/2511.02302, 2025-11-04
2025-11-09 16:53:06,533 - INFO - root - Page:0, Index:2, Outlier-Aware Post-Training Quantization for Image Super-Resolution, https://arxiv.org/pdf/2511.00682, 2025-11-01
2025-11-09 16:53:06,534 - INFO - root - Page:0, Index:3, Evaluation of Wafer-Scale SOT-MRAM for Analog Crossbar Array Applications, https://arxiv.org/pdf/2510.25853, 2025-11-03
2025-11-09 16:53:06,534 - INFO - root - Page:0, Index:4, Improving the Straight-Through Estimator with Zeroth-Order Information, https://arxiv.org/pdf/2510.23926, 2025-10-27
2025-11-09 16:53:06,534 - INFO - root - Page:0, Index:5, Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework, https://arxiv.org/pdf/2510.22243, 2025-10-25
2025-11-09 16:53:06,534 - INFO - root - Page:0, Index:6, TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge, https://arxiv.org/pdf/2510.21879, 2025-10-23
2025-11-09 16:53:06,535 - INFO - root - Page:0, Index:7, KARIPAP: Quantum-Inspired Tensor Network Compression of Large Language Models Using Infinite Projected Entangled Pair States and Tensor Renormalization Group, https://arxiv.org/pdf/2510.21844, 2025-10-22
2025-11-09 16:53:06,535 - INFO - root - Page:0, Index:8, A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization, https://arxiv.org/pdf/2510.21314, 2025-10-24
2025-11-09 16:53:06,535 - INFO - root - Page:0, Index:9, Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks, https://arxiv.org/pdf/2510.19760, 2025-10-22
2025-11-09 16:53:06,536 - INFO - root - Page:0, Index:10, CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training, https://arxiv.org/pdf/2510.18784, 2025-10-21
2025-11-09 16:53:06,536 - INFO - root - Page:0, Index:11, Mixed-Precision Quantization for Language Models: Techniques and Prospects, https://arxiv.org/pdf/2510.16805, 2025-10-19
2025-11-09 16:53:06,536 - INFO - root - Page:0, Index:12, SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation, https://arxiv.org/pdf/2510.16396, 2025-10-30
2025-11-09 16:53:06,536 - INFO - root - Page:0, Index:13, CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models, https://arxiv.org/pdf/2510.15962, 2025-10-11
2025-11-09 16:53:06,537 - INFO - root - Page:0, Index:14, SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization, https://arxiv.org/pdf/2510.15775, 2025-10-17
2025-11-09 16:53:06,537 - INFO - root - Page:0, Index:15, SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware, https://arxiv.org/pdf/2510.15542, 2025-10-31
2025-11-09 16:53:06,537 - INFO - root - Page:0, Index:16, GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework, https://arxiv.org/pdf/2510.15299, 2025-10-17
2025-11-09 16:53:06,538 - INFO - root - Page:0, Index:17, SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images, https://arxiv.org/pdf/2510.15072, 2025-10-16
2025-11-09 16:53:06,538 - INFO - root - Page:0, Index:18, FraQAT: Quantization Aware Training with Fractional bits, https://arxiv.org/pdf/2510.14823, 2025-10-16
2025-11-09 16:53:06,540 - INFO - root - Page:0, Index:19, Computing-In-Memory Aware Model Adaption For Edge Devices, https://arxiv.org/pdf/2510.14379, 2025-10-16
2025-11-09 16:53:06,540 - INFO - root - Page:0, Index:20, Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge, https://arxiv.org/pdf/2510.13760, 2025-11-04
2025-11-09 16:53:06,540 - INFO - root - Page:0, Index:21, NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models, https://arxiv.org/pdf/2510.13068, 2025-10-14
2025-11-09 16:53:06,543 - INFO - root - Page:0, Index:22, Detect Anything via Next Point Prediction, https://arxiv.org/pdf/2510.12798, 2025-10-14
2025-11-09 16:53:06,546 - INFO - root - Page:0, Index:23, AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model, https://arxiv.org/pdf/2510.11496, 2025-10-14
2025-11-09 16:53:06,548 - INFO - root - Page:0, Index:24, Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware, https://arxiv.org/pdf/2510.11484, 2025-10-13
2025-11-09 16:53:06,549 - INFO - root - Page:0, Index:25, SASER: Stego attacks on open-source LLMs, https://arxiv.org/pdf/2510.10486, 2025-10-12
2025-11-09 16:53:06,550 - INFO - root - Page:0, Index:26, Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition, https://arxiv.org/pdf/2510.09653, 2025-10-15
2025-11-09 16:53:06,550 - INFO - root - Page:0, Index:27, A Theoretically-Grounded Codebook for Digital Semantic Communications, https://arxiv.org/pdf/2510.07108, 2025-10-14
2025-11-09 16:53:06,551 - INFO - root - Page:0, Index:28, QuantDemoire: Quantization with Outlier Aware for Image Demoiréing, https://arxiv.org/pdf/2510.04066, 2025-10-05
2025-11-09 16:53:06,552 - INFO - root - Page:0, Index:29, Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization, https://arxiv.org/pdf/2510.03763, 2025-10-04
2025-11-09 16:53:06,552 - INFO - root - Page:0, Index:30, Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models, https://arxiv.org/pdf/2510.03274, 2025-09-27
2025-11-09 16:53:06,554 - INFO - root - Page:0, Index:31, PT$^2$-LLM: Post-Training Ternarization for Large Language Models, https://arxiv.org/pdf/2510.03267, 2025-09-26
2025-11-09 16:53:06,554 - INFO - root - Page:0, Index:32, Purrception: Variational Flow Matching for Vector-Quantized Image Generation, https://arxiv.org/pdf/2510.01478, 2025-10-01
2025-11-09 16:53:06,555 - INFO - root - Page:0, Index:33, Post-Training Quantization for Audio Diffusion Transformers, https://arxiv.org/pdf/2510.00313, 2025-09-30
2025-11-09 16:53:06,556 - INFO - root - Page:0, Index:34, Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling, https://arxiv.org/pdf/2510.00028, 2025-09-25
2025-11-09 16:53:06,557 - INFO - root - Page:0, Index:35, Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models, https://arxiv.org/pdf/2509.26436, 2025-09-30
2025-11-09 16:53:06,558 - INFO - root - Page:0, Index:36, Cat: Post-Training Quantization Error Reduction via Cluster-based Affine Transformation, https://arxiv.org/pdf/2509.26277, 2025-10-07
2025-11-09 16:53:06,558 - INFO - root - Page:0, Index:37, CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models, https://arxiv.org/pdf/2509.25996, 2025-09-30
2025-11-09 16:53:06,559 - INFO - root - Page:0, Index:38, Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications, https://arxiv.org/pdf/2509.25439, 2025-09-29
2025-11-09 16:53:06,561 - INFO - root - Page:0, Index:39, On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs, https://arxiv.org/pdf/2509.25214, 2025-09-22
2025-11-09 16:53:06,566 - INFO - root - Page:0, Index:40, VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning, https://arxiv.org/pdf/2509.24650, 2025-09-29
2025-11-09 16:53:06,567 - INFO - root - Page:0, Index:41, S$^2$NN: Sub-bit Spiking Neural Networks, https://arxiv.org/pdf/2509.24266, 2025-10-24
2025-11-09 16:53:06,568 - INFO - root - Page:0, Index:42, Tequila: Trapping-free Ternary Quantization for Large Language Models, https://arxiv.org/pdf/2509.23809, 2025-10-17
2025-11-09 16:53:06,568 - INFO - root - Page:0, Index:43, Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution, https://arxiv.org/pdf/2509.23774, 2025-09-30
2025-11-09 16:53:06,568 - INFO - root - Page:0, Index:44, RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization, https://arxiv.org/pdf/2509.23582, 2025-09-27
2025-11-09 16:53:06,568 - INFO - root - Page:0, Index:45, Beyond Outliers: A Study of Optimizers Under Quantization, https://arxiv.org/pdf/2509.23500, 2025-10-02
2025-11-09 16:53:06,569 - INFO - root - Page:0, Index:46, Compute-Optimal Quantization-Aware Training, https://arxiv.org/pdf/2509.22935, 2025-09-26
2025-11-09 16:53:06,569 - INFO - root - Page:0, Index:47, COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning, https://arxiv.org/pdf/2509.22075, 2025-10-06
2025-11-09 16:53:06,569 - INFO - root - Page:0, Index:48, SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models, https://arxiv.org/pdf/2509.21498, 2025-09-25
2025-11-09 16:53:06,571 - INFO - root - Page:0, Index:49, Quantized Visual Geometry Grounded Transformer, https://arxiv.org/pdf/2509.21302, 2025-09-29
2025-11-09 16:53:06,584 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 16:53:08,300 - INFO - root - get_all_titles_from_web 
2025-11-09 16:53:08,300 - INFO - root - Page:1, Index:0, Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy, https://arxiv.org/pdf/2509.21173, 2025-10-27
2025-11-09 16:53:08,300 - INFO - root - Page:1, Index:1, Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer, https://arxiv.org/pdf/2509.20854, 2025-09-25
2025-11-09 16:53:08,301 - INFO - root - Page:1, Index:2, TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection, https://arxiv.org/pdf/2509.18193, 2025-09-19
2025-11-09 16:53:08,301 - INFO - root - Page:1, Index:3, SBVR: Summation of BitVector Representation for Efficient LLM Quantization, https://arxiv.org/pdf/2509.18172, 2025-09-17
2025-11-09 16:53:08,301 - INFO - root - Page:1, Index:4, QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2509.17428, 2025-09-26
2025-11-09 16:53:08,302 - INFO - root - Page:1, Index:5, PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models, https://arxiv.org/pdf/2509.16989, 2025-10-28
2025-11-09 16:53:08,302 - INFO - root - Page:1, Index:6, MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training, https://arxiv.org/pdf/2509.15514, 2025-09-18
2025-11-09 16:53:08,302 - INFO - root - Page:1, Index:7, Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs, https://arxiv.org/pdf/2509.14391, 2025-09-17
2025-11-09 16:53:08,303 - INFO - root - Page:1, Index:8, Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization, https://arxiv.org/pdf/2509.13786, 2025-09-19
2025-11-09 16:53:08,303 - INFO - root - Page:1, Index:9, Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees, https://arxiv.org/pdf/2509.11054, 2025-09-13
2025-11-09 16:53:08,303 - INFO - root - Page:1, Index:10, SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models, https://arxiv.org/pdf/2509.09090, 2025-09-10
2025-11-09 16:53:08,304 - INFO - root - Page:1, Index:11, CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks with Entropy Regularization, https://arxiv.org/pdf/2509.08776, 2025-09-10
2025-11-09 16:53:08,304 - INFO - root - Page:1, Index:12, Explaining How Quantization Disparately Skews a Model, https://arxiv.org/pdf/2509.07222, 2025-09-08
2025-11-09 16:53:08,304 - INFO - root - Page:1, Index:13, LoaQ: Layer-wise Output Approximation Quantization, https://arxiv.org/pdf/2509.06297, 2025-09-07
2025-11-09 16:53:08,304 - INFO - root - Page:1, Index:14, FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving, https://arxiv.org/pdf/2509.06261, 2025-09-14
2025-11-09 16:53:08,305 - INFO - root - Page:1, Index:15, Sensitivity-Aware Post-Training Quantization for Deep Neural Networks, https://arxiv.org/pdf/2509.05576, 2025-09-05
2025-11-09 16:53:08,306 - INFO - root - Page:1, Index:16, SuperSNN: A Hardware-Aware Framework for Physically Realizable, High-Performance Superconducting Spiking Neural Network Chips, https://arxiv.org/pdf/2509.05532, 2025-09-05
2025-11-09 16:53:08,306 - INFO - root - Page:1, Index:17, Data-Augmented Quantization-Aware Knowledge Distillation, https://arxiv.org/pdf/2509.03850, 2025-09-03
2025-11-09 16:53:08,308 - INFO - root - Page:1, Index:18, DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling, https://arxiv.org/pdf/2509.03472, 2025-09-03
2025-11-09 16:53:08,309 - INFO - root - Page:1, Index:19, Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs, https://arxiv.org/pdf/2509.02017, 2025-09-02
2025-11-09 16:53:08,309 - INFO - root - Page:1, Index:20, Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling, https://arxiv.org/pdf/2509.01624, 2025-09-01
2025-11-09 16:53:08,310 - INFO - root - Page:1, Index:21, Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective, https://arxiv.org/pdf/2509.00859, 2025-08-31
2025-11-09 16:53:08,311 - INFO - root - Page:1, Index:22, Progressive Element-wise Gradient Estimation for Neural Network Quantization, https://arxiv.org/pdf/2509.00097, 2025-08-27
2025-11-09 16:53:08,312 - INFO - root - Page:1, Index:23, End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost, https://arxiv.org/pdf/2509.00031, 2025-09-29
2025-11-09 16:53:08,318 - INFO - root - Page:1, Index:24, Quantization Robustness to Input Degradations for Object Detection, https://arxiv.org/pdf/2508.19600, 2025-08-27
2025-11-09 16:53:08,322 - INFO - root - Page:1, Index:25, Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models, https://arxiv.org/pdf/2508.18609, 2025-08-27
2025-11-09 16:53:08,323 - INFO - root - Page:1, Index:26, AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration, https://arxiv.org/pdf/2508.18025, 2025-08-25
2025-11-09 16:53:08,323 - INFO - root - Page:1, Index:27, TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling, https://arxiv.org/pdf/2508.16790, 2025-08-22
2025-11-09 16:53:08,323 - INFO - root - Page:1, Index:28, A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering, https://arxiv.org/pdf/2508.16516, 2025-08-22
2025-11-09 16:53:08,325 - INFO - root - Page:1, Index:29, JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs, https://arxiv.org/pdf/2508.15468, 2025-08-21
2025-11-09 16:53:08,325 - INFO - root - Page:1, Index:30, MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation, https://arxiv.org/pdf/2508.15281, 2025-08-21
2025-11-09 16:53:08,325 - INFO - root - Page:1, Index:31, DLLMQuant: Quantizing Diffusion-based Large Language Models, https://arxiv.org/pdf/2508.14090, 2025-08-25
2025-11-09 16:53:08,326 - INFO - root - Page:1, Index:32, Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management, https://arxiv.org/pdf/2508.13905, 2025-08-19
2025-11-09 16:53:08,333 - INFO - root - Page:1, Index:33, XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads, https://arxiv.org/pdf/2508.13049, 2025-08-18
2025-11-09 16:53:08,334 - INFO - root - Page:1, Index:34, Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection, https://arxiv.org/pdf/2508.12230, 2025-08-17
2025-11-09 16:53:08,336 - INFO - root - Page:1, Index:35, TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks, https://arxiv.org/pdf/2508.12132, 2025-08-16
2025-11-09 16:53:08,336 - INFO - root - Page:1, Index:36, Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion, https://arxiv.org/pdf/2508.12094, 2025-10-13
2025-11-09 16:53:08,338 - INFO - root - Page:1, Index:37, Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware, https://arxiv.org/pdf/2508.11940, 2025-08-16
2025-11-09 16:53:08,339 - INFO - root - Page:1, Index:38, PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks, https://arxiv.org/pdf/2508.10557, 2025-08-15
2025-11-09 16:53:08,356 - INFO - root - Page:1, Index:39, MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI, https://arxiv.org/pdf/2508.09500, 2025-08-13
2025-11-09 16:53:08,356 - INFO - root - Page:1, Index:40, SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via Codebooks for recommender system, https://arxiv.org/pdf/2508.09090, 2025-08-12
2025-11-09 16:53:08,357 - INFO - root - Page:1, Index:41, Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models, https://arxiv.org/pdf/2508.06974, 2025-08-09
2025-11-09 16:53:08,357 - INFO - root - Page:1, Index:42, Efficient Deep Neural Receiver with Post-Training Quantization, https://arxiv.org/pdf/2508.06275, 2025-08-08
2025-11-09 16:53:08,357 - INFO - root - Page:1, Index:43, iFairy: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$, https://arxiv.org/pdf/2508.05571, 2025-08-16
2025-11-09 16:53:08,358 - INFO - root - Page:1, Index:44, S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation, https://arxiv.org/pdf/2508.04016, 2025-10-27
2025-11-09 16:53:08,360 - INFO - root - Page:1, Index:45, LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation, https://arxiv.org/pdf/2508.03485, 2025-09-23
2025-11-09 16:53:08,361 - INFO - root - Page:1, Index:46, VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation, https://arxiv.org/pdf/2508.03351, 2025-08-05
2025-11-09 16:53:08,361 - INFO - root - Page:1, Index:47, TF-MLPNet: Tiny Real-Time Neural Speech Separation, https://arxiv.org/pdf/2508.03047, 2025-08-04
2025-11-09 16:53:08,361 - INFO - root - Page:1, Index:48, SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation, https://arxiv.org/pdf/2508.01375, 2025-08-02
2025-11-09 16:53:08,363 - INFO - root - Page:1, Index:49, MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective, https://arxiv.org/pdf/2507.19131, 2025-07-25
2025-11-09 16:53:08,363 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 16:53:10,328 - INFO - root - get_all_titles_from_web 
2025-11-09 16:53:10,329 - INFO - root - Page:2, Index:0, Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction, https://arxiv.org/pdf/2507.17768, 2025-07-16
2025-11-09 16:53:10,329 - INFO - root - Page:2, Index:1, SiLQ: Simple Large Language Model Quantization-Aware Training, https://arxiv.org/pdf/2507.16933, 2025-07-22
2025-11-09 16:53:10,330 - INFO - root - Page:2, Index:2, Task-Specific Zero-shot Quantization-Aware Training for Object Detection, https://arxiv.org/pdf/2507.16782, 2025-07-22
2025-11-09 16:53:10,330 - INFO - root - Page:2, Index:3, TorchAO: PyTorch-Native Training-to-Serving Model Optimization, https://arxiv.org/pdf/2507.16099, 2025-07-21
2025-11-09 16:53:10,331 - INFO - root - Page:2, Index:4, SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models, https://arxiv.org/pdf/2507.14811, 2025-08-27
2025-11-09 16:53:10,332 - INFO - root - Page:2, Index:5, Apple Intelligence Foundation Language Models: Tech Report 2025, https://arxiv.org/pdf/2507.13575, 2025-08-27
2025-11-09 16:53:10,333 - INFO - root - Page:2, Index:6, Generative Multi-Target Cross-Domain Recommendation, https://arxiv.org/pdf/2507.12871, 2025-08-07
2025-11-09 16:53:10,334 - INFO - root - Page:2, Index:7, DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training, https://arxiv.org/pdf/2507.07149, 2025-07-09
2025-11-09 16:53:10,335 - INFO - root - Page:2, Index:8, Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics, https://arxiv.org/pdf/2507.07044, 2025-07-15
2025-11-09 16:53:10,336 - INFO - root - Page:2, Index:9, QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models, https://arxiv.org/pdf/2507.06079, 2025-07-08
2025-11-09 16:53:10,336 - INFO - root - Page:2, Index:10, GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field, https://arxiv.org/pdf/2507.05594, 2025-07-07
2025-11-09 16:53:10,340 - INFO - root - Page:2, Index:11, Hita: Holistic Tokenizer for Autoregressive Image Generation, https://arxiv.org/pdf/2507.02358, 2025-07-11
2025-11-09 16:53:10,341 - INFO - root - Page:2, Index:12, QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference, https://arxiv.org/pdf/2506.23934, 2025-06-30
2025-11-09 16:53:10,342 - INFO - root - Page:2, Index:13, FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization, https://arxiv.org/pdf/2506.23516, 2025-07-21
2025-11-09 16:53:10,347 - INFO - root - Page:2, Index:14, Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models, https://arxiv.org/pdf/2506.23025, 2025-06-28
2025-11-09 16:53:10,349 - INFO - root - Page:2, Index:15, Joint Quantization and Pruning Neural Networks Approach: A Case Study on FSO Receivers, https://arxiv.org/pdf/2506.20084, 2025-06-24
2025-11-09 16:53:10,374 - INFO - root - Page:2, Index:16, Single-step Diffusion for Image Compression at Ultra-Low Bitrates, https://arxiv.org/pdf/2506.16572, 2025-09-22
2025-11-09 16:53:10,389 - INFO - root - Page:2, Index:17, LittleBit: Ultra Low-Bit Quantization via Latent Factorization, https://arxiv.org/pdf/2506.13771, 2025-10-28
2025-11-09 16:53:10,394 - INFO - root - Page:2, Index:18, MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering, https://arxiv.org/pdf/2506.13755, 2025-06-16
2025-11-09 16:53:10,396 - INFO - root - Page:2, Index:19, EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization, https://arxiv.org/pdf/2506.13329, 2025-07-04
2025-11-09 16:53:10,397 - INFO - root - Page:2, Index:20, Towards Neural Audio Codec Source Parsing, https://arxiv.org/pdf/2506.12627, 2025-06-14
2025-11-09 16:53:10,397 - INFO - root - Page:2, Index:21, Quantizing Small-Scale State-Space Models for Edge AI, https://arxiv.org/pdf/2506.12480, 2025-06-14
2025-11-09 16:53:10,398 - INFO - root - Page:2, Index:22, EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction, https://arxiv.org/pdf/2506.12015, 2025-06-13
2025-11-09 16:53:10,399 - INFO - root - Page:2, Index:23, Compression Aware Certified Training, https://arxiv.org/pdf/2506.11992, 2025-06-13
2025-11-09 16:53:10,404 - INFO - root - Page:2, Index:24, GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers, https://arxiv.org/pdf/2506.11784, 2025-06-13
2025-11-09 16:53:10,414 - INFO - root - Page:2, Index:25, TruncQuant: Truncation-Ready Quantization for DNNs with Flexible Weight Bit Precision, https://arxiv.org/pdf/2506.11431, 2025-06-12
2025-11-09 16:53:10,415 - INFO - root - Page:2, Index:26, EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices, https://arxiv.org/pdf/2506.11093, 2025-06-05
2025-11-09 16:53:10,415 - INFO - root - Page:2, Index:27, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 16:53:10,419 - INFO - root - Page:2, Index:28, AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent, https://arxiv.org/pdf/2506.10205, 2025-06-11
2025-11-09 16:53:10,420 - INFO - root - Page:2, Index:29, Q-SAM2: Accurate Quantization for Segment Anything Model 2, https://arxiv.org/pdf/2506.09782, 2025-06-11
2025-11-09 16:53:10,421 - INFO - root - Page:2, Index:30, Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs, https://arxiv.org/pdf/2506.09104, 2025-06-10
2025-11-09 16:53:10,422 - INFO - root - Page:2, Index:31, Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU, https://arxiv.org/pdf/2506.08911, 2025-06-10
2025-11-09 16:53:10,422 - INFO - root - Page:2, Index:32, POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration, https://arxiv.org/pdf/2506.08785, 2025-06-10
2025-11-09 16:53:10,424 - INFO - root - Page:2, Index:33, MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts, https://arxiv.org/pdf/2506.07533, 2025-06-09
2025-11-09 16:53:10,425 - INFO - root - Page:2, Index:34, BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation, https://arxiv.org/pdf/2506.07530, 2025-06-09
2025-11-09 16:53:10,425 - INFO - root - Page:2, Index:35, RecGPT: A Foundation Model for Sequential Recommendation, https://arxiv.org/pdf/2506.06270, 2025-06-12
2025-11-09 16:53:10,428 - INFO - root - Page:2, Index:36, FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion, https://arxiv.org/pdf/2506.04648, 2025-06-05
2025-11-09 16:53:10,429 - INFO - root - Page:2, Index:37, BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing, https://arxiv.org/pdf/2506.03515, 2025-06-03
2025-11-09 16:53:10,438 - INFO - root - Page:2, Index:38, ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding, https://arxiv.org/pdf/2506.01853, 2025-06-02
2025-11-09 16:53:10,440 - INFO - root - Page:2, Index:39, Movable Antenna Enhanced Federated Fine-Tuning of Large Language Models via Hybrid Client Selection Optimization, https://arxiv.org/pdf/2506.00011, 2025-10-26
2025-11-09 16:53:10,441 - INFO - root - Page:2, Index:40, Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach, https://arxiv.org/pdf/2505.24721, 2025-05-30
2025-11-09 16:53:10,441 - INFO - root - Page:2, Index:41, GARLIC: GAussian Representation LearnIng for spaCe partitioning, https://arxiv.org/pdf/2505.24608, 2025-10-02
2025-11-09 16:53:10,442 - INFO - root - Page:2, Index:42, AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up, https://arxiv.org/pdf/2505.24584, 2025-08-18
2025-11-09 16:53:10,446 - INFO - root - Page:2, Index:43, Model-Preserving Adaptive Rounding, https://arxiv.org/pdf/2505.22988, 2025-09-25
2025-11-09 16:53:10,452 - INFO - root - Page:2, Index:44, Highly Efficient and Effective LLMs with Multi-Boolean Architectures, https://arxiv.org/pdf/2505.22811, 2025-10-03
2025-11-09 16:53:10,458 - INFO - root - Page:2, Index:45, Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning, https://arxiv.org/pdf/2505.21591, 2025-05-27
2025-11-09 16:53:10,460 - INFO - root - Page:2, Index:46, Frequency Composition for Compressed and Domain-Adaptive Neural Networks, https://arxiv.org/pdf/2505.20890, 2025-05-27
2025-11-09 16:53:10,475 - INFO - root - Page:2, Index:47, MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition, https://arxiv.org/pdf/2505.20744, 2025-10-27
2025-11-09 16:53:10,476 - INFO - root - Page:2, Index:48, Optimizing edge AI models on HPC systems with the edge in the loop, https://arxiv.org/pdf/2505.19995, 2025-05-26
2025-11-09 16:53:10,480 - INFO - root - Fetching page 4 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=150
2025-11-09 16:53:12,568 - INFO - root - get_all_titles_from_web 
2025-11-09 16:53:12,569 - INFO - root - Page:3, Index:0, DPASyn: Mechanism-Aware Drug Synergy Prediction via Dual Attention and Precision-Aware Quantization, https://arxiv.org/pdf/2505.19144, 2025-09-20
2025-11-09 16:53:12,569 - INFO - root - Page:3, Index:1, Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding, https://arxiv.org/pdf/2505.18758, 2025-05-24
2025-11-09 16:53:12,570 - INFO - root - Page:3, Index:2, Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization, https://arxiv.org/pdf/2505.18113, 2025-05-23
2025-11-09 16:53:12,570 - INFO - root - Page:3, Index:3, Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs, https://arxiv.org/pdf/2505.17662, 2025-09-19
2025-11-09 16:53:12,571 - INFO - root - Page:3, Index:4, FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design, https://arxiv.org/pdf/2505.16335, 2025-05-22
2025-11-09 16:53:12,573 - INFO - root - Page:3, Index:5, Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control, https://arxiv.org/pdf/2505.15304, 2025-05-30
2025-11-09 16:53:12,575 - INFO - root - Page:3, Index:6, Scaling Law for Quantization-Aware Training, https://arxiv.org/pdf/2505.14302, 2025-05-20
2025-11-09 16:53:12,579 - INFO - root - Page:3, Index:7, Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs, https://arxiv.org/pdf/2505.13060, 2025-05-19
2025-11-09 16:53:12,580 - INFO - root - Page:3, Index:8, Deep Unfolding with Kernel-based Quantization in MIMO Detection, https://arxiv.org/pdf/2505.12736, 2025-05-19
2025-11-09 16:53:12,580 - INFO - root - Page:3, Index:9, FedHQ: Hybrid Runtime Quantization for Federated Learning, https://arxiv.org/pdf/2505.11982, 2025-05-17
2025-11-09 16:53:12,581 - INFO - root - Page:3, Index:10, QVGen: Pushing the Limit of Quantized Video Generative Models, https://arxiv.org/pdf/2505.11497, 2025-09-27
2025-11-09 16:53:12,582 - INFO - root - Page:3, Index:11, EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes, https://arxiv.org/pdf/2505.10787, 2025-05-15
2025-11-09 16:53:12,583 - INFO - root - Page:3, Index:12, ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition, https://arxiv.org/pdf/2505.08981, 2025-05-13
2025-11-09 16:53:12,595 - INFO - root - Page:3, Index:13, PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs, https://arxiv.org/pdf/2505.03254, 2025-08-06
2025-11-09 16:53:12,599 - INFO - root - Page:3, Index:14, ICQuant: Index Coding enables Low-bit LLM Quantization, https://arxiv.org/pdf/2505.00850, 2025-08-24
2025-11-09 16:53:12,606 - INFO - root - Page:3, Index:15, Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining, https://arxiv.org/pdf/2504.13932, 2025-07-30
2025-11-09 16:53:12,609 - INFO - root - Page:3, Index:16, Achieving binary weight and activation for LLMs using Post-Training Quantization, https://arxiv.org/pdf/2504.05352, 2025-06-30
2025-11-09 16:53:12,610 - INFO - root - Page:3, Index:17, Wireless Hearables With Programmable Speech AI Accelerators, https://arxiv.org/pdf/2503.18698, 2025-10-21
2025-11-09 16:53:12,611 - INFO - root - Page:3, Index:18, GranQ: Efficient Channel-wise Quantization via Vectorized Pre-Scaling for Zero-Shot QAT, https://arxiv.org/pdf/2503.18339, 2025-10-15
2025-11-09 16:53:12,611 - INFO - root - Page:3, Index:19, CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting, https://arxiv.org/pdf/2503.12836, 2025-09-29
2025-11-09 16:53:12,612 - INFO - root - Page:3, Index:20, AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model, https://arxiv.org/pdf/2503.03088, 2025-07-11
2025-11-09 16:53:12,612 - INFO - root - Page:3, Index:21, Teaching Metric Distance to Discrete Autoregressive Language Models, https://arxiv.org/pdf/2503.02379, 2025-10-07
2025-11-09 16:53:12,615 - INFO - root - Page:3, Index:22, Regularization-based Framework for Quantization-, Fault- and Variability-Aware Training, https://arxiv.org/pdf/2503.01297, 2025-07-12
2025-11-09 16:53:12,616 - INFO - root - Fetching page 5 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=200
2025-11-09 16:53:14,317 - INFO - root - get_all_titles_from_web 
2025-11-09 16:53:14,317 - INFO - root - Page:4, Index:0, Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models, https://arxiv.org/pdf/2502.15799, 2025-06-29
2025-11-09 16:53:14,318 - INFO - root - Page:4, Index:1, Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer, https://arxiv.org/pdf/2502.15779, 2025-09-01
2025-11-09 16:53:14,318 - INFO - root - Page:4, Index:2, RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models, https://arxiv.org/pdf/2502.09003, 2025-06-05
2025-11-09 16:53:14,318 - INFO - root - Page:4, Index:3, Spatial Degradation-Aware and Temporal Consistent Diffusion Model for Compressed Video Super-Resolution, https://arxiv.org/pdf/2502.07381, 2025-06-27
2025-11-09 16:53:14,319 - INFO - root - Page:4, Index:4, QuEST: Stable Training of LLMs with 1-Bit Weights and Activations, https://arxiv.org/pdf/2502.05003, 2025-06-10
2025-11-09 16:53:14,319 - INFO - root - Page:4, Index:5, HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs, https://arxiv.org/pdf/2501.02625, 2025-11-05
2025-11-09 16:53:14,320 - INFO - root - Page:4, Index:6, TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction, https://arxiv.org/pdf/2412.16919, 2025-08-08
2025-11-09 16:53:14,321 - INFO - root - Page:4, Index:7, Training Multi-Layer Binary Neural Networks With Local Binary Error Signals, https://arxiv.org/pdf/2412.00119, 2025-08-05
2025-11-09 16:53:14,321 - INFO - root - Page:4, Index:8, Scaling Large-scale GNN Training to Thousands of Processors on CPU-based Supercomputers, https://arxiv.org/pdf/2411.16025, 2025-05-26
2025-11-09 16:53:14,321 - INFO - root - Page:4, Index:9, HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization, https://arxiv.org/pdf/2411.06581, 2025-05-16
2025-11-09 16:53:14,322 - INFO - root - Fetching page 6 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=250
2025-11-09 16:53:16,346 - INFO - root - get_all_titles_from_web 
2025-11-09 16:53:16,346 - INFO - root - Page:5, Index:0, Gradient-Free Training of Quantized Neural Networks, https://arxiv.org/pdf/2410.09734, 2025-09-29
2025-11-09 16:53:16,347 - INFO - root - Page:5, Index:1, QT-DoG: Quantization-aware Training for Domain Generalization, https://arxiv.org/pdf/2410.06020, 2025-06-26
2025-11-09 16:53:16,347 - INFO - root - Page:5, Index:2, Constraint Guided Model Quantization of Neural Networks, https://arxiv.org/pdf/2409.20138, 2025-09-12
2025-11-09 16:53:16,347 - INFO - root - Page:5, Index:3, Accumulator-Aware Post-Training Quantization for Large Language Models, https://arxiv.org/pdf/2409.17092, 2025-07-30
2025-11-09 16:53:16,348 - INFO - root - Page:5, Index:4, DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation, https://arxiv.org/pdf/2409.14307, 2025-07-09
2025-11-09 16:53:16,348 - INFO - root - Page:5, Index:5, Robust Training of Neural Networks at Arbitrary Precision and Sparsity, https://arxiv.org/pdf/2409.09245, 2025-09-23
2025-11-09 16:53:16,348 - INFO - root - Page:5, Index:6, VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing, https://arxiv.org/pdf/2408.05758, 2025-05-27
2025-11-09 16:53:16,349 - INFO - root - Page:5, Index:7, DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers, https://arxiv.org/pdf/2408.03291, 2025-06-19
2025-11-09 16:53:16,349 - INFO - root - Page:5, Index:8, Temporal Feature Matters: A Framework for Diffusion Model Quantization, https://arxiv.org/pdf/2407.19547, 2025-07-11
2025-11-09 16:53:16,349 - INFO - root - Page:5, Index:9, EfficientQAT: Efficient Quantization-Aware Training for Large Language Models, https://arxiv.org/pdf/2407.11062, 2025-05-19
2025-11-09 16:53:16,350 - INFO - root - Page:5, Index:10, Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT, https://arxiv.org/pdf/2407.11041, 2025-10-31
2025-11-09 16:53:16,350 - INFO - root - Fetching page 7 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=300
2025-11-09 16:53:18,928 - INFO - root - get_all_titles_from_web 
2025-11-09 16:53:18,929 - INFO - root - Page:6, Index:0, BoA: Attention-aware Post-training Quantization without Backpropagation, https://arxiv.org/pdf/2406.13474, 2025-06-06
2025-11-09 16:53:18,929 - INFO - root - Page:6, Index:1, A Rescaling-Invariant Lipschitz Bound Based on Path-Metrics for Modern ReLU Network Parameterizations, https://arxiv.org/pdf/2405.15006, 2025-06-13
2025-11-09 16:53:18,930 - INFO - root - Page:6, Index:2, Scheduling Weight Transitions for Quantization-Aware Training, https://arxiv.org/pdf/2404.19248, 2025-10-01
2025-11-09 16:53:18,930 - INFO - root - Page:6, Index:3, GPTVQ: The Blessing of Dimensionality for LLM Quantization, https://arxiv.org/pdf/2402.15319, 2025-06-03
2025-11-09 16:53:18,930 - INFO - root - Fetching page 8 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=350
2025-11-09 16:53:21,281 - INFO - root - get_all_titles_from_web 
2025-11-09 16:53:21,281 - INFO - root - Page:7, Index:0, Squat: Quant Small Language Models on the Edge, https://arxiv.org/pdf/2402.10787, 2025-07-01
2025-11-09 16:53:21,281 - INFO - root - Page:7, Index:1, L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2402.04902, 2025-07-21
2025-11-09 16:53:21,282 - INFO - root - Fetching page 9 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=400
2025-11-09 16:53:23,427 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 16:53:50,568 - INFO - root - 正在总结论文 1/10: A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies
2025-11-09 16:54:10,990 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 16:55:27,773 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 16:56:14,124 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 16:56:14,128 - INFO - root - 正在提取论文图片...
2025-11-09 16:56:18,215 - INFO - root - 已保存图片 1/10：./export\images_A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat\figure_1_page4.png
2025-11-09 16:56:18,531 - INFO - root - 已保存图片 2/10：./export\images_A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat\figure_2_page4.png
2025-11-09 16:56:18,874 - INFO - root - 已保存图片 3/10：./export\images_A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat\figure_3_page4.png
2025-11-09 16:56:19,094 - INFO - root - 已保存图片 4/10：./export\images_A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat\figure_4_page5.png
2025-11-09 16:56:19,201 - INFO - root - 已保存图片 5/10：./export\images_A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat\figure_5_page3.png
2025-11-09 16:56:19,379 - INFO - root - 已保存图片 6/10：./export\images_A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat\figure_6_page4.png
2025-11-09 16:56:19,381 - INFO - root - 成功添加图片 1：./export\images_A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat\figure_1_page4.png
2025-11-09 16:56:19,382 - INFO - root - 成功添加图片 2：./export\images_A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat\figure_2_page4.png
2025-11-09 16:56:19,382 - INFO - root - 成功添加图片 3：./export\images_A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat\figure_3_page4.png
2025-11-09 16:56:19,383 - INFO - root - 成功添加图片 4：./export\images_A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat\figure_4_page5.png
2025-11-09 16:56:19,383 - INFO - root - 成功添加图片 5：./export\images_A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat\figure_5_page3.png
2025-11-09 16:56:19,384 - INFO - root - 成功添加图片 6：./export\images_A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat\figure_6_page4.png
2025-11-09 16:56:19,393 - INFO - root - 论文《A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies》的分析已保存到 ./export\A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat.md
2025-11-09 16:56:19,415 - INFO - root - 正在总结论文 2/10: FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error
2025-11-09 16:56:42,330 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 16:56:57,262 - INFO - openai._base_client - Retrying request to /chat/completions in 0.390960 seconds
2025-11-09 16:56:59,684 - INFO - openai._base_client - Retrying request to /chat/completions in 0.900756 seconds
2025-11-09 16:57:02,610 - ERROR - root - DoubaoClient: generation error: Connection error.
Traceback (most recent call last):
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 256, in handle_request
    raise exc from None
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\http_proxy.py", line 288, in handle_request
    connect_response = self._connection.handle_request(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 101, in handle_request
    raise exc
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 78, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 124, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_backends\sync.py", line 207, in connect_tcp
    with map_exceptions(exc_map):
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_base_client.py", line 982, in request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 249, in handle_request
    with map_httpcore_exceptions():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ChatPaper\llm_client_merged.py", line 632, in generate
    completion = self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_utils\_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1156, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_base_client.py", line 1014, in request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2025-11-09 16:57:02,650 - WARNING - root - DoubaoClient: 网络连接问题 检测到，等待 60 秒后重试
2025-11-09 16:58:02,653 - INFO - root - DoubaoClient: retry attempt 2 for generation
2025-11-09 16:58:04,699 - INFO - openai._base_client - Retrying request to /chat/completions in 0.449534 seconds
2025-11-09 16:58:07,192 - INFO - openai._base_client - Retrying request to /chat/completions in 0.840435 seconds
2025-11-09 16:58:10,065 - ERROR - root - DoubaoClient: generation error: Connection error.
Traceback (most recent call last):
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 256, in handle_request
    raise exc from None
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\http_proxy.py", line 288, in handle_request
    connect_response = self._connection.handle_request(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 101, in handle_request
    raise exc
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 78, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 124, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_backends\sync.py", line 207, in connect_tcp
    with map_exceptions(exc_map):
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_base_client.py", line 982, in request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 249, in handle_request
    with map_httpcore_exceptions():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ChatPaper\llm_client_merged.py", line 632, in generate
    completion = self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_utils\_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1156, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_base_client.py", line 1014, in request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2025-11-09 16:58:10,074 - WARNING - root - DoubaoClient: 网络连接问题 检测到，等待 60 秒后重试
2025-11-09 16:59:10,077 - INFO - root - DoubaoClient: retry attempt 3 for generation
2025-11-09 16:59:12,128 - INFO - openai._base_client - Retrying request to /chat/completions in 0.419260 seconds
2025-11-09 16:59:14,569 - INFO - openai._base_client - Retrying request to /chat/completions in 0.863384 seconds
2025-11-09 16:59:17,468 - ERROR - root - DoubaoClient: generation error: Connection error.
Traceback (most recent call last):
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 256, in handle_request
    raise exc from None
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\http_proxy.py", line 288, in handle_request
    connect_response = self._connection.handle_request(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 101, in handle_request
    raise exc
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 78, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 124, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_backends\sync.py", line 207, in connect_tcp
    with map_exceptions(exc_map):
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_base_client.py", line 982, in request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 249, in handle_request
    with map_httpcore_exceptions():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ChatPaper\llm_client_merged.py", line 632, in generate
    completion = self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_utils\_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1156, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_base_client.py", line 1014, in request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2025-11-09 16:59:17,480 - ERROR - root - DoubaoClient: 最终失败 - 抱歉，豆包生成内容时遇到问题：Connection error.
2025-11-09 16:59:19,520 - INFO - openai._base_client - Retrying request to /chat/completions in 0.457668 seconds
2025-11-09 16:59:22,010 - INFO - openai._base_client - Retrying request to /chat/completions in 0.784081 seconds
2025-11-09 16:59:24,810 - ERROR - root - DoubaoClient: generation error: Connection error.
Traceback (most recent call last):
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 256, in handle_request
    raise exc from None
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\http_proxy.py", line 288, in handle_request
    connect_response = self._connection.handle_request(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 101, in handle_request
    raise exc
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 78, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 124, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_backends\sync.py", line 207, in connect_tcp
    with map_exceptions(exc_map):
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "D:\ChatPaper\.venv\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_base_client.py", line 982, in request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 249, in handle_request
    with map_httpcore_exceptions():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "D:\ChatPaper\.venv\Lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ChatPaper\llm_client_merged.py", line 632, in generate
    completion = self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_utils\_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1156, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\ChatPaper\.venv\Lib\site-packages\openai\_base_client.py", line 1014, in request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2025-11-09 16:59:24,818 - WARNING - root - DoubaoClient: 网络连接问题 检测到，等待 60 秒后重试
2025-11-09 17:12:12,059 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 17:12:12,062 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 17:12:12,065 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 17:25:58,418 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 17:25:58,423 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 17:25:58,426 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 17:36:52,097 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 17:36:52,104 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 17:36:52,110 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 17:36:54,026 - INFO - root - LLMClientManager: 指定使用客户端: Doubao
2025-11-09 17:36:54,028 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 17:36:58,431 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:36:58,453 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 17:36:58,454 - INFO - root - LLMClientManager: Doubao 客户端初始化成功
2025-11-09 17:36:58,454 - INFO - root - LLMClientManager: switched to Doubao client
2025-11-09 17:36:58,454 - INFO - root - 已手动切换到 LLM 客户端: Doubao
2025-11-09 17:36:58,454 - INFO - root - 使用 LLM 模型: doubao-seed-1-6-lite-251015
2025-11-09 17:36:58,454 - INFO - root - 可用客户端: ['Doubao']
2025-11-09 17:36:58,455 - INFO - root - === 运行配置 ===
2025-11-09 17:36:58,455 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 17:36:58,455 - INFO - root - 关键词: QAT
2025-11-09 17:36:58,456 - INFO - root - 查询: Quantization-Aware-Training
2025-11-09 17:36:58,456 - INFO - root - 排序: None
2025-11-09 17:36:58,456 - INFO - root - 最近天数: 180
2025-11-09 17:36:58,457 - INFO - root - 最大处理数量: 20
2025-11-09 17:36:58,457 - INFO - root - 保存图片: 是
2025-11-09 17:36:58,457 - INFO - root - 输出语言: 中文
2025-11-09 17:36:58,457 - INFO - root - 强制重新处理: 否
2025-11-09 17:36:58,457 - INFO - root - ====================
2025-11-09 17:36:58,457 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 17:36:58,459 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 17:37:16,996 - INFO - root - get_all_titles_from_web 
2025-11-09 17:37:16,997 - INFO - root - Page:0, Index:0, A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies, https://arxiv.org/pdf/2511.03201, 2025-11-05
2025-11-09 17:37:16,997 - INFO - root - Page:0, Index:1, FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error, https://arxiv.org/pdf/2511.02302, 2025-11-04
2025-11-09 17:37:16,997 - INFO - root - Page:0, Index:2, Outlier-Aware Post-Training Quantization for Image Super-Resolution, https://arxiv.org/pdf/2511.00682, 2025-11-01
2025-11-09 17:37:16,998 - INFO - root - Page:0, Index:3, Evaluation of Wafer-Scale SOT-MRAM for Analog Crossbar Array Applications, https://arxiv.org/pdf/2510.25853, 2025-11-03
2025-11-09 17:37:16,999 - INFO - root - Page:0, Index:4, Improving the Straight-Through Estimator with Zeroth-Order Information, https://arxiv.org/pdf/2510.23926, 2025-10-27
2025-11-09 17:37:16,999 - INFO - root - Page:0, Index:5, Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework, https://arxiv.org/pdf/2510.22243, 2025-10-25
2025-11-09 17:37:17,000 - INFO - root - Page:0, Index:6, TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge, https://arxiv.org/pdf/2510.21879, 2025-10-23
2025-11-09 17:37:17,000 - INFO - root - Page:0, Index:7, KARIPAP: Quantum-Inspired Tensor Network Compression of Large Language Models Using Infinite Projected Entangled Pair States and Tensor Renormalization Group, https://arxiv.org/pdf/2510.21844, 2025-10-22
2025-11-09 17:37:17,001 - INFO - root - Page:0, Index:8, A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization, https://arxiv.org/pdf/2510.21314, 2025-10-24
2025-11-09 17:37:17,001 - INFO - root - Page:0, Index:9, Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks, https://arxiv.org/pdf/2510.19760, 2025-10-22
2025-11-09 17:37:17,001 - INFO - root - Page:0, Index:10, CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training, https://arxiv.org/pdf/2510.18784, 2025-10-21
2025-11-09 17:37:17,002 - INFO - root - Page:0, Index:11, Mixed-Precision Quantization for Language Models: Techniques and Prospects, https://arxiv.org/pdf/2510.16805, 2025-10-19
2025-11-09 17:37:17,002 - INFO - root - Page:0, Index:12, SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation, https://arxiv.org/pdf/2510.16396, 2025-10-30
2025-11-09 17:37:17,002 - INFO - root - Page:0, Index:13, CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models, https://arxiv.org/pdf/2510.15962, 2025-10-11
2025-11-09 17:37:17,003 - INFO - root - Page:0, Index:14, SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization, https://arxiv.org/pdf/2510.15775, 2025-10-17
2025-11-09 17:37:17,004 - INFO - root - Page:0, Index:15, SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware, https://arxiv.org/pdf/2510.15542, 2025-10-31
2025-11-09 17:37:17,006 - INFO - root - Page:0, Index:16, GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework, https://arxiv.org/pdf/2510.15299, 2025-10-17
2025-11-09 17:37:17,007 - INFO - root - Page:0, Index:17, SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images, https://arxiv.org/pdf/2510.15072, 2025-10-16
2025-11-09 17:37:17,007 - INFO - root - Page:0, Index:18, FraQAT: Quantization Aware Training with Fractional bits, https://arxiv.org/pdf/2510.14823, 2025-10-16
2025-11-09 17:37:17,007 - INFO - root - Page:0, Index:19, Computing-In-Memory Aware Model Adaption For Edge Devices, https://arxiv.org/pdf/2510.14379, 2025-10-16
2025-11-09 17:37:17,007 - INFO - root - Page:0, Index:20, Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge, https://arxiv.org/pdf/2510.13760, 2025-11-04
2025-11-09 17:37:17,009 - INFO - root - Page:0, Index:21, NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models, https://arxiv.org/pdf/2510.13068, 2025-10-14
2025-11-09 17:37:17,009 - INFO - root - Page:0, Index:22, Detect Anything via Next Point Prediction, https://arxiv.org/pdf/2510.12798, 2025-10-14
2025-11-09 17:37:17,013 - INFO - root - Page:0, Index:23, AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model, https://arxiv.org/pdf/2510.11496, 2025-10-14
2025-11-09 17:37:17,042 - INFO - root - Page:0, Index:24, Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware, https://arxiv.org/pdf/2510.11484, 2025-10-13
2025-11-09 17:37:17,044 - INFO - root - Page:0, Index:25, SASER: Stego attacks on open-source LLMs, https://arxiv.org/pdf/2510.10486, 2025-10-12
2025-11-09 17:37:17,046 - INFO - root - Page:0, Index:26, Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition, https://arxiv.org/pdf/2510.09653, 2025-10-15
2025-11-09 17:37:17,047 - INFO - root - Page:0, Index:27, A Theoretically-Grounded Codebook for Digital Semantic Communications, https://arxiv.org/pdf/2510.07108, 2025-10-14
2025-11-09 17:37:17,047 - INFO - root - Page:0, Index:28, QuantDemoire: Quantization with Outlier Aware for Image Demoiréing, https://arxiv.org/pdf/2510.04066, 2025-10-05
2025-11-09 17:37:17,047 - INFO - root - Page:0, Index:29, Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization, https://arxiv.org/pdf/2510.03763, 2025-10-04
2025-11-09 17:37:17,047 - INFO - root - Page:0, Index:30, Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models, https://arxiv.org/pdf/2510.03274, 2025-09-27
2025-11-09 17:37:17,049 - INFO - root - Page:0, Index:31, PT$^2$-LLM: Post-Training Ternarization for Large Language Models, https://arxiv.org/pdf/2510.03267, 2025-09-26
2025-11-09 17:37:17,053 - INFO - root - Page:0, Index:32, Purrception: Variational Flow Matching for Vector-Quantized Image Generation, https://arxiv.org/pdf/2510.01478, 2025-10-01
2025-11-09 17:37:17,054 - INFO - root - Page:0, Index:33, Post-Training Quantization for Audio Diffusion Transformers, https://arxiv.org/pdf/2510.00313, 2025-09-30
2025-11-09 17:37:17,055 - INFO - root - Page:0, Index:34, Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling, https://arxiv.org/pdf/2510.00028, 2025-09-25
2025-11-09 17:37:17,056 - INFO - root - Page:0, Index:35, Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models, https://arxiv.org/pdf/2509.26436, 2025-09-30
2025-11-09 17:37:17,056 - INFO - root - Page:0, Index:36, Cat: Post-Training Quantization Error Reduction via Cluster-based Affine Transformation, https://arxiv.org/pdf/2509.26277, 2025-10-07
2025-11-09 17:37:17,057 - INFO - root - Page:0, Index:37, CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models, https://arxiv.org/pdf/2509.25996, 2025-09-30
2025-11-09 17:37:17,057 - INFO - root - Page:0, Index:38, Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications, https://arxiv.org/pdf/2509.25439, 2025-09-29
2025-11-09 17:37:17,072 - INFO - root - Page:0, Index:39, On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs, https://arxiv.org/pdf/2509.25214, 2025-09-22
2025-11-09 17:37:17,076 - INFO - root - Page:0, Index:40, VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning, https://arxiv.org/pdf/2509.24650, 2025-09-29
2025-11-09 17:37:17,076 - INFO - root - Page:0, Index:41, S$^2$NN: Sub-bit Spiking Neural Networks, https://arxiv.org/pdf/2509.24266, 2025-10-24
2025-11-09 17:37:17,077 - INFO - root - Page:0, Index:42, Tequila: Trapping-free Ternary Quantization for Large Language Models, https://arxiv.org/pdf/2509.23809, 2025-10-17
2025-11-09 17:37:17,077 - INFO - root - Page:0, Index:43, Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution, https://arxiv.org/pdf/2509.23774, 2025-09-30
2025-11-09 17:37:17,077 - INFO - root - Page:0, Index:44, RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization, https://arxiv.org/pdf/2509.23582, 2025-09-27
2025-11-09 17:37:17,078 - INFO - root - Page:0, Index:45, Beyond Outliers: A Study of Optimizers Under Quantization, https://arxiv.org/pdf/2509.23500, 2025-10-02
2025-11-09 17:37:17,078 - INFO - root - Page:0, Index:46, Compute-Optimal Quantization-Aware Training, https://arxiv.org/pdf/2509.22935, 2025-09-26
2025-11-09 17:37:17,079 - INFO - root - Page:0, Index:47, COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning, https://arxiv.org/pdf/2509.22075, 2025-10-06
2025-11-09 17:37:17,079 - INFO - root - Page:0, Index:48, SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models, https://arxiv.org/pdf/2509.21498, 2025-09-25
2025-11-09 17:37:17,086 - INFO - root - Page:0, Index:49, Quantized Visual Geometry Grounded Transformer, https://arxiv.org/pdf/2509.21302, 2025-09-29
2025-11-09 17:37:17,089 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 17:37:39,510 - INFO - root - get_all_titles_from_web 
2025-11-09 17:37:39,510 - INFO - root - Page:1, Index:0, Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy, https://arxiv.org/pdf/2509.21173, 2025-10-27
2025-11-09 17:37:39,510 - INFO - root - Page:1, Index:1, Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer, https://arxiv.org/pdf/2509.20854, 2025-09-25
2025-11-09 17:37:39,512 - INFO - root - Page:1, Index:2, TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection, https://arxiv.org/pdf/2509.18193, 2025-09-19
2025-11-09 17:37:39,512 - INFO - root - Page:1, Index:3, SBVR: Summation of BitVector Representation for Efficient LLM Quantization, https://arxiv.org/pdf/2509.18172, 2025-09-17
2025-11-09 17:37:39,512 - INFO - root - Page:1, Index:4, QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2509.17428, 2025-09-26
2025-11-09 17:37:39,512 - INFO - root - Page:1, Index:5, PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models, https://arxiv.org/pdf/2509.16989, 2025-10-28
2025-11-09 17:37:39,513 - INFO - root - Page:1, Index:6, MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training, https://arxiv.org/pdf/2509.15514, 2025-09-18
2025-11-09 17:37:39,513 - INFO - root - Page:1, Index:7, Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs, https://arxiv.org/pdf/2509.14391, 2025-09-17
2025-11-09 17:37:39,513 - INFO - root - Page:1, Index:8, Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization, https://arxiv.org/pdf/2509.13786, 2025-09-19
2025-11-09 17:37:39,514 - INFO - root - Page:1, Index:9, Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees, https://arxiv.org/pdf/2509.11054, 2025-09-13
2025-11-09 17:37:39,514 - INFO - root - Page:1, Index:10, SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models, https://arxiv.org/pdf/2509.09090, 2025-09-10
2025-11-09 17:37:39,514 - INFO - root - Page:1, Index:11, CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks with Entropy Regularization, https://arxiv.org/pdf/2509.08776, 2025-09-10
2025-11-09 17:37:39,514 - INFO - root - Page:1, Index:12, Explaining How Quantization Disparately Skews a Model, https://arxiv.org/pdf/2509.07222, 2025-09-08
2025-11-09 17:37:39,515 - INFO - root - Page:1, Index:13, LoaQ: Layer-wise Output Approximation Quantization, https://arxiv.org/pdf/2509.06297, 2025-09-07
2025-11-09 17:37:39,515 - INFO - root - Page:1, Index:14, FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving, https://arxiv.org/pdf/2509.06261, 2025-09-14
2025-11-09 17:37:39,515 - INFO - root - Page:1, Index:15, Sensitivity-Aware Post-Training Quantization for Deep Neural Networks, https://arxiv.org/pdf/2509.05576, 2025-09-05
2025-11-09 17:37:39,515 - INFO - root - Page:1, Index:16, SuperSNN: A Hardware-Aware Framework for Physically Realizable, High-Performance Superconducting Spiking Neural Network Chips, https://arxiv.org/pdf/2509.05532, 2025-09-05
2025-11-09 17:37:39,517 - INFO - root - Page:1, Index:17, Data-Augmented Quantization-Aware Knowledge Distillation, https://arxiv.org/pdf/2509.03850, 2025-09-03
2025-11-09 17:37:39,517 - INFO - root - Page:1, Index:18, DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling, https://arxiv.org/pdf/2509.03472, 2025-09-03
2025-11-09 17:37:39,517 - INFO - root - Page:1, Index:19, Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs, https://arxiv.org/pdf/2509.02017, 2025-09-02
2025-11-09 17:37:39,518 - INFO - root - Page:1, Index:20, Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling, https://arxiv.org/pdf/2509.01624, 2025-09-01
2025-11-09 17:37:39,518 - INFO - root - Page:1, Index:21, Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective, https://arxiv.org/pdf/2509.00859, 2025-08-31
2025-11-09 17:37:39,519 - INFO - root - Page:1, Index:22, Progressive Element-wise Gradient Estimation for Neural Network Quantization, https://arxiv.org/pdf/2509.00097, 2025-08-27
2025-11-09 17:37:39,519 - INFO - root - Page:1, Index:23, End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost, https://arxiv.org/pdf/2509.00031, 2025-09-29
2025-11-09 17:37:39,519 - INFO - root - Page:1, Index:24, Quantization Robustness to Input Degradations for Object Detection, https://arxiv.org/pdf/2508.19600, 2025-08-27
2025-11-09 17:37:39,520 - INFO - root - Page:1, Index:25, Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models, https://arxiv.org/pdf/2508.18609, 2025-08-27
2025-11-09 17:37:39,521 - INFO - root - Page:1, Index:26, AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration, https://arxiv.org/pdf/2508.18025, 2025-08-25
2025-11-09 17:37:39,522 - INFO - root - Page:1, Index:27, TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling, https://arxiv.org/pdf/2508.16790, 2025-08-22
2025-11-09 17:37:39,522 - INFO - root - Page:1, Index:28, A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering, https://arxiv.org/pdf/2508.16516, 2025-08-22
2025-11-09 17:37:39,523 - INFO - root - Page:1, Index:29, JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs, https://arxiv.org/pdf/2508.15468, 2025-08-21
2025-11-09 17:37:39,523 - INFO - root - Page:1, Index:30, MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation, https://arxiv.org/pdf/2508.15281, 2025-08-21
2025-11-09 17:37:39,524 - INFO - root - Page:1, Index:31, DLLMQuant: Quantizing Diffusion-based Large Language Models, https://arxiv.org/pdf/2508.14090, 2025-08-25
2025-11-09 17:37:39,524 - INFO - root - Page:1, Index:32, Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management, https://arxiv.org/pdf/2508.13905, 2025-08-19
2025-11-09 17:37:39,525 - INFO - root - Page:1, Index:33, XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads, https://arxiv.org/pdf/2508.13049, 2025-08-18
2025-11-09 17:37:39,526 - INFO - root - Page:1, Index:34, Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection, https://arxiv.org/pdf/2508.12230, 2025-08-17
2025-11-09 17:37:39,530 - INFO - root - Page:1, Index:35, TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks, https://arxiv.org/pdf/2508.12132, 2025-08-16
2025-11-09 17:37:39,534 - INFO - root - Page:1, Index:36, Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion, https://arxiv.org/pdf/2508.12094, 2025-10-13
2025-11-09 17:37:39,534 - INFO - root - Page:1, Index:37, Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware, https://arxiv.org/pdf/2508.11940, 2025-08-16
2025-11-09 17:37:39,535 - INFO - root - Page:1, Index:38, PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks, https://arxiv.org/pdf/2508.10557, 2025-08-15
2025-11-09 17:37:39,536 - INFO - root - Page:1, Index:39, MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI, https://arxiv.org/pdf/2508.09500, 2025-08-13
2025-11-09 17:37:39,537 - INFO - root - Page:1, Index:40, SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via Codebooks for recommender system, https://arxiv.org/pdf/2508.09090, 2025-08-12
2025-11-09 17:37:39,537 - INFO - root - Page:1, Index:41, Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models, https://arxiv.org/pdf/2508.06974, 2025-08-09
2025-11-09 17:37:39,542 - INFO - root - Page:1, Index:42, Efficient Deep Neural Receiver with Post-Training Quantization, https://arxiv.org/pdf/2508.06275, 2025-08-08
2025-11-09 17:37:39,545 - INFO - root - Page:1, Index:43, iFairy: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$, https://arxiv.org/pdf/2508.05571, 2025-08-16
2025-11-09 17:37:39,546 - INFO - root - Page:1, Index:44, S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation, https://arxiv.org/pdf/2508.04016, 2025-10-27
2025-11-09 17:37:39,547 - INFO - root - Page:1, Index:45, LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation, https://arxiv.org/pdf/2508.03485, 2025-09-23
2025-11-09 17:37:39,547 - INFO - root - Page:1, Index:46, VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation, https://arxiv.org/pdf/2508.03351, 2025-08-05
2025-11-09 17:37:39,547 - INFO - root - Page:1, Index:47, TF-MLPNet: Tiny Real-Time Neural Speech Separation, https://arxiv.org/pdf/2508.03047, 2025-08-04
2025-11-09 17:37:39,548 - INFO - root - Page:1, Index:48, SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation, https://arxiv.org/pdf/2508.01375, 2025-08-02
2025-11-09 17:37:39,548 - INFO - root - Page:1, Index:49, MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective, https://arxiv.org/pdf/2507.19131, 2025-07-25
2025-11-09 17:37:39,549 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 17:37:50,057 - INFO - root - get_all_titles_from_web 
2025-11-09 17:37:50,058 - INFO - root - Page:2, Index:0, Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction, https://arxiv.org/pdf/2507.17768, 2025-07-16
2025-11-09 17:37:50,058 - INFO - root - Page:2, Index:1, SiLQ: Simple Large Language Model Quantization-Aware Training, https://arxiv.org/pdf/2507.16933, 2025-07-22
2025-11-09 17:37:50,059 - INFO - root - Page:2, Index:2, Task-Specific Zero-shot Quantization-Aware Training for Object Detection, https://arxiv.org/pdf/2507.16782, 2025-07-22
2025-11-09 17:37:50,059 - INFO - root - Page:2, Index:3, TorchAO: PyTorch-Native Training-to-Serving Model Optimization, https://arxiv.org/pdf/2507.16099, 2025-07-21
2025-11-09 17:37:50,059 - INFO - root - Page:2, Index:4, SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models, https://arxiv.org/pdf/2507.14811, 2025-08-27
2025-11-09 17:37:50,059 - INFO - root - Page:2, Index:5, Apple Intelligence Foundation Language Models: Tech Report 2025, https://arxiv.org/pdf/2507.13575, 2025-08-27
2025-11-09 17:37:50,060 - INFO - root - Page:2, Index:6, Generative Multi-Target Cross-Domain Recommendation, https://arxiv.org/pdf/2507.12871, 2025-08-07
2025-11-09 17:37:50,060 - INFO - root - Page:2, Index:7, DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training, https://arxiv.org/pdf/2507.07149, 2025-07-09
2025-11-09 17:37:50,060 - INFO - root - Page:2, Index:8, Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics, https://arxiv.org/pdf/2507.07044, 2025-07-15
2025-11-09 17:37:50,060 - INFO - root - Page:2, Index:9, QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models, https://arxiv.org/pdf/2507.06079, 2025-07-08
2025-11-09 17:37:50,062 - INFO - root - Page:2, Index:10, GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field, https://arxiv.org/pdf/2507.05594, 2025-07-07
2025-11-09 17:37:50,062 - INFO - root - Page:2, Index:11, Hita: Holistic Tokenizer for Autoregressive Image Generation, https://arxiv.org/pdf/2507.02358, 2025-07-11
2025-11-09 17:37:50,062 - INFO - root - Page:2, Index:12, QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference, https://arxiv.org/pdf/2506.23934, 2025-06-30
2025-11-09 17:37:50,063 - INFO - root - Page:2, Index:13, FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization, https://arxiv.org/pdf/2506.23516, 2025-07-21
2025-11-09 17:37:50,064 - INFO - root - Page:2, Index:14, Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models, https://arxiv.org/pdf/2506.23025, 2025-06-28
2025-11-09 17:37:50,064 - INFO - root - Page:2, Index:15, Joint Quantization and Pruning Neural Networks Approach: A Case Study on FSO Receivers, https://arxiv.org/pdf/2506.20084, 2025-06-24
2025-11-09 17:37:50,066 - INFO - root - Page:2, Index:16, Single-step Diffusion for Image Compression at Ultra-Low Bitrates, https://arxiv.org/pdf/2506.16572, 2025-09-22
2025-11-09 17:37:50,066 - INFO - root - Page:2, Index:17, LittleBit: Ultra Low-Bit Quantization via Latent Factorization, https://arxiv.org/pdf/2506.13771, 2025-10-28
2025-11-09 17:37:50,067 - INFO - root - Page:2, Index:18, MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering, https://arxiv.org/pdf/2506.13755, 2025-06-16
2025-11-09 17:37:50,067 - INFO - root - Page:2, Index:19, EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization, https://arxiv.org/pdf/2506.13329, 2025-07-04
2025-11-09 17:37:50,068 - INFO - root - Page:2, Index:20, Towards Neural Audio Codec Source Parsing, https://arxiv.org/pdf/2506.12627, 2025-06-14
2025-11-09 17:37:50,068 - INFO - root - Page:2, Index:21, Quantizing Small-Scale State-Space Models for Edge AI, https://arxiv.org/pdf/2506.12480, 2025-06-14
2025-11-09 17:37:50,069 - INFO - root - Page:2, Index:22, EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction, https://arxiv.org/pdf/2506.12015, 2025-06-13
2025-11-09 17:37:50,069 - INFO - root - Page:2, Index:23, Compression Aware Certified Training, https://arxiv.org/pdf/2506.11992, 2025-06-13
2025-11-09 17:37:50,069 - INFO - root - Page:2, Index:24, GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers, https://arxiv.org/pdf/2506.11784, 2025-06-13
2025-11-09 17:37:50,070 - INFO - root - Page:2, Index:25, TruncQuant: Truncation-Ready Quantization for DNNs with Flexible Weight Bit Precision, https://arxiv.org/pdf/2506.11431, 2025-06-12
2025-11-09 17:37:50,071 - INFO - root - Page:2, Index:26, EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices, https://arxiv.org/pdf/2506.11093, 2025-06-05
2025-11-09 17:37:50,072 - INFO - root - Page:2, Index:27, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 17:37:50,072 - INFO - root - Page:2, Index:28, AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent, https://arxiv.org/pdf/2506.10205, 2025-06-11
2025-11-09 17:37:50,072 - INFO - root - Page:2, Index:29, Q-SAM2: Accurate Quantization for Segment Anything Model 2, https://arxiv.org/pdf/2506.09782, 2025-06-11
2025-11-09 17:37:50,074 - INFO - root - Page:2, Index:30, Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs, https://arxiv.org/pdf/2506.09104, 2025-06-10
2025-11-09 17:37:50,078 - INFO - root - Page:2, Index:31, Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU, https://arxiv.org/pdf/2506.08911, 2025-06-10
2025-11-09 17:37:50,081 - INFO - root - Page:2, Index:32, POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration, https://arxiv.org/pdf/2506.08785, 2025-06-10
2025-11-09 17:37:50,084 - INFO - root - Page:2, Index:33, MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts, https://arxiv.org/pdf/2506.07533, 2025-06-09
2025-11-09 17:37:50,088 - INFO - root - Page:2, Index:34, BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation, https://arxiv.org/pdf/2506.07530, 2025-06-09
2025-11-09 17:37:50,094 - INFO - root - Page:2, Index:35, RecGPT: A Foundation Model for Sequential Recommendation, https://arxiv.org/pdf/2506.06270, 2025-06-12
2025-11-09 17:37:50,096 - INFO - root - Page:2, Index:36, FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion, https://arxiv.org/pdf/2506.04648, 2025-06-05
2025-11-09 17:37:50,097 - INFO - root - Page:2, Index:37, BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing, https://arxiv.org/pdf/2506.03515, 2025-06-03
2025-11-09 17:37:50,098 - INFO - root - Page:2, Index:38, ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding, https://arxiv.org/pdf/2506.01853, 2025-06-02
2025-11-09 17:37:50,098 - INFO - root - Page:2, Index:39, Movable Antenna Enhanced Federated Fine-Tuning of Large Language Models via Hybrid Client Selection Optimization, https://arxiv.org/pdf/2506.00011, 2025-10-26
2025-11-09 17:37:50,114 - INFO - root - Page:2, Index:40, Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach, https://arxiv.org/pdf/2505.24721, 2025-05-30
2025-11-09 17:37:50,119 - INFO - root - Page:2, Index:41, GARLIC: GAussian Representation LearnIng for spaCe partitioning, https://arxiv.org/pdf/2505.24608, 2025-10-02
2025-11-09 17:37:50,121 - INFO - root - Page:2, Index:42, AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up, https://arxiv.org/pdf/2505.24584, 2025-08-18
2025-11-09 17:37:50,122 - INFO - root - Page:2, Index:43, Model-Preserving Adaptive Rounding, https://arxiv.org/pdf/2505.22988, 2025-09-25
2025-11-09 17:37:50,123 - INFO - root - Page:2, Index:44, Highly Efficient and Effective LLMs with Multi-Boolean Architectures, https://arxiv.org/pdf/2505.22811, 2025-10-03
2025-11-09 17:37:50,123 - INFO - root - Page:2, Index:45, Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning, https://arxiv.org/pdf/2505.21591, 2025-05-27
2025-11-09 17:37:50,123 - INFO - root - Page:2, Index:46, Frequency Composition for Compressed and Domain-Adaptive Neural Networks, https://arxiv.org/pdf/2505.20890, 2025-05-27
2025-11-09 17:37:50,124 - INFO - root - Page:2, Index:47, MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition, https://arxiv.org/pdf/2505.20744, 2025-10-27
2025-11-09 17:37:50,125 - INFO - root - Page:2, Index:48, Optimizing edge AI models on HPC systems with the edge in the loop, https://arxiv.org/pdf/2505.19995, 2025-05-26
2025-11-09 17:37:50,126 - INFO - root - Fetching page 4 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=150
2025-11-09 17:38:07,293 - INFO - root - get_all_titles_from_web 
2025-11-09 17:38:07,294 - INFO - root - Page:3, Index:0, DPASyn: Mechanism-Aware Drug Synergy Prediction via Dual Attention and Precision-Aware Quantization, https://arxiv.org/pdf/2505.19144, 2025-09-20
2025-11-09 17:38:07,294 - INFO - root - Page:3, Index:1, Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding, https://arxiv.org/pdf/2505.18758, 2025-05-24
2025-11-09 17:38:07,294 - INFO - root - Page:3, Index:2, Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization, https://arxiv.org/pdf/2505.18113, 2025-05-23
2025-11-09 17:38:07,295 - INFO - root - Page:3, Index:3, Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs, https://arxiv.org/pdf/2505.17662, 2025-09-19
2025-11-09 17:38:07,295 - INFO - root - Page:3, Index:4, FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design, https://arxiv.org/pdf/2505.16335, 2025-05-22
2025-11-09 17:38:07,295 - INFO - root - Page:3, Index:5, Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control, https://arxiv.org/pdf/2505.15304, 2025-05-30
2025-11-09 17:38:07,296 - INFO - root - Page:3, Index:6, Scaling Law for Quantization-Aware Training, https://arxiv.org/pdf/2505.14302, 2025-05-20
2025-11-09 17:38:07,296 - INFO - root - Page:3, Index:7, Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs, https://arxiv.org/pdf/2505.13060, 2025-05-19
2025-11-09 17:38:07,296 - INFO - root - Page:3, Index:8, Deep Unfolding with Kernel-based Quantization in MIMO Detection, https://arxiv.org/pdf/2505.12736, 2025-05-19
2025-11-09 17:38:07,297 - INFO - root - Page:3, Index:9, FedHQ: Hybrid Runtime Quantization for Federated Learning, https://arxiv.org/pdf/2505.11982, 2025-05-17
2025-11-09 17:38:07,297 - INFO - root - Page:3, Index:10, QVGen: Pushing the Limit of Quantized Video Generative Models, https://arxiv.org/pdf/2505.11497, 2025-09-27
2025-11-09 17:38:07,297 - INFO - root - Page:3, Index:11, EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes, https://arxiv.org/pdf/2505.10787, 2025-05-15
2025-11-09 17:38:07,297 - INFO - root - Page:3, Index:12, ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition, https://arxiv.org/pdf/2505.08981, 2025-05-13
2025-11-09 17:38:07,298 - INFO - root - Page:3, Index:13, PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs, https://arxiv.org/pdf/2505.03254, 2025-08-06
2025-11-09 17:38:07,298 - INFO - root - Page:3, Index:14, ICQuant: Index Coding enables Low-bit LLM Quantization, https://arxiv.org/pdf/2505.00850, 2025-08-24
2025-11-09 17:38:07,298 - INFO - root - Page:3, Index:15, Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining, https://arxiv.org/pdf/2504.13932, 2025-07-30
2025-11-09 17:38:07,298 - INFO - root - Page:3, Index:16, Achieving binary weight and activation for LLMs using Post-Training Quantization, https://arxiv.org/pdf/2504.05352, 2025-06-30
2025-11-09 17:38:07,299 - INFO - root - Page:3, Index:17, Wireless Hearables With Programmable Speech AI Accelerators, https://arxiv.org/pdf/2503.18698, 2025-10-21
2025-11-09 17:38:07,299 - INFO - root - Page:3, Index:18, GranQ: Efficient Channel-wise Quantization via Vectorized Pre-Scaling for Zero-Shot QAT, https://arxiv.org/pdf/2503.18339, 2025-10-15
2025-11-09 17:38:07,300 - INFO - root - Page:3, Index:19, CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting, https://arxiv.org/pdf/2503.12836, 2025-09-29
2025-11-09 17:38:07,300 - INFO - root - Page:3, Index:20, AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model, https://arxiv.org/pdf/2503.03088, 2025-07-11
2025-11-09 17:38:07,302 - INFO - root - Page:3, Index:21, Teaching Metric Distance to Discrete Autoregressive Language Models, https://arxiv.org/pdf/2503.02379, 2025-10-07
2025-11-09 17:38:07,303 - INFO - root - Page:3, Index:22, Regularization-based Framework for Quantization-, Fault- and Variability-Aware Training, https://arxiv.org/pdf/2503.01297, 2025-07-12
2025-11-09 17:38:07,303 - INFO - root - Fetching page 5 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=200
2025-11-09 17:38:12,599 - INFO - root - get_all_titles_from_web 
2025-11-09 17:38:12,600 - INFO - root - Page:4, Index:0, Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models, https://arxiv.org/pdf/2502.15799, 2025-06-29
2025-11-09 17:38:12,600 - INFO - root - Page:4, Index:1, Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer, https://arxiv.org/pdf/2502.15779, 2025-09-01
2025-11-09 17:38:12,601 - INFO - root - Page:4, Index:2, RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models, https://arxiv.org/pdf/2502.09003, 2025-06-05
2025-11-09 17:38:12,601 - INFO - root - Page:4, Index:3, Spatial Degradation-Aware and Temporal Consistent Diffusion Model for Compressed Video Super-Resolution, https://arxiv.org/pdf/2502.07381, 2025-06-27
2025-11-09 17:38:12,601 - INFO - root - Page:4, Index:4, QuEST: Stable Training of LLMs with 1-Bit Weights and Activations, https://arxiv.org/pdf/2502.05003, 2025-06-10
2025-11-09 17:38:12,601 - INFO - root - Page:4, Index:5, HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs, https://arxiv.org/pdf/2501.02625, 2025-11-05
2025-11-09 17:38:12,602 - INFO - root - Page:4, Index:6, TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction, https://arxiv.org/pdf/2412.16919, 2025-08-08
2025-11-09 17:38:12,602 - INFO - root - Page:4, Index:7, Training Multi-Layer Binary Neural Networks With Local Binary Error Signals, https://arxiv.org/pdf/2412.00119, 2025-08-05
2025-11-09 17:38:12,602 - INFO - root - Page:4, Index:8, Scaling Large-scale GNN Training to Thousands of Processors on CPU-based Supercomputers, https://arxiv.org/pdf/2411.16025, 2025-05-26
2025-11-09 17:38:12,602 - INFO - root - Page:4, Index:9, HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization, https://arxiv.org/pdf/2411.06581, 2025-05-16
2025-11-09 17:38:12,604 - INFO - root - Fetching page 6 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=250
2025-11-09 17:38:25,080 - INFO - root - get_all_titles_from_web 
2025-11-09 17:38:25,082 - INFO - root - Page:5, Index:0, Gradient-Free Training of Quantized Neural Networks, https://arxiv.org/pdf/2410.09734, 2025-09-29
2025-11-09 17:38:25,082 - INFO - root - Page:5, Index:1, QT-DoG: Quantization-aware Training for Domain Generalization, https://arxiv.org/pdf/2410.06020, 2025-06-26
2025-11-09 17:38:25,083 - INFO - root - Page:5, Index:2, Constraint Guided Model Quantization of Neural Networks, https://arxiv.org/pdf/2409.20138, 2025-09-12
2025-11-09 17:38:25,083 - INFO - root - Page:5, Index:3, Accumulator-Aware Post-Training Quantization for Large Language Models, https://arxiv.org/pdf/2409.17092, 2025-07-30
2025-11-09 17:38:25,084 - INFO - root - Page:5, Index:4, DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation, https://arxiv.org/pdf/2409.14307, 2025-07-09
2025-11-09 17:38:25,086 - INFO - root - Page:5, Index:5, Robust Training of Neural Networks at Arbitrary Precision and Sparsity, https://arxiv.org/pdf/2409.09245, 2025-09-23
2025-11-09 17:38:25,116 - INFO - root - Page:5, Index:6, VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing, https://arxiv.org/pdf/2408.05758, 2025-05-27
2025-11-09 17:38:25,140 - INFO - root - Page:5, Index:7, DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers, https://arxiv.org/pdf/2408.03291, 2025-06-19
2025-11-09 17:38:25,174 - INFO - root - Page:5, Index:8, Temporal Feature Matters: A Framework for Diffusion Model Quantization, https://arxiv.org/pdf/2407.19547, 2025-07-11
2025-11-09 17:38:25,208 - INFO - root - Page:5, Index:9, EfficientQAT: Efficient Quantization-Aware Training for Large Language Models, https://arxiv.org/pdf/2407.11062, 2025-05-19
2025-11-09 17:38:25,260 - INFO - root - Page:5, Index:10, Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT, https://arxiv.org/pdf/2407.11041, 2025-10-31
2025-11-09 17:38:25,361 - INFO - root - Fetching page 7 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=300
2025-11-09 17:39:04,384 - INFO - root - get_all_titles_from_web 
2025-11-09 17:39:04,385 - INFO - root - Page:6, Index:0, BoA: Attention-aware Post-training Quantization without Backpropagation, https://arxiv.org/pdf/2406.13474, 2025-06-06
2025-11-09 17:39:04,386 - INFO - root - Page:6, Index:1, A Rescaling-Invariant Lipschitz Bound Based on Path-Metrics for Modern ReLU Network Parameterizations, https://arxiv.org/pdf/2405.15006, 2025-06-13
2025-11-09 17:39:04,386 - INFO - root - Page:6, Index:2, Scheduling Weight Transitions for Quantization-Aware Training, https://arxiv.org/pdf/2404.19248, 2025-10-01
2025-11-09 17:39:04,387 - INFO - root - Page:6, Index:3, GPTVQ: The Blessing of Dimensionality for LLM Quantization, https://arxiv.org/pdf/2402.15319, 2025-06-03
2025-11-09 17:39:04,392 - INFO - root - Fetching page 8 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=350
2025-11-09 17:39:32,163 - INFO - root - get_all_titles_from_web 
2025-11-09 17:39:32,163 - INFO - root - Page:7, Index:0, Squat: Quant Small Language Models on the Edge, https://arxiv.org/pdf/2402.10787, 2025-07-01
2025-11-09 17:39:32,163 - INFO - root - Page:7, Index:1, L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2402.04902, 2025-07-21
2025-11-09 17:39:32,164 - INFO - root - Fetching page 9 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=400
2025-11-09 17:39:50,822 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 19:30:28,901 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 19:30:28,903 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 19:30:28,907 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 19:31:20,197 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 19:31:20,198 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 19:31:20,204 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 19:31:21,363 - INFO - root - LLMClientManager: 指定使用客户端: Doubao
2025-11-09 19:31:21,366 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 19:31:25,824 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 19:31:25,870 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 19:31:25,871 - INFO - root - LLMClientManager: Doubao 客户端初始化成功
2025-11-09 19:31:25,871 - INFO - root - LLMClientManager: switched to Doubao client
2025-11-09 19:31:25,871 - INFO - root - 已手动切换到 LLM 客户端: Doubao
2025-11-09 19:31:25,873 - INFO - root - 使用 LLM 模型: doubao-seed-1-6-lite-251015
2025-11-09 19:31:25,876 - INFO - root - 可用客户端: ['Doubao']
2025-11-09 19:31:25,877 - INFO - root - === 运行配置 ===
2025-11-09 19:31:25,877 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 19:31:25,877 - INFO - root - 关键词: hp
2025-11-09 19:31:25,877 - INFO - root - 查询: high pressure selenium
2025-11-09 19:31:25,877 - INFO - root - 排序: None
2025-11-09 19:31:25,879 - INFO - root - 最近天数: 180
2025-11-09 19:31:25,879 - INFO - root - 最大处理数量: 2
2025-11-09 19:31:25,881 - INFO - root - 保存图片: 是
2025-11-09 19:31:25,882 - INFO - root - 输出语言: 中文
2025-11-09 19:31:25,882 - INFO - root - 强制重新处理: 否
2025-11-09 19:31:25,882 - INFO - root - ====================
2025-11-09 19:31:25,883 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 19:31:25,883 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=high+pressure+selenium&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 19:31:31,884 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 19:31:31,884 - INFO - root - 没有找到要处理的论文，程序退出
2025-11-09 19:31:31,885 - INFO - root - summary time: 11.69 seconds
2025-11-09 19:42:52,555 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 19:42:52,558 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 19:42:52,559 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 19:42:53,724 - INFO - root - LLMClientManager: 指定使用客户端: Deepseek
2025-11-09 19:42:53,725 - INFO - root - DeepSeekClient: API key found: your_deeps...
2025-11-09 19:42:53,726 - INFO - root - DeepSeekClient: 使用模式: 直接API
2025-11-09 19:42:53,727 - WARNING - root - DeepSeekClient: API key not provided or using placeholder. LLM disabled.
2025-11-09 19:42:53,727 - ERROR - root - LLMClientManager: 指定的客户端 DeepSeek 初始化失败
2025-11-09 19:42:53,728 - WARNING - root - LLMClientManager: 指定的客户端 Deepseek 不可用，将尝试其他客户端
2025-11-09 19:42:54,520 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 19:44:04,414 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 19:44:04,416 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 19:44:04,419 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 19:44:07,571 - INFO - root - LLMClientManager: 指定使用客户端: Deepseek
2025-11-09 19:44:07,572 - INFO - root - DeepSeekClient: API key found: your_deeps...
2025-11-09 19:44:07,573 - INFO - root - DeepSeekClient: 使用模式: 直接API
2025-11-09 19:44:07,573 - WARNING - root - DeepSeekClient: API key not provided or using placeholder. LLM disabled.
2025-11-09 19:44:07,574 - ERROR - root - LLMClientManager: 指定的客户端 DeepSeek 初始化失败
2025-11-09 19:44:07,574 - WARNING - root - LLMClientManager: 指定的客户端 Deepseek 不可用，将尝试其他客户端
2025-11-09 19:44:08,406 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 19:44:11,267 - INFO - root - GeminiClient: initialized model gemini-2.5-flash
2025-11-09 19:44:11,269 - INFO - root - LLMClientManager: Gemini client initialized successfully
2025-11-09 19:44:11,269 - INFO - root - LLMClientManager: using Gemini as default client
2025-11-09 19:44:11,270 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 19:44:11,271 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 19:44:11,272 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 19:44:11,272 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 19:44:11,282 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 19:44:19,859 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 19:44:19,910 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 19:44:19,910 - INFO - root - LLMClientManager: Doubao client initialized successfully
2025-11-09 19:44:19,910 - WARNING - root - LLMClientManager: client Deepseek not available
2025-11-09 19:44:19,911 - WARNING - root - 无法切换到指定的客户端 Deepseek，将使用默认客户端
2025-11-09 19:44:19,911 - INFO - root - 可用客户端: ['Gemini', 'Doubao']
2025-11-09 19:44:19,912 - INFO - root - 使用 LLM 模型: gemini-2.5-flash
2025-11-09 19:44:19,912 - INFO - root - 可用客户端: ['Gemini', 'Doubao']
2025-11-09 19:44:19,913 - INFO - root - === 运行配置 ===
2025-11-09 19:44:19,914 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 19:44:19,914 - INFO - root - 关键词: QAT
2025-11-09 19:44:19,914 - INFO - root - 查询: Quantization-Aware-Training
2025-11-09 19:44:19,915 - INFO - root - 排序: None
2025-11-09 19:44:19,917 - INFO - root - 最近天数: 180
2025-11-09 19:44:19,919 - INFO - root - 最大处理数量: 2
2025-11-09 19:44:19,922 - INFO - root - 保存图片: 是
2025-11-09 19:44:19,924 - INFO - root - 输出语言: 中文
2025-11-09 19:44:19,925 - INFO - root - 强制重新处理: 否
2025-11-09 19:44:19,925 - INFO - root - ====================
2025-11-09 19:44:19,927 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 19:44:19,928 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 19:44:26,277 - INFO - root - get_all_titles_from_web 
2025-11-09 19:44:26,277 - INFO - root - Page:0, Index:0, A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies, https://arxiv.org/pdf/2511.03201, 2025-11-05
2025-11-09 19:44:26,279 - INFO - root - Page:0, Index:1, FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error, https://arxiv.org/pdf/2511.02302, 2025-11-04
2025-11-09 19:44:26,280 - INFO - root - Page:0, Index:2, Outlier-Aware Post-Training Quantization for Image Super-Resolution, https://arxiv.org/pdf/2511.00682, 2025-11-01
2025-11-09 19:44:26,281 - INFO - root - Page:0, Index:3, Evaluation of Wafer-Scale SOT-MRAM for Analog Crossbar Array Applications, https://arxiv.org/pdf/2510.25853, 2025-11-03
2025-11-09 19:44:26,281 - INFO - root - Page:0, Index:4, Improving the Straight-Through Estimator with Zeroth-Order Information, https://arxiv.org/pdf/2510.23926, 2025-10-27
2025-11-09 19:44:26,281 - INFO - root - Page:0, Index:5, Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework, https://arxiv.org/pdf/2510.22243, 2025-10-25
2025-11-09 19:44:26,282 - INFO - root - Page:0, Index:6, TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge, https://arxiv.org/pdf/2510.21879, 2025-10-23
2025-11-09 19:44:26,282 - INFO - root - Page:0, Index:7, KARIPAP: Quantum-Inspired Tensor Network Compression of Large Language Models Using Infinite Projected Entangled Pair States and Tensor Renormalization Group, https://arxiv.org/pdf/2510.21844, 2025-10-22
2025-11-09 19:44:26,283 - INFO - root - Page:0, Index:8, A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization, https://arxiv.org/pdf/2510.21314, 2025-10-24
2025-11-09 19:44:26,283 - INFO - root - Page:0, Index:9, Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks, https://arxiv.org/pdf/2510.19760, 2025-10-22
2025-11-09 19:44:26,283 - INFO - root - Page:0, Index:10, CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training, https://arxiv.org/pdf/2510.18784, 2025-10-21
2025-11-09 19:44:26,284 - INFO - root - Page:0, Index:11, Mixed-Precision Quantization for Language Models: Techniques and Prospects, https://arxiv.org/pdf/2510.16805, 2025-10-19
2025-11-09 19:44:26,284 - INFO - root - Page:0, Index:12, SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation, https://arxiv.org/pdf/2510.16396, 2025-10-30
2025-11-09 19:44:26,285 - INFO - root - Page:0, Index:13, CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models, https://arxiv.org/pdf/2510.15962, 2025-10-11
2025-11-09 19:44:26,285 - INFO - root - Page:0, Index:14, SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization, https://arxiv.org/pdf/2510.15775, 2025-10-17
2025-11-09 19:44:26,286 - INFO - root - Page:0, Index:15, SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware, https://arxiv.org/pdf/2510.15542, 2025-10-31
2025-11-09 19:44:26,286 - INFO - root - Page:0, Index:16, GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework, https://arxiv.org/pdf/2510.15299, 2025-10-17
2025-11-09 19:44:26,287 - INFO - root - Page:0, Index:17, SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images, https://arxiv.org/pdf/2510.15072, 2025-10-16
2025-11-09 19:44:26,288 - INFO - root - Page:0, Index:18, FraQAT: Quantization Aware Training with Fractional bits, https://arxiv.org/pdf/2510.14823, 2025-10-16
2025-11-09 19:44:26,290 - INFO - root - Page:0, Index:19, Computing-In-Memory Aware Model Adaption For Edge Devices, https://arxiv.org/pdf/2510.14379, 2025-10-16
2025-11-09 19:44:26,292 - INFO - root - Page:0, Index:20, Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge, https://arxiv.org/pdf/2510.13760, 2025-11-04
2025-11-09 19:44:26,292 - INFO - root - Page:0, Index:21, NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models, https://arxiv.org/pdf/2510.13068, 2025-10-14
2025-11-09 19:44:26,293 - INFO - root - Page:0, Index:22, Detect Anything via Next Point Prediction, https://arxiv.org/pdf/2510.12798, 2025-10-14
2025-11-09 19:44:26,294 - INFO - root - Page:0, Index:23, AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model, https://arxiv.org/pdf/2510.11496, 2025-10-14
2025-11-09 19:44:26,294 - INFO - root - Page:0, Index:24, Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware, https://arxiv.org/pdf/2510.11484, 2025-10-13
2025-11-09 19:44:26,295 - INFO - root - Page:0, Index:25, SASER: Stego attacks on open-source LLMs, https://arxiv.org/pdf/2510.10486, 2025-10-12
2025-11-09 19:44:26,295 - INFO - root - Page:0, Index:26, Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition, https://arxiv.org/pdf/2510.09653, 2025-10-15
2025-11-09 19:44:26,296 - INFO - root - Page:0, Index:27, A Theoretically-Grounded Codebook for Digital Semantic Communications, https://arxiv.org/pdf/2510.07108, 2025-10-14
2025-11-09 19:44:26,297 - INFO - root - Page:0, Index:28, QuantDemoire: Quantization with Outlier Aware for Image Demoiréing, https://arxiv.org/pdf/2510.04066, 2025-10-05
2025-11-09 19:44:26,300 - INFO - root - Page:0, Index:29, Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization, https://arxiv.org/pdf/2510.03763, 2025-10-04
2025-11-09 19:44:26,301 - INFO - root - Page:0, Index:30, Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models, https://arxiv.org/pdf/2510.03274, 2025-09-27
2025-11-09 19:44:26,302 - INFO - root - Page:0, Index:31, PT$^2$-LLM: Post-Training Ternarization for Large Language Models, https://arxiv.org/pdf/2510.03267, 2025-09-26
2025-11-09 19:44:26,302 - INFO - root - Page:0, Index:32, Purrception: Variational Flow Matching for Vector-Quantized Image Generation, https://arxiv.org/pdf/2510.01478, 2025-10-01
2025-11-09 19:44:26,302 - INFO - root - Page:0, Index:33, Post-Training Quantization for Audio Diffusion Transformers, https://arxiv.org/pdf/2510.00313, 2025-09-30
2025-11-09 19:44:26,305 - INFO - root - Page:0, Index:34, Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling, https://arxiv.org/pdf/2510.00028, 2025-09-25
2025-11-09 19:44:26,306 - INFO - root - Page:0, Index:35, Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models, https://arxiv.org/pdf/2509.26436, 2025-09-30
2025-11-09 19:44:26,307 - INFO - root - Page:0, Index:36, Cat: Post-Training Quantization Error Reduction via Cluster-based Affine Transformation, https://arxiv.org/pdf/2509.26277, 2025-10-07
2025-11-09 19:44:26,308 - INFO - root - Page:0, Index:37, CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models, https://arxiv.org/pdf/2509.25996, 2025-09-30
2025-11-09 19:44:26,309 - INFO - root - Page:0, Index:38, Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications, https://arxiv.org/pdf/2509.25439, 2025-09-29
2025-11-09 19:44:26,310 - INFO - root - Page:0, Index:39, On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs, https://arxiv.org/pdf/2509.25214, 2025-09-22
2025-11-09 19:44:26,310 - INFO - root - Page:0, Index:40, VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning, https://arxiv.org/pdf/2509.24650, 2025-09-29
2025-11-09 19:44:26,311 - INFO - root - Page:0, Index:41, S$^2$NN: Sub-bit Spiking Neural Networks, https://arxiv.org/pdf/2509.24266, 2025-10-24
2025-11-09 19:44:26,311 - INFO - root - Page:0, Index:42, Tequila: Trapping-free Ternary Quantization for Large Language Models, https://arxiv.org/pdf/2509.23809, 2025-10-17
2025-11-09 19:44:26,312 - INFO - root - Page:0, Index:43, Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution, https://arxiv.org/pdf/2509.23774, 2025-09-30
2025-11-09 19:44:26,312 - INFO - root - Page:0, Index:44, RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization, https://arxiv.org/pdf/2509.23582, 2025-09-27
2025-11-09 19:44:26,313 - INFO - root - Page:0, Index:45, Beyond Outliers: A Study of Optimizers Under Quantization, https://arxiv.org/pdf/2509.23500, 2025-10-02
2025-11-09 19:44:26,313 - INFO - root - Page:0, Index:46, Compute-Optimal Quantization-Aware Training, https://arxiv.org/pdf/2509.22935, 2025-09-26
2025-11-09 19:44:26,314 - INFO - root - Page:0, Index:47, COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning, https://arxiv.org/pdf/2509.22075, 2025-10-06
2025-11-09 19:44:26,314 - INFO - root - Page:0, Index:48, SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models, https://arxiv.org/pdf/2509.21498, 2025-09-25
2025-11-09 19:44:26,314 - INFO - root - Page:0, Index:49, Quantized Visual Geometry Grounded Transformer, https://arxiv.org/pdf/2509.21302, 2025-09-29
2025-11-09 19:44:26,316 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 19:44:36,067 - INFO - root - get_all_titles_from_web 
2025-11-09 19:44:36,068 - INFO - root - Page:1, Index:0, Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy, https://arxiv.org/pdf/2509.21173, 2025-10-27
2025-11-09 19:44:36,069 - INFO - root - Page:1, Index:1, Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer, https://arxiv.org/pdf/2509.20854, 2025-09-25
2025-11-09 19:44:36,069 - INFO - root - Page:1, Index:2, TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection, https://arxiv.org/pdf/2509.18193, 2025-09-19
2025-11-09 19:44:36,070 - INFO - root - Page:1, Index:3, SBVR: Summation of BitVector Representation for Efficient LLM Quantization, https://arxiv.org/pdf/2509.18172, 2025-09-17
2025-11-09 19:44:36,070 - INFO - root - Page:1, Index:4, QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2509.17428, 2025-09-26
2025-11-09 19:44:36,071 - INFO - root - Page:1, Index:5, PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models, https://arxiv.org/pdf/2509.16989, 2025-10-28
2025-11-09 19:44:36,071 - INFO - root - Page:1, Index:6, MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training, https://arxiv.org/pdf/2509.15514, 2025-09-18
2025-11-09 19:44:36,071 - INFO - root - Page:1, Index:7, Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs, https://arxiv.org/pdf/2509.14391, 2025-09-17
2025-11-09 19:44:36,076 - INFO - root - Page:1, Index:8, Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization, https://arxiv.org/pdf/2509.13786, 2025-09-19
2025-11-09 19:44:36,077 - INFO - root - Page:1, Index:9, Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees, https://arxiv.org/pdf/2509.11054, 2025-09-13
2025-11-09 19:44:36,078 - INFO - root - Page:1, Index:10, SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models, https://arxiv.org/pdf/2509.09090, 2025-09-10
2025-11-09 19:44:36,078 - INFO - root - Page:1, Index:11, CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks with Entropy Regularization, https://arxiv.org/pdf/2509.08776, 2025-09-10
2025-11-09 19:44:36,079 - INFO - root - Page:1, Index:12, Explaining How Quantization Disparately Skews a Model, https://arxiv.org/pdf/2509.07222, 2025-09-08
2025-11-09 19:44:36,079 - INFO - root - Page:1, Index:13, LoaQ: Layer-wise Output Approximation Quantization, https://arxiv.org/pdf/2509.06297, 2025-09-07
2025-11-09 19:44:36,080 - INFO - root - Page:1, Index:14, FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving, https://arxiv.org/pdf/2509.06261, 2025-09-14
2025-11-09 19:44:36,081 - INFO - root - Page:1, Index:15, Sensitivity-Aware Post-Training Quantization for Deep Neural Networks, https://arxiv.org/pdf/2509.05576, 2025-09-05
2025-11-09 19:44:36,082 - INFO - root - Page:1, Index:16, SuperSNN: A Hardware-Aware Framework for Physically Realizable, High-Performance Superconducting Spiking Neural Network Chips, https://arxiv.org/pdf/2509.05532, 2025-09-05
2025-11-09 19:44:36,084 - INFO - root - Page:1, Index:17, Data-Augmented Quantization-Aware Knowledge Distillation, https://arxiv.org/pdf/2509.03850, 2025-09-03
2025-11-09 19:44:36,085 - INFO - root - Page:1, Index:18, DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling, https://arxiv.org/pdf/2509.03472, 2025-09-03
2025-11-09 19:44:36,086 - INFO - root - Page:1, Index:19, Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs, https://arxiv.org/pdf/2509.02017, 2025-09-02
2025-11-09 19:44:36,086 - INFO - root - Page:1, Index:20, Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling, https://arxiv.org/pdf/2509.01624, 2025-09-01
2025-11-09 19:44:36,087 - INFO - root - Page:1, Index:21, Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective, https://arxiv.org/pdf/2509.00859, 2025-08-31
2025-11-09 19:44:36,087 - INFO - root - Page:1, Index:22, Progressive Element-wise Gradient Estimation for Neural Network Quantization, https://arxiv.org/pdf/2509.00097, 2025-08-27
2025-11-09 19:44:36,088 - INFO - root - Page:1, Index:23, End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost, https://arxiv.org/pdf/2509.00031, 2025-09-29
2025-11-09 19:44:36,089 - INFO - root - Page:1, Index:24, Quantization Robustness to Input Degradations for Object Detection, https://arxiv.org/pdf/2508.19600, 2025-08-27
2025-11-09 19:44:36,090 - INFO - root - Page:1, Index:25, Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models, https://arxiv.org/pdf/2508.18609, 2025-08-27
2025-11-09 19:44:36,091 - INFO - root - Page:1, Index:26, AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration, https://arxiv.org/pdf/2508.18025, 2025-08-25
2025-11-09 19:44:36,091 - INFO - root - Page:1, Index:27, TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling, https://arxiv.org/pdf/2508.16790, 2025-08-22
2025-11-09 19:44:36,092 - INFO - root - Page:1, Index:28, A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering, https://arxiv.org/pdf/2508.16516, 2025-08-22
2025-11-09 19:44:36,093 - INFO - root - Page:1, Index:29, JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs, https://arxiv.org/pdf/2508.15468, 2025-08-21
2025-11-09 19:44:36,094 - INFO - root - Page:1, Index:30, MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation, https://arxiv.org/pdf/2508.15281, 2025-08-21
2025-11-09 19:44:36,094 - INFO - root - Page:1, Index:31, DLLMQuant: Quantizing Diffusion-based Large Language Models, https://arxiv.org/pdf/2508.14090, 2025-08-25
2025-11-09 19:44:36,096 - INFO - root - Page:1, Index:32, Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management, https://arxiv.org/pdf/2508.13905, 2025-08-19
2025-11-09 19:44:36,106 - INFO - root - Page:1, Index:33, XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads, https://arxiv.org/pdf/2508.13049, 2025-08-18
2025-11-09 19:44:36,107 - INFO - root - Page:1, Index:34, Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection, https://arxiv.org/pdf/2508.12230, 2025-08-17
2025-11-09 19:44:36,107 - INFO - root - Page:1, Index:35, TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks, https://arxiv.org/pdf/2508.12132, 2025-08-16
2025-11-09 19:44:36,107 - INFO - root - Page:1, Index:36, Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion, https://arxiv.org/pdf/2508.12094, 2025-10-13
2025-11-09 19:44:36,108 - INFO - root - Page:1, Index:37, Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware, https://arxiv.org/pdf/2508.11940, 2025-08-16
2025-11-09 19:44:36,108 - INFO - root - Page:1, Index:38, PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks, https://arxiv.org/pdf/2508.10557, 2025-08-15
2025-11-09 19:44:36,108 - INFO - root - Page:1, Index:39, MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI, https://arxiv.org/pdf/2508.09500, 2025-08-13
2025-11-09 19:44:36,109 - INFO - root - Page:1, Index:40, SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via Codebooks for recommender system, https://arxiv.org/pdf/2508.09090, 2025-08-12
2025-11-09 19:44:36,109 - INFO - root - Page:1, Index:41, Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models, https://arxiv.org/pdf/2508.06974, 2025-08-09
2025-11-09 19:44:36,110 - INFO - root - Page:1, Index:42, Efficient Deep Neural Receiver with Post-Training Quantization, https://arxiv.org/pdf/2508.06275, 2025-08-08
2025-11-09 19:44:36,110 - INFO - root - Page:1, Index:43, iFairy: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$, https://arxiv.org/pdf/2508.05571, 2025-08-16
2025-11-09 19:44:36,111 - INFO - root - Page:1, Index:44, S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation, https://arxiv.org/pdf/2508.04016, 2025-10-27
2025-11-09 19:44:36,111 - INFO - root - Page:1, Index:45, LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation, https://arxiv.org/pdf/2508.03485, 2025-09-23
2025-11-09 19:44:36,112 - INFO - root - Page:1, Index:46, VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation, https://arxiv.org/pdf/2508.03351, 2025-08-05
2025-11-09 19:44:36,113 - INFO - root - Page:1, Index:47, TF-MLPNet: Tiny Real-Time Neural Speech Separation, https://arxiv.org/pdf/2508.03047, 2025-08-04
2025-11-09 19:44:36,113 - INFO - root - Page:1, Index:48, SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation, https://arxiv.org/pdf/2508.01375, 2025-08-02
2025-11-09 19:44:36,114 - INFO - root - Page:1, Index:49, MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective, https://arxiv.org/pdf/2507.19131, 2025-07-25
2025-11-09 19:44:36,114 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 19:44:42,546 - INFO - root - get_all_titles_from_web 
2025-11-09 19:44:42,546 - INFO - root - Page:2, Index:0, Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction, https://arxiv.org/pdf/2507.17768, 2025-07-16
2025-11-09 19:44:42,547 - INFO - root - Page:2, Index:1, SiLQ: Simple Large Language Model Quantization-Aware Training, https://arxiv.org/pdf/2507.16933, 2025-07-22
2025-11-09 19:44:42,547 - INFO - root - Page:2, Index:2, Task-Specific Zero-shot Quantization-Aware Training for Object Detection, https://arxiv.org/pdf/2507.16782, 2025-07-22
2025-11-09 19:44:42,548 - INFO - root - Page:2, Index:3, TorchAO: PyTorch-Native Training-to-Serving Model Optimization, https://arxiv.org/pdf/2507.16099, 2025-07-21
2025-11-09 19:44:42,548 - INFO - root - Page:2, Index:4, SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models, https://arxiv.org/pdf/2507.14811, 2025-08-27
2025-11-09 19:44:42,549 - INFO - root - Page:2, Index:5, Apple Intelligence Foundation Language Models: Tech Report 2025, https://arxiv.org/pdf/2507.13575, 2025-08-27
2025-11-09 19:44:42,549 - INFO - root - Page:2, Index:6, Generative Multi-Target Cross-Domain Recommendation, https://arxiv.org/pdf/2507.12871, 2025-08-07
2025-11-09 19:44:42,550 - INFO - root - Page:2, Index:7, DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training, https://arxiv.org/pdf/2507.07149, 2025-07-09
2025-11-09 19:44:42,550 - INFO - root - Page:2, Index:8, Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics, https://arxiv.org/pdf/2507.07044, 2025-07-15
2025-11-09 19:44:42,550 - INFO - root - Page:2, Index:9, QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models, https://arxiv.org/pdf/2507.06079, 2025-07-08
2025-11-09 19:44:42,551 - INFO - root - Page:2, Index:10, GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field, https://arxiv.org/pdf/2507.05594, 2025-07-07
2025-11-09 19:44:42,551 - INFO - root - Page:2, Index:11, Hita: Holistic Tokenizer for Autoregressive Image Generation, https://arxiv.org/pdf/2507.02358, 2025-07-11
2025-11-09 19:44:42,552 - INFO - root - Page:2, Index:12, QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference, https://arxiv.org/pdf/2506.23934, 2025-06-30
2025-11-09 19:44:42,552 - INFO - root - Page:2, Index:13, FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization, https://arxiv.org/pdf/2506.23516, 2025-07-21
2025-11-09 19:44:42,553 - INFO - root - Page:2, Index:14, Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models, https://arxiv.org/pdf/2506.23025, 2025-06-28
2025-11-09 19:44:42,554 - INFO - root - Page:2, Index:15, Joint Quantization and Pruning Neural Networks Approach: A Case Study on FSO Receivers, https://arxiv.org/pdf/2506.20084, 2025-06-24
2025-11-09 19:44:42,555 - INFO - root - Page:2, Index:16, Single-step Diffusion for Image Compression at Ultra-Low Bitrates, https://arxiv.org/pdf/2506.16572, 2025-09-22
2025-11-09 19:44:42,561 - INFO - root - Page:2, Index:17, LittleBit: Ultra Low-Bit Quantization via Latent Factorization, https://arxiv.org/pdf/2506.13771, 2025-10-28
2025-11-09 19:44:42,562 - INFO - root - Page:2, Index:18, MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering, https://arxiv.org/pdf/2506.13755, 2025-06-16
2025-11-09 19:44:42,562 - INFO - root - Page:2, Index:19, EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization, https://arxiv.org/pdf/2506.13329, 2025-07-04
2025-11-09 19:44:42,562 - INFO - root - Page:2, Index:20, Towards Neural Audio Codec Source Parsing, https://arxiv.org/pdf/2506.12627, 2025-06-14
2025-11-09 19:44:42,563 - INFO - root - Page:2, Index:21, Quantizing Small-Scale State-Space Models for Edge AI, https://arxiv.org/pdf/2506.12480, 2025-06-14
2025-11-09 19:44:42,563 - INFO - root - Page:2, Index:22, EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction, https://arxiv.org/pdf/2506.12015, 2025-06-13
2025-11-09 19:44:42,564 - INFO - root - Page:2, Index:23, Compression Aware Certified Training, https://arxiv.org/pdf/2506.11992, 2025-06-13
2025-11-09 19:44:42,565 - INFO - root - Page:2, Index:24, GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers, https://arxiv.org/pdf/2506.11784, 2025-06-13
2025-11-09 19:44:42,566 - INFO - root - Page:2, Index:25, TruncQuant: Truncation-Ready Quantization for DNNs with Flexible Weight Bit Precision, https://arxiv.org/pdf/2506.11431, 2025-06-12
2025-11-09 19:44:42,568 - INFO - root - Page:2, Index:26, EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices, https://arxiv.org/pdf/2506.11093, 2025-06-05
2025-11-09 19:44:42,569 - INFO - root - Page:2, Index:27, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 19:44:42,570 - INFO - root - Page:2, Index:28, AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent, https://arxiv.org/pdf/2506.10205, 2025-06-11
2025-11-09 19:44:42,571 - INFO - root - Page:2, Index:29, Q-SAM2: Accurate Quantization for Segment Anything Model 2, https://arxiv.org/pdf/2506.09782, 2025-06-11
2025-11-09 19:44:42,571 - INFO - root - Page:2, Index:30, Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs, https://arxiv.org/pdf/2506.09104, 2025-06-10
2025-11-09 19:44:42,571 - INFO - root - Page:2, Index:31, Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU, https://arxiv.org/pdf/2506.08911, 2025-06-10
2025-11-09 19:44:42,572 - INFO - root - Page:2, Index:32, POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration, https://arxiv.org/pdf/2506.08785, 2025-06-10
2025-11-09 19:44:42,574 - INFO - root - Page:2, Index:33, MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts, https://arxiv.org/pdf/2506.07533, 2025-06-09
2025-11-09 19:44:42,575 - INFO - root - Page:2, Index:34, BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation, https://arxiv.org/pdf/2506.07530, 2025-06-09
2025-11-09 19:44:42,577 - INFO - root - Page:2, Index:35, RecGPT: A Foundation Model for Sequential Recommendation, https://arxiv.org/pdf/2506.06270, 2025-06-12
2025-11-09 19:44:42,577 - INFO - root - Page:2, Index:36, FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion, https://arxiv.org/pdf/2506.04648, 2025-06-05
2025-11-09 19:44:42,577 - INFO - root - Page:2, Index:37, BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing, https://arxiv.org/pdf/2506.03515, 2025-06-03
2025-11-09 19:44:42,579 - INFO - root - Page:2, Index:38, ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding, https://arxiv.org/pdf/2506.01853, 2025-06-02
2025-11-09 19:44:42,579 - INFO - root - Page:2, Index:39, Movable Antenna Enhanced Federated Fine-Tuning of Large Language Models via Hybrid Client Selection Optimization, https://arxiv.org/pdf/2506.00011, 2025-10-26
2025-11-09 19:44:42,580 - INFO - root - Page:2, Index:40, Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach, https://arxiv.org/pdf/2505.24721, 2025-05-30
2025-11-09 19:44:42,580 - INFO - root - Page:2, Index:41, GARLIC: GAussian Representation LearnIng for spaCe partitioning, https://arxiv.org/pdf/2505.24608, 2025-10-02
2025-11-09 19:44:42,582 - INFO - root - Page:2, Index:42, AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up, https://arxiv.org/pdf/2505.24584, 2025-08-18
2025-11-09 19:44:42,582 - INFO - root - Page:2, Index:43, Model-Preserving Adaptive Rounding, https://arxiv.org/pdf/2505.22988, 2025-09-25
2025-11-09 19:44:42,584 - INFO - root - Page:2, Index:44, Highly Efficient and Effective LLMs with Multi-Boolean Architectures, https://arxiv.org/pdf/2505.22811, 2025-10-03
2025-11-09 19:44:42,587 - INFO - root - Page:2, Index:45, Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning, https://arxiv.org/pdf/2505.21591, 2025-05-27
2025-11-09 19:44:42,592 - INFO - root - Page:2, Index:46, Frequency Composition for Compressed and Domain-Adaptive Neural Networks, https://arxiv.org/pdf/2505.20890, 2025-05-27
2025-11-09 19:44:42,596 - INFO - root - Page:2, Index:47, MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition, https://arxiv.org/pdf/2505.20744, 2025-10-27
2025-11-09 19:44:42,597 - INFO - root - Page:2, Index:48, Optimizing edge AI models on HPC systems with the edge in the loop, https://arxiv.org/pdf/2505.19995, 2025-05-26
2025-11-09 19:44:42,597 - INFO - root - Fetching page 4 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=150
2025-11-09 19:44:49,700 - INFO - root - get_all_titles_from_web 
2025-11-09 19:44:49,701 - INFO - root - Page:3, Index:0, DPASyn: Mechanism-Aware Drug Synergy Prediction via Dual Attention and Precision-Aware Quantization, https://arxiv.org/pdf/2505.19144, 2025-09-20
2025-11-09 19:44:49,701 - INFO - root - Page:3, Index:1, Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding, https://arxiv.org/pdf/2505.18758, 2025-05-24
2025-11-09 19:44:49,701 - INFO - root - Page:3, Index:2, Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization, https://arxiv.org/pdf/2505.18113, 2025-05-23
2025-11-09 19:44:49,702 - INFO - root - Page:3, Index:3, Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs, https://arxiv.org/pdf/2505.17662, 2025-09-19
2025-11-09 19:44:49,702 - INFO - root - Page:3, Index:4, FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design, https://arxiv.org/pdf/2505.16335, 2025-05-22
2025-11-09 19:44:49,702 - INFO - root - Page:3, Index:5, Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control, https://arxiv.org/pdf/2505.15304, 2025-05-30
2025-11-09 19:44:49,703 - INFO - root - Page:3, Index:6, Scaling Law for Quantization-Aware Training, https://arxiv.org/pdf/2505.14302, 2025-05-20
2025-11-09 19:44:49,703 - INFO - root - Page:3, Index:7, Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs, https://arxiv.org/pdf/2505.13060, 2025-05-19
2025-11-09 19:44:49,703 - INFO - root - Page:3, Index:8, Deep Unfolding with Kernel-based Quantization in MIMO Detection, https://arxiv.org/pdf/2505.12736, 2025-05-19
2025-11-09 19:44:49,703 - INFO - root - Page:3, Index:9, FedHQ: Hybrid Runtime Quantization for Federated Learning, https://arxiv.org/pdf/2505.11982, 2025-05-17
2025-11-09 19:44:49,704 - INFO - root - Page:3, Index:10, QVGen: Pushing the Limit of Quantized Video Generative Models, https://arxiv.org/pdf/2505.11497, 2025-09-27
2025-11-09 19:44:49,704 - INFO - root - Page:3, Index:11, EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes, https://arxiv.org/pdf/2505.10787, 2025-05-15
2025-11-09 19:44:49,704 - INFO - root - Page:3, Index:12, ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition, https://arxiv.org/pdf/2505.08981, 2025-05-13
2025-11-09 19:44:49,705 - INFO - root - Page:3, Index:13, PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs, https://arxiv.org/pdf/2505.03254, 2025-08-06
2025-11-09 19:44:49,705 - INFO - root - Page:3, Index:14, ICQuant: Index Coding enables Low-bit LLM Quantization, https://arxiv.org/pdf/2505.00850, 2025-08-24
2025-11-09 19:44:49,705 - INFO - root - Page:3, Index:15, Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining, https://arxiv.org/pdf/2504.13932, 2025-07-30
2025-11-09 19:44:49,706 - INFO - root - Page:3, Index:16, Achieving binary weight and activation for LLMs using Post-Training Quantization, https://arxiv.org/pdf/2504.05352, 2025-06-30
2025-11-09 19:44:49,706 - INFO - root - Page:3, Index:17, Wireless Hearables With Programmable Speech AI Accelerators, https://arxiv.org/pdf/2503.18698, 2025-10-21
2025-11-09 19:44:49,707 - INFO - root - Page:3, Index:18, GranQ: Efficient Channel-wise Quantization via Vectorized Pre-Scaling for Zero-Shot QAT, https://arxiv.org/pdf/2503.18339, 2025-10-15
2025-11-09 19:44:49,707 - INFO - root - Page:3, Index:19, CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting, https://arxiv.org/pdf/2503.12836, 2025-09-29
2025-11-09 19:44:49,707 - INFO - root - Page:3, Index:20, AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model, https://arxiv.org/pdf/2503.03088, 2025-07-11
2025-11-09 19:44:49,708 - INFO - root - Page:3, Index:21, Teaching Metric Distance to Discrete Autoregressive Language Models, https://arxiv.org/pdf/2503.02379, 2025-10-07
2025-11-09 19:44:49,708 - INFO - root - Page:3, Index:22, Regularization-based Framework for Quantization-, Fault- and Variability-Aware Training, https://arxiv.org/pdf/2503.01297, 2025-07-12
2025-11-09 19:44:49,709 - INFO - root - Fetching page 5 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=200
2025-11-09 19:44:54,374 - INFO - root - get_all_titles_from_web 
2025-11-09 19:44:54,374 - INFO - root - Page:4, Index:0, Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models, https://arxiv.org/pdf/2502.15799, 2025-06-29
2025-11-09 19:44:54,374 - INFO - root - Page:4, Index:1, Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer, https://arxiv.org/pdf/2502.15779, 2025-09-01
2025-11-09 19:44:54,375 - INFO - root - Page:4, Index:2, RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models, https://arxiv.org/pdf/2502.09003, 2025-06-05
2025-11-09 19:44:54,375 - INFO - root - Page:4, Index:3, Spatial Degradation-Aware and Temporal Consistent Diffusion Model for Compressed Video Super-Resolution, https://arxiv.org/pdf/2502.07381, 2025-06-27
2025-11-09 19:44:54,375 - INFO - root - Page:4, Index:4, QuEST: Stable Training of LLMs with 1-Bit Weights and Activations, https://arxiv.org/pdf/2502.05003, 2025-06-10
2025-11-09 19:44:54,376 - INFO - root - Page:4, Index:5, HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs, https://arxiv.org/pdf/2501.02625, 2025-11-05
2025-11-09 19:44:54,376 - INFO - root - Page:4, Index:6, TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction, https://arxiv.org/pdf/2412.16919, 2025-08-08
2025-11-09 19:44:54,376 - INFO - root - Page:4, Index:7, Training Multi-Layer Binary Neural Networks With Local Binary Error Signals, https://arxiv.org/pdf/2412.00119, 2025-08-05
2025-11-09 19:44:54,376 - INFO - root - Page:4, Index:8, Scaling Large-scale GNN Training to Thousands of Processors on CPU-based Supercomputers, https://arxiv.org/pdf/2411.16025, 2025-05-26
2025-11-09 19:44:54,377 - INFO - root - Page:4, Index:9, HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization, https://arxiv.org/pdf/2411.06581, 2025-05-16
2025-11-09 19:44:54,377 - INFO - root - Fetching page 6 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=250
2025-11-09 19:45:00,926 - INFO - root - get_all_titles_from_web 
2025-11-09 19:45:00,927 - INFO - root - Page:5, Index:0, Gradient-Free Training of Quantized Neural Networks, https://arxiv.org/pdf/2410.09734, 2025-09-29
2025-11-09 19:45:00,927 - INFO - root - Page:5, Index:1, QT-DoG: Quantization-aware Training for Domain Generalization, https://arxiv.org/pdf/2410.06020, 2025-06-26
2025-11-09 19:45:00,928 - INFO - root - Page:5, Index:2, Constraint Guided Model Quantization of Neural Networks, https://arxiv.org/pdf/2409.20138, 2025-09-12
2025-11-09 19:45:00,928 - INFO - root - Page:5, Index:3, Accumulator-Aware Post-Training Quantization for Large Language Models, https://arxiv.org/pdf/2409.17092, 2025-07-30
2025-11-09 19:45:00,928 - INFO - root - Page:5, Index:4, DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation, https://arxiv.org/pdf/2409.14307, 2025-07-09
2025-11-09 19:45:00,928 - INFO - root - Page:5, Index:5, Robust Training of Neural Networks at Arbitrary Precision and Sparsity, https://arxiv.org/pdf/2409.09245, 2025-09-23
2025-11-09 19:45:00,929 - INFO - root - Page:5, Index:6, VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing, https://arxiv.org/pdf/2408.05758, 2025-05-27
2025-11-09 19:45:00,929 - INFO - root - Page:5, Index:7, DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers, https://arxiv.org/pdf/2408.03291, 2025-06-19
2025-11-09 19:45:00,929 - INFO - root - Page:5, Index:8, Temporal Feature Matters: A Framework for Diffusion Model Quantization, https://arxiv.org/pdf/2407.19547, 2025-07-11
2025-11-09 19:45:00,930 - INFO - root - Page:5, Index:9, EfficientQAT: Efficient Quantization-Aware Training for Large Language Models, https://arxiv.org/pdf/2407.11062, 2025-05-19
2025-11-09 19:45:00,930 - INFO - root - Page:5, Index:10, Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT, https://arxiv.org/pdf/2407.11041, 2025-10-31
2025-11-09 19:45:00,930 - INFO - root - Fetching page 7 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=300
2025-11-09 19:45:07,602 - INFO - root - get_all_titles_from_web 
2025-11-09 19:45:07,603 - INFO - root - Page:6, Index:0, BoA: Attention-aware Post-training Quantization without Backpropagation, https://arxiv.org/pdf/2406.13474, 2025-06-06
2025-11-09 19:45:07,603 - INFO - root - Page:6, Index:1, A Rescaling-Invariant Lipschitz Bound Based on Path-Metrics for Modern ReLU Network Parameterizations, https://arxiv.org/pdf/2405.15006, 2025-06-13
2025-11-09 19:45:07,604 - INFO - root - Page:6, Index:2, Scheduling Weight Transitions for Quantization-Aware Training, https://arxiv.org/pdf/2404.19248, 2025-10-01
2025-11-09 19:45:07,604 - INFO - root - Page:6, Index:3, GPTVQ: The Blessing of Dimensionality for LLM Quantization, https://arxiv.org/pdf/2402.15319, 2025-06-03
2025-11-09 19:45:07,604 - INFO - root - Fetching page 8 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=350
2025-11-09 19:45:13,986 - INFO - root - get_all_titles_from_web 
2025-11-09 19:45:13,989 - INFO - root - Page:7, Index:0, Squat: Quant Small Language Models on the Edge, https://arxiv.org/pdf/2402.10787, 2025-07-01
2025-11-09 19:45:13,990 - INFO - root - Page:7, Index:1, L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2402.04902, 2025-07-21
2025-11-09 19:45:13,990 - INFO - root - Fetching page 9 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=400
2025-11-09 19:45:21,043 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 19:45:33,772 - INFO - root - 跳过已处理论文 A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies：D:\ChatPaper\academic Papers\Quantization-Aware-Training\A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat-2.pdf
2025-11-09 19:45:33,773 - INFO - root - 正在总结论文 2/2: FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error
2025-11-09 19:46:16,637 - INFO - root - LLMClient: rate limit reached, sleeping 17.1s
2025-11-09 19:46:52,812 - INFO - root - 正在提取论文图片...
2025-11-09 19:47:00,391 - INFO - root - 已保存图片 1/10：./export\images_FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error\figure_1_page6.png
2025-11-09 19:47:01,367 - INFO - root - 已保存图片 2/10：./export\images_FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error\figure_2_page6.png
2025-11-09 19:47:01,725 - INFO - root - 已保存图片 3/10：./export\images_FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error\figure_3_page6.png
2025-11-09 19:47:02,103 - INFO - root - 已保存图片 4/10：./export\images_FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error\figure_4_page6.png
2025-11-09 19:47:02,152 - INFO - root - 成功添加图片 1：./export\images_FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error\figure_1_page6.png
2025-11-09 19:47:02,153 - INFO - root - 成功添加图片 2：./export\images_FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error\figure_2_page6.png
2025-11-09 19:47:02,153 - INFO - root - 成功添加图片 3：./export\images_FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error\figure_3_page6.png
2025-11-09 19:47:02,153 - INFO - root - 成功添加图片 4：./export\images_FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error\figure_4_page6.png
2025-11-09 19:47:02,178 - INFO - root - 论文《FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error》的分析已保存到 ./export\FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error.md
2025-11-09 19:47:02,276 - INFO - root - summary time: 177.86 seconds
2025-11-09 20:00:55,741 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 20:00:55,743 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 20:00:55,745 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 20:00:57,021 - INFO - root - LLMClientManager: 指定使用客户端: Deepseek
2025-11-09 20:00:57,021 - INFO - root - DeepSeekClient: API key found: 9f9e270e-b...
2025-11-09 20:00:57,023 - INFO - root - DeepSeekClient: 使用模式: 火山引擎
2025-11-09 20:01:00,188 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 404 Not Found"
2025-11-09 20:01:00,190 - ERROR - root - DeepSeekClient: error during initialization: Error code: 404 - {'error': {'code': 'InvalidEndpointOrModel.NotFound', 'message': 'The model or endpoint deepseek-chat does not exist or you do not have access to it. Request id: 021762689659689f83b1f853f59e3d47d2ccd8cfd02b65a46c588', 'param': '', 'type': 'Not Found'}}
2025-11-09 20:01:00,190 - ERROR - root - LLMClientManager: 指定的客户端 DeepSeek 初始化失败
2025-11-09 20:01:00,190 - WARNING - root - LLMClientManager: 指定的客户端 Deepseek 不可用，将尝试其他客户端
2025-11-09 20:01:01,087 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 20:01:03,306 - INFO - root - GeminiClient: initialized model gemini-2.5-flash
2025-11-09 20:01:03,307 - INFO - root - LLMClientManager: Gemini client initialized successfully
2025-11-09 20:01:03,307 - INFO - root - LLMClientManager: using Gemini as default client
2025-11-09 20:01:03,307 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 20:01:03,308 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 20:01:03,308 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 20:01:03,309 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 20:01:03,309 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 20:01:07,975 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 20:01:07,985 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 20:01:07,986 - INFO - root - LLMClientManager: Doubao client initialized successfully
2025-11-09 20:01:07,986 - WARNING - root - LLMClientManager: client Deepseek not available
2025-11-09 20:01:07,987 - WARNING - root - 无法切换到指定的客户端 Deepseek，将使用默认客户端
2025-11-09 20:01:07,987 - INFO - root - 可用客户端: ['Gemini', 'Doubao']
2025-11-09 20:01:07,987 - INFO - root - 使用 LLM 模型: gemini-2.5-flash
2025-11-09 20:01:07,988 - INFO - root - 可用客户端: ['Gemini', 'Doubao']
2025-11-09 20:01:07,988 - INFO - root - === 运行配置 ===
2025-11-09 20:01:07,988 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 20:01:07,989 - INFO - root - 关键词: QAT
2025-11-09 20:01:07,989 - INFO - root - 查询: Quantization-Aware-Training
2025-11-09 20:01:07,989 - INFO - root - 排序: None
2025-11-09 20:01:07,989 - INFO - root - 最近天数: 180
2025-11-09 20:01:07,990 - INFO - root - 最大处理数量: 2
2025-11-09 20:01:07,990 - INFO - root - 保存图片: 是
2025-11-09 20:01:07,991 - INFO - root - 输出语言: 中文
2025-11-09 20:01:07,991 - INFO - root - 强制重新处理: 否
2025-11-09 20:01:07,991 - INFO - root - ====================
2025-11-09 20:01:07,992 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 20:01:07,994 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 20:01:14,406 - INFO - root - get_all_titles_from_web 
2025-11-09 20:01:14,407 - INFO - root - Page:0, Index:0, A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies, https://arxiv.org/pdf/2511.03201, 2025-11-05
2025-11-09 20:01:14,407 - INFO - root - Page:0, Index:1, FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error, https://arxiv.org/pdf/2511.02302, 2025-11-04
2025-11-09 20:01:14,407 - INFO - root - Page:0, Index:2, Outlier-Aware Post-Training Quantization for Image Super-Resolution, https://arxiv.org/pdf/2511.00682, 2025-11-01
2025-11-09 20:01:14,407 - INFO - root - Page:0, Index:3, Evaluation of Wafer-Scale SOT-MRAM for Analog Crossbar Array Applications, https://arxiv.org/pdf/2510.25853, 2025-11-03
2025-11-09 20:01:14,408 - INFO - root - Page:0, Index:4, Improving the Straight-Through Estimator with Zeroth-Order Information, https://arxiv.org/pdf/2510.23926, 2025-10-27
2025-11-09 20:01:14,408 - INFO - root - Page:0, Index:5, Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework, https://arxiv.org/pdf/2510.22243, 2025-10-25
2025-11-09 20:01:14,408 - INFO - root - Page:0, Index:6, TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge, https://arxiv.org/pdf/2510.21879, 2025-10-23
2025-11-09 20:01:14,409 - INFO - root - Page:0, Index:7, KARIPAP: Quantum-Inspired Tensor Network Compression of Large Language Models Using Infinite Projected Entangled Pair States and Tensor Renormalization Group, https://arxiv.org/pdf/2510.21844, 2025-10-22
2025-11-09 20:01:14,409 - INFO - root - Page:0, Index:8, A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization, https://arxiv.org/pdf/2510.21314, 2025-10-24
2025-11-09 20:01:14,409 - INFO - root - Page:0, Index:9, Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks, https://arxiv.org/pdf/2510.19760, 2025-10-22
2025-11-09 20:01:14,409 - INFO - root - Page:0, Index:10, CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training, https://arxiv.org/pdf/2510.18784, 2025-10-21
2025-11-09 20:01:14,409 - INFO - root - Page:0, Index:11, Mixed-Precision Quantization for Language Models: Techniques and Prospects, https://arxiv.org/pdf/2510.16805, 2025-10-19
2025-11-09 20:01:14,410 - INFO - root - Page:0, Index:12, SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation, https://arxiv.org/pdf/2510.16396, 2025-10-30
2025-11-09 20:01:14,411 - INFO - root - Page:0, Index:13, CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models, https://arxiv.org/pdf/2510.15962, 2025-10-11
2025-11-09 20:01:14,412 - INFO - root - Page:0, Index:14, SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization, https://arxiv.org/pdf/2510.15775, 2025-10-17
2025-11-09 20:01:14,412 - INFO - root - Page:0, Index:15, SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware, https://arxiv.org/pdf/2510.15542, 2025-10-31
2025-11-09 20:01:14,413 - INFO - root - Page:0, Index:16, GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework, https://arxiv.org/pdf/2510.15299, 2025-10-17
2025-11-09 20:01:14,413 - INFO - root - Page:0, Index:17, SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images, https://arxiv.org/pdf/2510.15072, 2025-10-16
2025-11-09 20:01:14,414 - INFO - root - Page:0, Index:18, FraQAT: Quantization Aware Training with Fractional bits, https://arxiv.org/pdf/2510.14823, 2025-10-16
2025-11-09 20:01:14,414 - INFO - root - Page:0, Index:19, Computing-In-Memory Aware Model Adaption For Edge Devices, https://arxiv.org/pdf/2510.14379, 2025-10-16
2025-11-09 20:01:14,414 - INFO - root - Page:0, Index:20, Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge, https://arxiv.org/pdf/2510.13760, 2025-11-04
2025-11-09 20:01:14,419 - INFO - root - Page:0, Index:21, NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models, https://arxiv.org/pdf/2510.13068, 2025-10-14
2025-11-09 20:01:14,425 - INFO - root - Page:0, Index:22, Detect Anything via Next Point Prediction, https://arxiv.org/pdf/2510.12798, 2025-10-14
2025-11-09 20:01:14,426 - INFO - root - Page:0, Index:23, AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model, https://arxiv.org/pdf/2510.11496, 2025-10-14
2025-11-09 20:01:14,426 - INFO - root - Page:0, Index:24, Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware, https://arxiv.org/pdf/2510.11484, 2025-10-13
2025-11-09 20:01:14,427 - INFO - root - Page:0, Index:25, SASER: Stego attacks on open-source LLMs, https://arxiv.org/pdf/2510.10486, 2025-10-12
2025-11-09 20:01:14,427 - INFO - root - Page:0, Index:26, Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition, https://arxiv.org/pdf/2510.09653, 2025-10-15
2025-11-09 20:01:14,428 - INFO - root - Page:0, Index:27, A Theoretically-Grounded Codebook for Digital Semantic Communications, https://arxiv.org/pdf/2510.07108, 2025-10-14
2025-11-09 20:01:14,428 - INFO - root - Page:0, Index:28, QuantDemoire: Quantization with Outlier Aware for Image Demoiréing, https://arxiv.org/pdf/2510.04066, 2025-10-05
2025-11-09 20:01:14,429 - INFO - root - Page:0, Index:29, Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization, https://arxiv.org/pdf/2510.03763, 2025-10-04
2025-11-09 20:01:14,429 - INFO - root - Page:0, Index:30, Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models, https://arxiv.org/pdf/2510.03274, 2025-09-27
2025-11-09 20:01:14,430 - INFO - root - Page:0, Index:31, PT$^2$-LLM: Post-Training Ternarization for Large Language Models, https://arxiv.org/pdf/2510.03267, 2025-09-26
2025-11-09 20:01:14,430 - INFO - root - Page:0, Index:32, Purrception: Variational Flow Matching for Vector-Quantized Image Generation, https://arxiv.org/pdf/2510.01478, 2025-10-01
2025-11-09 20:01:14,431 - INFO - root - Page:0, Index:33, Post-Training Quantization for Audio Diffusion Transformers, https://arxiv.org/pdf/2510.00313, 2025-09-30
2025-11-09 20:01:14,431 - INFO - root - Page:0, Index:34, Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling, https://arxiv.org/pdf/2510.00028, 2025-09-25
2025-11-09 20:01:14,443 - INFO - root - Page:0, Index:35, Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models, https://arxiv.org/pdf/2509.26436, 2025-09-30
2025-11-09 20:01:14,450 - INFO - root - Page:0, Index:36, Cat: Post-Training Quantization Error Reduction via Cluster-based Affine Transformation, https://arxiv.org/pdf/2509.26277, 2025-10-07
2025-11-09 20:01:14,452 - INFO - root - Page:0, Index:37, CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models, https://arxiv.org/pdf/2509.25996, 2025-09-30
2025-11-09 20:01:14,452 - INFO - root - Page:0, Index:38, Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications, https://arxiv.org/pdf/2509.25439, 2025-09-29
2025-11-09 20:01:14,453 - INFO - root - Page:0, Index:39, On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs, https://arxiv.org/pdf/2509.25214, 2025-09-22
2025-11-09 20:01:14,453 - INFO - root - Page:0, Index:40, VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning, https://arxiv.org/pdf/2509.24650, 2025-09-29
2025-11-09 20:01:14,454 - INFO - root - Page:0, Index:41, S$^2$NN: Sub-bit Spiking Neural Networks, https://arxiv.org/pdf/2509.24266, 2025-10-24
2025-11-09 20:01:14,454 - INFO - root - Page:0, Index:42, Tequila: Trapping-free Ternary Quantization for Large Language Models, https://arxiv.org/pdf/2509.23809, 2025-10-17
2025-11-09 20:01:14,454 - INFO - root - Page:0, Index:43, Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution, https://arxiv.org/pdf/2509.23774, 2025-09-30
2025-11-09 20:01:14,454 - INFO - root - Page:0, Index:44, RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization, https://arxiv.org/pdf/2509.23582, 2025-09-27
2025-11-09 20:01:14,455 - INFO - root - Page:0, Index:45, Beyond Outliers: A Study of Optimizers Under Quantization, https://arxiv.org/pdf/2509.23500, 2025-10-02
2025-11-09 20:01:14,455 - INFO - root - Page:0, Index:46, Compute-Optimal Quantization-Aware Training, https://arxiv.org/pdf/2509.22935, 2025-09-26
2025-11-09 20:01:14,456 - INFO - root - Page:0, Index:47, COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning, https://arxiv.org/pdf/2509.22075, 2025-10-06
2025-11-09 20:01:14,457 - INFO - root - Page:0, Index:48, SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models, https://arxiv.org/pdf/2509.21498, 2025-09-25
2025-11-09 20:01:14,459 - INFO - root - Page:0, Index:49, Quantized Visual Geometry Grounded Transformer, https://arxiv.org/pdf/2509.21302, 2025-09-29
2025-11-09 20:01:14,459 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 20:01:20,927 - INFO - root - get_all_titles_from_web 
2025-11-09 20:01:20,927 - INFO - root - Page:1, Index:0, Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy, https://arxiv.org/pdf/2509.21173, 2025-10-27
2025-11-09 20:01:20,927 - INFO - root - Page:1, Index:1, Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer, https://arxiv.org/pdf/2509.20854, 2025-09-25
2025-11-09 20:01:20,928 - INFO - root - Page:1, Index:2, TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection, https://arxiv.org/pdf/2509.18193, 2025-09-19
2025-11-09 20:01:20,928 - INFO - root - Page:1, Index:3, SBVR: Summation of BitVector Representation for Efficient LLM Quantization, https://arxiv.org/pdf/2509.18172, 2025-09-17
2025-11-09 20:01:20,929 - INFO - root - Page:1, Index:4, QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2509.17428, 2025-09-26
2025-11-09 20:01:20,929 - INFO - root - Page:1, Index:5, PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models, https://arxiv.org/pdf/2509.16989, 2025-10-28
2025-11-09 20:01:20,929 - INFO - root - Page:1, Index:6, MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training, https://arxiv.org/pdf/2509.15514, 2025-09-18
2025-11-09 20:01:20,929 - INFO - root - Page:1, Index:7, Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs, https://arxiv.org/pdf/2509.14391, 2025-09-17
2025-11-09 20:01:20,930 - INFO - root - Page:1, Index:8, Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization, https://arxiv.org/pdf/2509.13786, 2025-09-19
2025-11-09 20:01:20,930 - INFO - root - Page:1, Index:9, Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees, https://arxiv.org/pdf/2509.11054, 2025-09-13
2025-11-09 20:01:20,930 - INFO - root - Page:1, Index:10, SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models, https://arxiv.org/pdf/2509.09090, 2025-09-10
2025-11-09 20:01:20,931 - INFO - root - Page:1, Index:11, CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks with Entropy Regularization, https://arxiv.org/pdf/2509.08776, 2025-09-10
2025-11-09 20:01:20,931 - INFO - root - Page:1, Index:12, Explaining How Quantization Disparately Skews a Model, https://arxiv.org/pdf/2509.07222, 2025-09-08
2025-11-09 20:01:20,932 - INFO - root - Page:1, Index:13, LoaQ: Layer-wise Output Approximation Quantization, https://arxiv.org/pdf/2509.06297, 2025-09-07
2025-11-09 20:01:20,932 - INFO - root - Page:1, Index:14, FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving, https://arxiv.org/pdf/2509.06261, 2025-09-14
2025-11-09 20:01:20,933 - INFO - root - Page:1, Index:15, Sensitivity-Aware Post-Training Quantization for Deep Neural Networks, https://arxiv.org/pdf/2509.05576, 2025-09-05
2025-11-09 20:01:20,933 - INFO - root - Page:1, Index:16, SuperSNN: A Hardware-Aware Framework for Physically Realizable, High-Performance Superconducting Spiking Neural Network Chips, https://arxiv.org/pdf/2509.05532, 2025-09-05
2025-11-09 20:01:20,934 - INFO - root - Page:1, Index:17, Data-Augmented Quantization-Aware Knowledge Distillation, https://arxiv.org/pdf/2509.03850, 2025-09-03
2025-11-09 20:01:20,934 - INFO - root - Page:1, Index:18, DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling, https://arxiv.org/pdf/2509.03472, 2025-09-03
2025-11-09 20:01:20,934 - INFO - root - Page:1, Index:19, Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs, https://arxiv.org/pdf/2509.02017, 2025-09-02
2025-11-09 20:01:20,935 - INFO - root - Page:1, Index:20, Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling, https://arxiv.org/pdf/2509.01624, 2025-09-01
2025-11-09 20:01:20,935 - INFO - root - Page:1, Index:21, Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective, https://arxiv.org/pdf/2509.00859, 2025-08-31
2025-11-09 20:01:20,936 - INFO - root - Page:1, Index:22, Progressive Element-wise Gradient Estimation for Neural Network Quantization, https://arxiv.org/pdf/2509.00097, 2025-08-27
2025-11-09 20:01:20,936 - INFO - root - Page:1, Index:23, End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost, https://arxiv.org/pdf/2509.00031, 2025-09-29
2025-11-09 20:01:20,936 - INFO - root - Page:1, Index:24, Quantization Robustness to Input Degradations for Object Detection, https://arxiv.org/pdf/2508.19600, 2025-08-27
2025-11-09 20:01:20,937 - INFO - root - Page:1, Index:25, Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models, https://arxiv.org/pdf/2508.18609, 2025-08-27
2025-11-09 20:01:20,937 - INFO - root - Page:1, Index:26, AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration, https://arxiv.org/pdf/2508.18025, 2025-08-25
2025-11-09 20:01:20,937 - INFO - root - Page:1, Index:27, TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling, https://arxiv.org/pdf/2508.16790, 2025-08-22
2025-11-09 20:01:20,937 - INFO - root - Page:1, Index:28, A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering, https://arxiv.org/pdf/2508.16516, 2025-08-22
2025-11-09 20:01:20,939 - INFO - root - Page:1, Index:29, JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs, https://arxiv.org/pdf/2508.15468, 2025-08-21
2025-11-09 20:01:20,944 - INFO - root - Page:1, Index:30, MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation, https://arxiv.org/pdf/2508.15281, 2025-08-21
2025-11-09 20:01:20,945 - INFO - root - Page:1, Index:31, DLLMQuant: Quantizing Diffusion-based Large Language Models, https://arxiv.org/pdf/2508.14090, 2025-08-25
2025-11-09 20:01:20,946 - INFO - root - Page:1, Index:32, Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management, https://arxiv.org/pdf/2508.13905, 2025-08-19
2025-11-09 20:01:20,946 - INFO - root - Page:1, Index:33, XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads, https://arxiv.org/pdf/2508.13049, 2025-08-18
2025-11-09 20:01:20,946 - INFO - root - Page:1, Index:34, Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection, https://arxiv.org/pdf/2508.12230, 2025-08-17
2025-11-09 20:01:20,947 - INFO - root - Page:1, Index:35, TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks, https://arxiv.org/pdf/2508.12132, 2025-08-16
2025-11-09 20:01:20,947 - INFO - root - Page:1, Index:36, Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion, https://arxiv.org/pdf/2508.12094, 2025-10-13
2025-11-09 20:01:20,948 - INFO - root - Page:1, Index:37, Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware, https://arxiv.org/pdf/2508.11940, 2025-08-16
2025-11-09 20:01:20,948 - INFO - root - Page:1, Index:38, PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks, https://arxiv.org/pdf/2508.10557, 2025-08-15
2025-11-09 20:01:20,948 - INFO - root - Page:1, Index:39, MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI, https://arxiv.org/pdf/2508.09500, 2025-08-13
2025-11-09 20:01:20,949 - INFO - root - Page:1, Index:40, SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via Codebooks for recommender system, https://arxiv.org/pdf/2508.09090, 2025-08-12
2025-11-09 20:01:20,949 - INFO - root - Page:1, Index:41, Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models, https://arxiv.org/pdf/2508.06974, 2025-08-09
2025-11-09 20:01:20,949 - INFO - root - Page:1, Index:42, Efficient Deep Neural Receiver with Post-Training Quantization, https://arxiv.org/pdf/2508.06275, 2025-08-08
2025-11-09 20:01:20,950 - INFO - root - Page:1, Index:43, iFairy: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$, https://arxiv.org/pdf/2508.05571, 2025-08-16
2025-11-09 20:01:20,950 - INFO - root - Page:1, Index:44, S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation, https://arxiv.org/pdf/2508.04016, 2025-10-27
2025-11-09 20:01:20,950 - INFO - root - Page:1, Index:45, LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation, https://arxiv.org/pdf/2508.03485, 2025-09-23
2025-11-09 20:01:20,951 - INFO - root - Page:1, Index:46, VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation, https://arxiv.org/pdf/2508.03351, 2025-08-05
2025-11-09 20:01:20,951 - INFO - root - Page:1, Index:47, TF-MLPNet: Tiny Real-Time Neural Speech Separation, https://arxiv.org/pdf/2508.03047, 2025-08-04
2025-11-09 20:01:20,951 - INFO - root - Page:1, Index:48, SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation, https://arxiv.org/pdf/2508.01375, 2025-08-02
2025-11-09 20:01:20,952 - INFO - root - Page:1, Index:49, MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective, https://arxiv.org/pdf/2507.19131, 2025-07-25
2025-11-09 20:01:20,952 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 20:01:27,192 - INFO - root - get_all_titles_from_web 
2025-11-09 20:01:27,192 - INFO - root - Page:2, Index:0, Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction, https://arxiv.org/pdf/2507.17768, 2025-07-16
2025-11-09 20:01:27,193 - INFO - root - Page:2, Index:1, SiLQ: Simple Large Language Model Quantization-Aware Training, https://arxiv.org/pdf/2507.16933, 2025-07-22
2025-11-09 20:01:27,193 - INFO - root - Page:2, Index:2, Task-Specific Zero-shot Quantization-Aware Training for Object Detection, https://arxiv.org/pdf/2507.16782, 2025-07-22
2025-11-09 20:01:27,193 - INFO - root - Page:2, Index:3, TorchAO: PyTorch-Native Training-to-Serving Model Optimization, https://arxiv.org/pdf/2507.16099, 2025-07-21
2025-11-09 20:01:27,193 - INFO - root - Page:2, Index:4, SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models, https://arxiv.org/pdf/2507.14811, 2025-08-27
2025-11-09 20:01:27,193 - INFO - root - Page:2, Index:5, Apple Intelligence Foundation Language Models: Tech Report 2025, https://arxiv.org/pdf/2507.13575, 2025-08-27
2025-11-09 20:01:27,193 - INFO - root - Page:2, Index:6, Generative Multi-Target Cross-Domain Recommendation, https://arxiv.org/pdf/2507.12871, 2025-08-07
2025-11-09 20:01:27,195 - INFO - root - Page:2, Index:7, DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training, https://arxiv.org/pdf/2507.07149, 2025-07-09
2025-11-09 20:01:27,195 - INFO - root - Page:2, Index:8, Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics, https://arxiv.org/pdf/2507.07044, 2025-07-15
2025-11-09 20:01:27,196 - INFO - root - Page:2, Index:9, QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models, https://arxiv.org/pdf/2507.06079, 2025-07-08
2025-11-09 20:01:27,196 - INFO - root - Page:2, Index:10, GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field, https://arxiv.org/pdf/2507.05594, 2025-07-07
2025-11-09 20:01:27,196 - INFO - root - Page:2, Index:11, Hita: Holistic Tokenizer for Autoregressive Image Generation, https://arxiv.org/pdf/2507.02358, 2025-07-11
2025-11-09 20:01:27,196 - INFO - root - Page:2, Index:12, QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference, https://arxiv.org/pdf/2506.23934, 2025-06-30
2025-11-09 20:01:27,197 - INFO - root - Page:2, Index:13, FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization, https://arxiv.org/pdf/2506.23516, 2025-07-21
2025-11-09 20:01:27,197 - INFO - root - Page:2, Index:14, Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models, https://arxiv.org/pdf/2506.23025, 2025-06-28
2025-11-09 20:01:27,198 - INFO - root - Page:2, Index:15, Joint Quantization and Pruning Neural Networks Approach: A Case Study on FSO Receivers, https://arxiv.org/pdf/2506.20084, 2025-06-24
2025-11-09 20:01:27,198 - INFO - root - Page:2, Index:16, Single-step Diffusion for Image Compression at Ultra-Low Bitrates, https://arxiv.org/pdf/2506.16572, 2025-09-22
2025-11-09 20:01:27,199 - INFO - root - Page:2, Index:17, LittleBit: Ultra Low-Bit Quantization via Latent Factorization, https://arxiv.org/pdf/2506.13771, 2025-10-28
2025-11-09 20:01:27,199 - INFO - root - Page:2, Index:18, MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering, https://arxiv.org/pdf/2506.13755, 2025-06-16
2025-11-09 20:01:27,200 - INFO - root - Page:2, Index:19, EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization, https://arxiv.org/pdf/2506.13329, 2025-07-04
2025-11-09 20:01:27,200 - INFO - root - Page:2, Index:20, Towards Neural Audio Codec Source Parsing, https://arxiv.org/pdf/2506.12627, 2025-06-14
2025-11-09 20:01:27,200 - INFO - root - Page:2, Index:21, Quantizing Small-Scale State-Space Models for Edge AI, https://arxiv.org/pdf/2506.12480, 2025-06-14
2025-11-09 20:01:27,200 - INFO - root - Page:2, Index:22, EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction, https://arxiv.org/pdf/2506.12015, 2025-06-13
2025-11-09 20:01:27,200 - INFO - root - Page:2, Index:23, Compression Aware Certified Training, https://arxiv.org/pdf/2506.11992, 2025-06-13
2025-11-09 20:01:27,203 - INFO - root - Page:2, Index:24, GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers, https://arxiv.org/pdf/2506.11784, 2025-06-13
2025-11-09 20:01:27,203 - INFO - root - Page:2, Index:25, TruncQuant: Truncation-Ready Quantization for DNNs with Flexible Weight Bit Precision, https://arxiv.org/pdf/2506.11431, 2025-06-12
2025-11-09 20:01:27,203 - INFO - root - Page:2, Index:26, EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices, https://arxiv.org/pdf/2506.11093, 2025-06-05
2025-11-09 20:01:27,204 - INFO - root - Page:2, Index:27, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 20:01:27,204 - INFO - root - Page:2, Index:28, AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent, https://arxiv.org/pdf/2506.10205, 2025-06-11
2025-11-09 20:01:27,204 - INFO - root - Page:2, Index:29, Q-SAM2: Accurate Quantization for Segment Anything Model 2, https://arxiv.org/pdf/2506.09782, 2025-06-11
2025-11-09 20:01:27,204 - INFO - root - Page:2, Index:30, Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs, https://arxiv.org/pdf/2506.09104, 2025-06-10
2025-11-09 20:01:27,205 - INFO - root - Page:2, Index:31, Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU, https://arxiv.org/pdf/2506.08911, 2025-06-10
2025-11-09 20:01:27,207 - INFO - root - Page:2, Index:32, POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration, https://arxiv.org/pdf/2506.08785, 2025-06-10
2025-11-09 20:01:27,207 - INFO - root - Page:2, Index:33, MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts, https://arxiv.org/pdf/2506.07533, 2025-06-09
2025-11-09 20:01:27,209 - INFO - root - Page:2, Index:34, BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation, https://arxiv.org/pdf/2506.07530, 2025-06-09
2025-11-09 20:01:27,209 - INFO - root - Page:2, Index:35, RecGPT: A Foundation Model for Sequential Recommendation, https://arxiv.org/pdf/2506.06270, 2025-06-12
2025-11-09 20:01:27,209 - INFO - root - Page:2, Index:36, FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion, https://arxiv.org/pdf/2506.04648, 2025-06-05
2025-11-09 20:01:27,211 - INFO - root - Page:2, Index:37, BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing, https://arxiv.org/pdf/2506.03515, 2025-06-03
2025-11-09 20:01:27,212 - INFO - root - Page:2, Index:38, ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding, https://arxiv.org/pdf/2506.01853, 2025-06-02
2025-11-09 20:01:27,213 - INFO - root - Page:2, Index:39, Movable Antenna Enhanced Federated Fine-Tuning of Large Language Models via Hybrid Client Selection Optimization, https://arxiv.org/pdf/2506.00011, 2025-10-26
2025-11-09 20:01:27,215 - INFO - root - Page:2, Index:40, Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach, https://arxiv.org/pdf/2505.24721, 2025-05-30
2025-11-09 20:01:27,215 - INFO - root - Page:2, Index:41, GARLIC: GAussian Representation LearnIng for spaCe partitioning, https://arxiv.org/pdf/2505.24608, 2025-10-02
2025-11-09 20:01:27,216 - INFO - root - Page:2, Index:42, AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up, https://arxiv.org/pdf/2505.24584, 2025-08-18
2025-11-09 20:01:27,217 - INFO - root - Page:2, Index:43, Model-Preserving Adaptive Rounding, https://arxiv.org/pdf/2505.22988, 2025-09-25
2025-11-09 20:01:27,217 - INFO - root - Page:2, Index:44, Highly Efficient and Effective LLMs with Multi-Boolean Architectures, https://arxiv.org/pdf/2505.22811, 2025-10-03
2025-11-09 20:01:27,217 - INFO - root - Page:2, Index:45, Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning, https://arxiv.org/pdf/2505.21591, 2025-05-27
2025-11-09 20:01:27,217 - INFO - root - Page:2, Index:46, Frequency Composition for Compressed and Domain-Adaptive Neural Networks, https://arxiv.org/pdf/2505.20890, 2025-05-27
2025-11-09 20:01:27,218 - INFO - root - Page:2, Index:47, MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition, https://arxiv.org/pdf/2505.20744, 2025-10-27
2025-11-09 20:01:27,218 - INFO - root - Page:2, Index:48, Optimizing edge AI models on HPC systems with the edge in the loop, https://arxiv.org/pdf/2505.19995, 2025-05-26
2025-11-09 20:01:27,223 - INFO - root - Fetching page 4 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=150
2025-11-09 20:01:33,695 - INFO - root - get_all_titles_from_web 
2025-11-09 20:01:33,696 - INFO - root - Page:3, Index:0, DPASyn: Mechanism-Aware Drug Synergy Prediction via Dual Attention and Precision-Aware Quantization, https://arxiv.org/pdf/2505.19144, 2025-09-20
2025-11-09 20:01:33,697 - INFO - root - Page:3, Index:1, Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding, https://arxiv.org/pdf/2505.18758, 2025-05-24
2025-11-09 20:01:33,697 - INFO - root - Page:3, Index:2, Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization, https://arxiv.org/pdf/2505.18113, 2025-05-23
2025-11-09 20:01:33,698 - INFO - root - Page:3, Index:3, Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs, https://arxiv.org/pdf/2505.17662, 2025-09-19
2025-11-09 20:01:33,698 - INFO - root - Page:3, Index:4, FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design, https://arxiv.org/pdf/2505.16335, 2025-05-22
2025-11-09 20:01:33,698 - INFO - root - Page:3, Index:5, Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control, https://arxiv.org/pdf/2505.15304, 2025-05-30
2025-11-09 20:01:33,700 - INFO - root - Page:3, Index:6, Scaling Law for Quantization-Aware Training, https://arxiv.org/pdf/2505.14302, 2025-05-20
2025-11-09 20:01:33,701 - INFO - root - Page:3, Index:7, Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs, https://arxiv.org/pdf/2505.13060, 2025-05-19
2025-11-09 20:01:33,706 - INFO - root - Page:3, Index:8, Deep Unfolding with Kernel-based Quantization in MIMO Detection, https://arxiv.org/pdf/2505.12736, 2025-05-19
2025-11-09 20:01:33,707 - INFO - root - Page:3, Index:9, FedHQ: Hybrid Runtime Quantization for Federated Learning, https://arxiv.org/pdf/2505.11982, 2025-05-17
2025-11-09 20:01:33,709 - INFO - root - Page:3, Index:10, QVGen: Pushing the Limit of Quantized Video Generative Models, https://arxiv.org/pdf/2505.11497, 2025-09-27
2025-11-09 20:01:33,711 - INFO - root - Page:3, Index:11, EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes, https://arxiv.org/pdf/2505.10787, 2025-05-15
2025-11-09 20:01:33,713 - INFO - root - Page:3, Index:12, ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition, https://arxiv.org/pdf/2505.08981, 2025-05-13
2025-11-09 20:01:33,714 - INFO - root - Page:3, Index:13, PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs, https://arxiv.org/pdf/2505.03254, 2025-08-06
2025-11-09 20:01:33,714 - INFO - root - Page:3, Index:14, ICQuant: Index Coding enables Low-bit LLM Quantization, https://arxiv.org/pdf/2505.00850, 2025-08-24
2025-11-09 20:01:33,714 - INFO - root - Page:3, Index:15, Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining, https://arxiv.org/pdf/2504.13932, 2025-07-30
2025-11-09 20:01:33,715 - INFO - root - Page:3, Index:16, Achieving binary weight and activation for LLMs using Post-Training Quantization, https://arxiv.org/pdf/2504.05352, 2025-06-30
2025-11-09 20:01:33,715 - INFO - root - Page:3, Index:17, Wireless Hearables With Programmable Speech AI Accelerators, https://arxiv.org/pdf/2503.18698, 2025-10-21
2025-11-09 20:01:33,715 - INFO - root - Page:3, Index:18, GranQ: Efficient Channel-wise Quantization via Vectorized Pre-Scaling for Zero-Shot QAT, https://arxiv.org/pdf/2503.18339, 2025-10-15
2025-11-09 20:01:33,716 - INFO - root - Page:3, Index:19, CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting, https://arxiv.org/pdf/2503.12836, 2025-09-29
2025-11-09 20:01:33,716 - INFO - root - Page:3, Index:20, AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model, https://arxiv.org/pdf/2503.03088, 2025-07-11
2025-11-09 20:01:33,717 - INFO - root - Page:3, Index:21, Teaching Metric Distance to Discrete Autoregressive Language Models, https://arxiv.org/pdf/2503.02379, 2025-10-07
2025-11-09 20:01:33,718 - INFO - root - Page:3, Index:22, Regularization-based Framework for Quantization-, Fault- and Variability-Aware Training, https://arxiv.org/pdf/2503.01297, 2025-07-12
2025-11-09 20:01:33,721 - INFO - root - Fetching page 5 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=200
2025-11-09 20:01:41,539 - INFO - root - get_all_titles_from_web 
2025-11-09 20:01:41,539 - INFO - root - Page:4, Index:0, Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models, https://arxiv.org/pdf/2502.15799, 2025-06-29
2025-11-09 20:01:41,540 - INFO - root - Page:4, Index:1, Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer, https://arxiv.org/pdf/2502.15779, 2025-09-01
2025-11-09 20:01:41,540 - INFO - root - Page:4, Index:2, RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models, https://arxiv.org/pdf/2502.09003, 2025-06-05
2025-11-09 20:01:41,540 - INFO - root - Page:4, Index:3, Spatial Degradation-Aware and Temporal Consistent Diffusion Model for Compressed Video Super-Resolution, https://arxiv.org/pdf/2502.07381, 2025-06-27
2025-11-09 20:01:41,540 - INFO - root - Page:4, Index:4, QuEST: Stable Training of LLMs with 1-Bit Weights and Activations, https://arxiv.org/pdf/2502.05003, 2025-06-10
2025-11-09 20:01:41,540 - INFO - root - Page:4, Index:5, HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs, https://arxiv.org/pdf/2501.02625, 2025-11-05
2025-11-09 20:01:41,540 - INFO - root - Page:4, Index:6, TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction, https://arxiv.org/pdf/2412.16919, 2025-08-08
2025-11-09 20:01:41,542 - INFO - root - Page:4, Index:7, Training Multi-Layer Binary Neural Networks With Local Binary Error Signals, https://arxiv.org/pdf/2412.00119, 2025-08-05
2025-11-09 20:01:41,542 - INFO - root - Page:4, Index:8, Scaling Large-scale GNN Training to Thousands of Processors on CPU-based Supercomputers, https://arxiv.org/pdf/2411.16025, 2025-05-26
2025-11-09 20:01:41,542 - INFO - root - Page:4, Index:9, HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization, https://arxiv.org/pdf/2411.06581, 2025-05-16
2025-11-09 20:01:41,544 - INFO - root - Fetching page 6 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=250
2025-11-09 20:01:51,894 - INFO - root - get_all_titles_from_web 
2025-11-09 20:01:51,895 - INFO - root - Page:5, Index:0, Gradient-Free Training of Quantized Neural Networks, https://arxiv.org/pdf/2410.09734, 2025-09-29
2025-11-09 20:01:51,895 - INFO - root - Page:5, Index:1, QT-DoG: Quantization-aware Training for Domain Generalization, https://arxiv.org/pdf/2410.06020, 2025-06-26
2025-11-09 20:01:51,897 - INFO - root - Page:5, Index:2, Constraint Guided Model Quantization of Neural Networks, https://arxiv.org/pdf/2409.20138, 2025-09-12
2025-11-09 20:01:51,899 - INFO - root - Page:5, Index:3, Accumulator-Aware Post-Training Quantization for Large Language Models, https://arxiv.org/pdf/2409.17092, 2025-07-30
2025-11-09 20:01:51,899 - INFO - root - Page:5, Index:4, DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation, https://arxiv.org/pdf/2409.14307, 2025-07-09
2025-11-09 20:01:51,901 - INFO - root - Page:5, Index:5, Robust Training of Neural Networks at Arbitrary Precision and Sparsity, https://arxiv.org/pdf/2409.09245, 2025-09-23
2025-11-09 20:01:51,931 - INFO - root - Page:5, Index:6, VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing, https://arxiv.org/pdf/2408.05758, 2025-05-27
2025-11-09 20:01:51,934 - INFO - root - Page:5, Index:7, DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers, https://arxiv.org/pdf/2408.03291, 2025-06-19
2025-11-09 20:01:51,935 - INFO - root - Page:5, Index:8, Temporal Feature Matters: A Framework for Diffusion Model Quantization, https://arxiv.org/pdf/2407.19547, 2025-07-11
2025-11-09 20:01:51,953 - INFO - root - Page:5, Index:9, EfficientQAT: Efficient Quantization-Aware Training for Large Language Models, https://arxiv.org/pdf/2407.11062, 2025-05-19
2025-11-09 20:01:51,977 - INFO - root - Page:5, Index:10, Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT, https://arxiv.org/pdf/2407.11041, 2025-10-31
2025-11-09 20:01:51,979 - INFO - root - Fetching page 7 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=300
2025-11-09 20:01:58,534 - INFO - root - get_all_titles_from_web 
2025-11-09 20:01:58,534 - INFO - root - Page:6, Index:0, BoA: Attention-aware Post-training Quantization without Backpropagation, https://arxiv.org/pdf/2406.13474, 2025-06-06
2025-11-09 20:01:58,534 - INFO - root - Page:6, Index:1, A Rescaling-Invariant Lipschitz Bound Based on Path-Metrics for Modern ReLU Network Parameterizations, https://arxiv.org/pdf/2405.15006, 2025-06-13
2025-11-09 20:01:58,535 - INFO - root - Page:6, Index:2, Scheduling Weight Transitions for Quantization-Aware Training, https://arxiv.org/pdf/2404.19248, 2025-10-01
2025-11-09 20:01:58,535 - INFO - root - Page:6, Index:3, GPTVQ: The Blessing of Dimensionality for LLM Quantization, https://arxiv.org/pdf/2402.15319, 2025-06-03
2025-11-09 20:01:58,535 - INFO - root - Fetching page 8 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=350
2025-11-09 20:02:06,126 - INFO - root - get_all_titles_from_web 
2025-11-09 20:02:06,126 - INFO - root - Page:7, Index:0, Squat: Quant Small Language Models on the Edge, https://arxiv.org/pdf/2402.10787, 2025-07-01
2025-11-09 20:02:06,127 - INFO - root - Page:7, Index:1, L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2402.04902, 2025-07-21
2025-11-09 20:02:06,127 - INFO - root - Fetching page 9 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=400
2025-11-09 20:02:12,933 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 20:02:25,773 - INFO - root - 跳过已处理论文 A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies：D:\ChatPaper\academic Papers\Quantization-Aware-Training\A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat-3.pdf
2025-11-09 20:02:25,779 - INFO - root - 跳过已处理论文 FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error：D:\ChatPaper\academic Papers\Quantization-Aware-Training\FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error-3.pdf
2025-11-09 20:02:25,832 - INFO - root - summary time: 90.09 seconds
2025-11-09 20:12:00,720 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 20:12:00,723 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 20:12:00,726 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 20:12:02,579 - INFO - root - LLMClientManager: 指定使用客户端: Deepseek
2025-11-09 20:12:02,580 - INFO - root - DeepSeekClient: API key found: 9f9e270e-b...
2025-11-09 20:12:02,582 - INFO - root - DeepSeekClient: 使用模式: 火山引擎
2025-11-09 20:12:02,583 - INFO - root - DeepSeekClient: 模型名称: deepseek-v3-1-terminus
2025-11-09 20:12:06,717 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 20:12:06,745 - INFO - root - DeepSeekClient: initialized successfully with model deepseek-v3-1-terminus
2025-11-09 20:12:06,745 - INFO - root - LLMClientManager: DeepSeek 客户端初始化成功
2025-11-09 20:12:06,745 - WARNING - root - LLMClientManager: client Deepseek not available
2025-11-09 20:12:06,746 - WARNING - root - 无法切换到指定的客户端 Deepseek，将使用默认客户端
2025-11-09 20:12:06,746 - INFO - root - 可用客户端: ['DeepSeek']
2025-11-09 20:12:06,746 - INFO - root - 使用 LLM 模型: deepseek-v3-1-terminus
2025-11-09 20:12:06,747 - INFO - root - 可用客户端: ['DeepSeek']
2025-11-09 20:12:06,748 - INFO - root - === 运行配置 ===
2025-11-09 20:12:06,749 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 20:12:06,750 - INFO - root - 关键词: QAT
2025-11-09 20:12:06,760 - INFO - root - 查询: Quantization-Aware-Training
2025-11-09 20:12:06,776 - INFO - root - 排序: None
2025-11-09 20:12:06,793 - INFO - root - 最近天数: 180
2025-11-09 20:12:06,794 - INFO - root - 最大处理数量: 2
2025-11-09 20:12:06,794 - INFO - root - 保存图片: 是
2025-11-09 20:12:06,794 - INFO - root - 输出语言: 中文
2025-11-09 20:12:06,795 - INFO - root - 强制重新处理: 否
2025-11-09 20:12:06,795 - INFO - root - ====================
2025-11-09 20:12:06,796 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 20:12:06,797 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 20:12:13,586 - INFO - root - get_all_titles_from_web 
2025-11-09 20:12:13,587 - INFO - root - Page:0, Index:0, A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies, https://arxiv.org/pdf/2511.03201, 2025-11-05
2025-11-09 20:12:13,587 - INFO - root - Page:0, Index:1, FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error, https://arxiv.org/pdf/2511.02302, 2025-11-04
2025-11-09 20:12:13,587 - INFO - root - Page:0, Index:2, Outlier-Aware Post-Training Quantization for Image Super-Resolution, https://arxiv.org/pdf/2511.00682, 2025-11-01
2025-11-09 20:12:13,587 - INFO - root - Page:0, Index:3, Evaluation of Wafer-Scale SOT-MRAM for Analog Crossbar Array Applications, https://arxiv.org/pdf/2510.25853, 2025-11-03
2025-11-09 20:12:13,588 - INFO - root - Page:0, Index:4, Improving the Straight-Through Estimator with Zeroth-Order Information, https://arxiv.org/pdf/2510.23926, 2025-10-27
2025-11-09 20:12:13,588 - INFO - root - Page:0, Index:5, Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework, https://arxiv.org/pdf/2510.22243, 2025-10-25
2025-11-09 20:12:13,588 - INFO - root - Page:0, Index:6, TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge, https://arxiv.org/pdf/2510.21879, 2025-10-23
2025-11-09 20:12:13,588 - INFO - root - Page:0, Index:7, KARIPAP: Quantum-Inspired Tensor Network Compression of Large Language Models Using Infinite Projected Entangled Pair States and Tensor Renormalization Group, https://arxiv.org/pdf/2510.21844, 2025-10-22
2025-11-09 20:12:13,590 - INFO - root - Page:0, Index:8, A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization, https://arxiv.org/pdf/2510.21314, 2025-10-24
2025-11-09 20:12:13,590 - INFO - root - Page:0, Index:9, Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks, https://arxiv.org/pdf/2510.19760, 2025-10-22
2025-11-09 20:12:13,590 - INFO - root - Page:0, Index:10, CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training, https://arxiv.org/pdf/2510.18784, 2025-10-21
2025-11-09 20:12:13,590 - INFO - root - Page:0, Index:11, Mixed-Precision Quantization for Language Models: Techniques and Prospects, https://arxiv.org/pdf/2510.16805, 2025-10-19
2025-11-09 20:12:13,591 - INFO - root - Page:0, Index:12, SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation, https://arxiv.org/pdf/2510.16396, 2025-10-30
2025-11-09 20:12:13,591 - INFO - root - Page:0, Index:13, CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models, https://arxiv.org/pdf/2510.15962, 2025-10-11
2025-11-09 20:12:13,592 - INFO - root - Page:0, Index:14, SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization, https://arxiv.org/pdf/2510.15775, 2025-10-17
2025-11-09 20:12:13,592 - INFO - root - Page:0, Index:15, SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware, https://arxiv.org/pdf/2510.15542, 2025-10-31
2025-11-09 20:12:13,592 - INFO - root - Page:0, Index:16, GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework, https://arxiv.org/pdf/2510.15299, 2025-10-17
2025-11-09 20:12:13,593 - INFO - root - Page:0, Index:17, SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images, https://arxiv.org/pdf/2510.15072, 2025-10-16
2025-11-09 20:12:13,593 - INFO - root - Page:0, Index:18, FraQAT: Quantization Aware Training with Fractional bits, https://arxiv.org/pdf/2510.14823, 2025-10-16
2025-11-09 20:12:13,594 - INFO - root - Page:0, Index:19, Computing-In-Memory Aware Model Adaption For Edge Devices, https://arxiv.org/pdf/2510.14379, 2025-10-16
2025-11-09 20:12:13,594 - INFO - root - Page:0, Index:20, Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge, https://arxiv.org/pdf/2510.13760, 2025-11-04
2025-11-09 20:12:13,594 - INFO - root - Page:0, Index:21, NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models, https://arxiv.org/pdf/2510.13068, 2025-10-14
2025-11-09 20:12:13,595 - INFO - root - Page:0, Index:22, Detect Anything via Next Point Prediction, https://arxiv.org/pdf/2510.12798, 2025-10-14
2025-11-09 20:12:13,595 - INFO - root - Page:0, Index:23, AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model, https://arxiv.org/pdf/2510.11496, 2025-10-14
2025-11-09 20:12:13,595 - INFO - root - Page:0, Index:24, Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware, https://arxiv.org/pdf/2510.11484, 2025-10-13
2025-11-09 20:12:13,595 - INFO - root - Page:0, Index:25, SASER: Stego attacks on open-source LLMs, https://arxiv.org/pdf/2510.10486, 2025-10-12
2025-11-09 20:12:13,597 - INFO - root - Page:0, Index:26, Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition, https://arxiv.org/pdf/2510.09653, 2025-10-15
2025-11-09 20:12:13,597 - INFO - root - Page:0, Index:27, A Theoretically-Grounded Codebook for Digital Semantic Communications, https://arxiv.org/pdf/2510.07108, 2025-10-14
2025-11-09 20:12:13,597 - INFO - root - Page:0, Index:28, QuantDemoire: Quantization with Outlier Aware for Image Demoiréing, https://arxiv.org/pdf/2510.04066, 2025-10-05
2025-11-09 20:12:13,598 - INFO - root - Page:0, Index:29, Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization, https://arxiv.org/pdf/2510.03763, 2025-10-04
2025-11-09 20:12:13,598 - INFO - root - Page:0, Index:30, Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models, https://arxiv.org/pdf/2510.03274, 2025-09-27
2025-11-09 20:12:13,598 - INFO - root - Page:0, Index:31, PT$^2$-LLM: Post-Training Ternarization for Large Language Models, https://arxiv.org/pdf/2510.03267, 2025-09-26
2025-11-09 20:12:13,602 - INFO - root - Page:0, Index:32, Purrception: Variational Flow Matching for Vector-Quantized Image Generation, https://arxiv.org/pdf/2510.01478, 2025-10-01
2025-11-09 20:12:13,602 - INFO - root - Page:0, Index:33, Post-Training Quantization for Audio Diffusion Transformers, https://arxiv.org/pdf/2510.00313, 2025-09-30
2025-11-09 20:12:13,603 - INFO - root - Page:0, Index:34, Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling, https://arxiv.org/pdf/2510.00028, 2025-09-25
2025-11-09 20:12:13,603 - INFO - root - Page:0, Index:35, Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models, https://arxiv.org/pdf/2509.26436, 2025-09-30
2025-11-09 20:12:13,604 - INFO - root - Page:0, Index:36, Cat: Post-Training Quantization Error Reduction via Cluster-based Affine Transformation, https://arxiv.org/pdf/2509.26277, 2025-10-07
2025-11-09 20:12:13,604 - INFO - root - Page:0, Index:37, CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models, https://arxiv.org/pdf/2509.25996, 2025-09-30
2025-11-09 20:12:13,604 - INFO - root - Page:0, Index:38, Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications, https://arxiv.org/pdf/2509.25439, 2025-09-29
2025-11-09 20:12:13,606 - INFO - root - Page:0, Index:39, On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs, https://arxiv.org/pdf/2509.25214, 2025-09-22
2025-11-09 20:12:13,606 - INFO - root - Page:0, Index:40, VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning, https://arxiv.org/pdf/2509.24650, 2025-09-29
2025-11-09 20:12:13,606 - INFO - root - Page:0, Index:41, S$^2$NN: Sub-bit Spiking Neural Networks, https://arxiv.org/pdf/2509.24266, 2025-10-24
2025-11-09 20:12:13,606 - INFO - root - Page:0, Index:42, Tequila: Trapping-free Ternary Quantization for Large Language Models, https://arxiv.org/pdf/2509.23809, 2025-10-17
2025-11-09 20:12:13,607 - INFO - root - Page:0, Index:43, Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution, https://arxiv.org/pdf/2509.23774, 2025-09-30
2025-11-09 20:12:13,607 - INFO - root - Page:0, Index:44, RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization, https://arxiv.org/pdf/2509.23582, 2025-09-27
2025-11-09 20:12:13,612 - INFO - root - Page:0, Index:45, Beyond Outliers: A Study of Optimizers Under Quantization, https://arxiv.org/pdf/2509.23500, 2025-10-02
2025-11-09 20:12:13,613 - INFO - root - Page:0, Index:46, Compute-Optimal Quantization-Aware Training, https://arxiv.org/pdf/2509.22935, 2025-09-26
2025-11-09 20:12:13,613 - INFO - root - Page:0, Index:47, COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning, https://arxiv.org/pdf/2509.22075, 2025-10-06
2025-11-09 20:12:13,614 - INFO - root - Page:0, Index:48, SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models, https://arxiv.org/pdf/2509.21498, 2025-09-25
2025-11-09 20:12:13,614 - INFO - root - Page:0, Index:49, Quantized Visual Geometry Grounded Transformer, https://arxiv.org/pdf/2509.21302, 2025-09-29
2025-11-09 20:12:13,615 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 20:12:20,154 - INFO - root - get_all_titles_from_web 
2025-11-09 20:12:20,155 - INFO - root - Page:1, Index:0, Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy, https://arxiv.org/pdf/2509.21173, 2025-10-27
2025-11-09 20:12:20,156 - INFO - root - Page:1, Index:1, Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer, https://arxiv.org/pdf/2509.20854, 2025-09-25
2025-11-09 20:12:20,156 - INFO - root - Page:1, Index:2, TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection, https://arxiv.org/pdf/2509.18193, 2025-09-19
2025-11-09 20:12:20,156 - INFO - root - Page:1, Index:3, SBVR: Summation of BitVector Representation for Efficient LLM Quantization, https://arxiv.org/pdf/2509.18172, 2025-09-17
2025-11-09 20:12:20,156 - INFO - root - Page:1, Index:4, QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2509.17428, 2025-09-26
2025-11-09 20:12:20,156 - INFO - root - Page:1, Index:5, PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models, https://arxiv.org/pdf/2509.16989, 2025-10-28
2025-11-09 20:12:20,156 - INFO - root - Page:1, Index:6, MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training, https://arxiv.org/pdf/2509.15514, 2025-09-18
2025-11-09 20:12:20,157 - INFO - root - Page:1, Index:7, Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs, https://arxiv.org/pdf/2509.14391, 2025-09-17
2025-11-09 20:12:20,157 - INFO - root - Page:1, Index:8, Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization, https://arxiv.org/pdf/2509.13786, 2025-09-19
2025-11-09 20:12:20,157 - INFO - root - Page:1, Index:9, Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees, https://arxiv.org/pdf/2509.11054, 2025-09-13
2025-11-09 20:12:20,157 - INFO - root - Page:1, Index:10, SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models, https://arxiv.org/pdf/2509.09090, 2025-09-10
2025-11-09 20:12:20,158 - INFO - root - Page:1, Index:11, CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks with Entropy Regularization, https://arxiv.org/pdf/2509.08776, 2025-09-10
2025-11-09 20:12:20,158 - INFO - root - Page:1, Index:12, Explaining How Quantization Disparately Skews a Model, https://arxiv.org/pdf/2509.07222, 2025-09-08
2025-11-09 20:12:20,158 - INFO - root - Page:1, Index:13, LoaQ: Layer-wise Output Approximation Quantization, https://arxiv.org/pdf/2509.06297, 2025-09-07
2025-11-09 20:12:20,159 - INFO - root - Page:1, Index:14, FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving, https://arxiv.org/pdf/2509.06261, 2025-09-14
2025-11-09 20:12:20,159 - INFO - root - Page:1, Index:15, Sensitivity-Aware Post-Training Quantization for Deep Neural Networks, https://arxiv.org/pdf/2509.05576, 2025-09-05
2025-11-09 20:12:20,159 - INFO - root - Page:1, Index:16, SuperSNN: A Hardware-Aware Framework for Physically Realizable, High-Performance Superconducting Spiking Neural Network Chips, https://arxiv.org/pdf/2509.05532, 2025-09-05
2025-11-09 20:12:20,160 - INFO - root - Page:1, Index:17, Data-Augmented Quantization-Aware Knowledge Distillation, https://arxiv.org/pdf/2509.03850, 2025-09-03
2025-11-09 20:12:20,161 - INFO - root - Page:1, Index:18, DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling, https://arxiv.org/pdf/2509.03472, 2025-09-03
2025-11-09 20:12:20,161 - INFO - root - Page:1, Index:19, Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs, https://arxiv.org/pdf/2509.02017, 2025-09-02
2025-11-09 20:12:20,162 - INFO - root - Page:1, Index:20, Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling, https://arxiv.org/pdf/2509.01624, 2025-09-01
2025-11-09 20:12:20,162 - INFO - root - Page:1, Index:21, Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective, https://arxiv.org/pdf/2509.00859, 2025-08-31
2025-11-09 20:12:20,163 - INFO - root - Page:1, Index:22, Progressive Element-wise Gradient Estimation for Neural Network Quantization, https://arxiv.org/pdf/2509.00097, 2025-08-27
2025-11-09 20:12:20,163 - INFO - root - Page:1, Index:23, End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost, https://arxiv.org/pdf/2509.00031, 2025-09-29
2025-11-09 20:12:20,163 - INFO - root - Page:1, Index:24, Quantization Robustness to Input Degradations for Object Detection, https://arxiv.org/pdf/2508.19600, 2025-08-27
2025-11-09 20:12:20,163 - INFO - root - Page:1, Index:25, Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models, https://arxiv.org/pdf/2508.18609, 2025-08-27
2025-11-09 20:12:20,164 - INFO - root - Page:1, Index:26, AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration, https://arxiv.org/pdf/2508.18025, 2025-08-25
2025-11-09 20:12:20,166 - INFO - root - Page:1, Index:27, TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling, https://arxiv.org/pdf/2508.16790, 2025-08-22
2025-11-09 20:12:20,166 - INFO - root - Page:1, Index:28, A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering, https://arxiv.org/pdf/2508.16516, 2025-08-22
2025-11-09 20:12:20,166 - INFO - root - Page:1, Index:29, JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs, https://arxiv.org/pdf/2508.15468, 2025-08-21
2025-11-09 20:12:20,166 - INFO - root - Page:1, Index:30, MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation, https://arxiv.org/pdf/2508.15281, 2025-08-21
2025-11-09 20:12:20,167 - INFO - root - Page:1, Index:31, DLLMQuant: Quantizing Diffusion-based Large Language Models, https://arxiv.org/pdf/2508.14090, 2025-08-25
2025-11-09 20:12:20,168 - INFO - root - Page:1, Index:32, Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management, https://arxiv.org/pdf/2508.13905, 2025-08-19
2025-11-09 20:12:20,172 - INFO - root - Page:1, Index:33, XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads, https://arxiv.org/pdf/2508.13049, 2025-08-18
2025-11-09 20:12:20,172 - INFO - root - Page:1, Index:34, Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection, https://arxiv.org/pdf/2508.12230, 2025-08-17
2025-11-09 20:12:20,172 - INFO - root - Page:1, Index:35, TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks, https://arxiv.org/pdf/2508.12132, 2025-08-16
2025-11-09 20:12:20,173 - INFO - root - Page:1, Index:36, Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion, https://arxiv.org/pdf/2508.12094, 2025-10-13
2025-11-09 20:12:20,173 - INFO - root - Page:1, Index:37, Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware, https://arxiv.org/pdf/2508.11940, 2025-08-16
2025-11-09 20:12:20,173 - INFO - root - Page:1, Index:38, PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks, https://arxiv.org/pdf/2508.10557, 2025-08-15
2025-11-09 20:12:20,173 - INFO - root - Page:1, Index:39, MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI, https://arxiv.org/pdf/2508.09500, 2025-08-13
2025-11-09 20:12:20,174 - INFO - root - Page:1, Index:40, SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via Codebooks for recommender system, https://arxiv.org/pdf/2508.09090, 2025-08-12
2025-11-09 20:12:20,174 - INFO - root - Page:1, Index:41, Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models, https://arxiv.org/pdf/2508.06974, 2025-08-09
2025-11-09 20:12:20,174 - INFO - root - Page:1, Index:42, Efficient Deep Neural Receiver with Post-Training Quantization, https://arxiv.org/pdf/2508.06275, 2025-08-08
2025-11-09 20:12:20,175 - INFO - root - Page:1, Index:43, iFairy: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$, https://arxiv.org/pdf/2508.05571, 2025-08-16
2025-11-09 20:12:20,175 - INFO - root - Page:1, Index:44, S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation, https://arxiv.org/pdf/2508.04016, 2025-10-27
2025-11-09 20:12:20,175 - INFO - root - Page:1, Index:45, LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation, https://arxiv.org/pdf/2508.03485, 2025-09-23
2025-11-09 20:12:20,176 - INFO - root - Page:1, Index:46, VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation, https://arxiv.org/pdf/2508.03351, 2025-08-05
2025-11-09 20:12:20,176 - INFO - root - Page:1, Index:47, TF-MLPNet: Tiny Real-Time Neural Speech Separation, https://arxiv.org/pdf/2508.03047, 2025-08-04
2025-11-09 20:12:20,177 - INFO - root - Page:1, Index:48, SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation, https://arxiv.org/pdf/2508.01375, 2025-08-02
2025-11-09 20:12:20,177 - INFO - root - Page:1, Index:49, MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective, https://arxiv.org/pdf/2507.19131, 2025-07-25
2025-11-09 20:12:20,177 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 20:12:26,765 - INFO - root - get_all_titles_from_web 
2025-11-09 20:12:26,765 - INFO - root - Page:2, Index:0, Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction, https://arxiv.org/pdf/2507.17768, 2025-07-16
2025-11-09 20:12:26,766 - INFO - root - Page:2, Index:1, SiLQ: Simple Large Language Model Quantization-Aware Training, https://arxiv.org/pdf/2507.16933, 2025-07-22
2025-11-09 20:12:26,770 - INFO - root - Page:2, Index:2, Task-Specific Zero-shot Quantization-Aware Training for Object Detection, https://arxiv.org/pdf/2507.16782, 2025-07-22
2025-11-09 20:12:26,779 - INFO - root - Page:2, Index:3, TorchAO: PyTorch-Native Training-to-Serving Model Optimization, https://arxiv.org/pdf/2507.16099, 2025-07-21
2025-11-09 20:12:26,783 - INFO - root - Page:2, Index:4, SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models, https://arxiv.org/pdf/2507.14811, 2025-08-27
2025-11-09 20:12:26,799 - INFO - root - Page:2, Index:5, Apple Intelligence Foundation Language Models: Tech Report 2025, https://arxiv.org/pdf/2507.13575, 2025-08-27
2025-11-09 20:12:26,813 - INFO - root - Page:2, Index:6, Generative Multi-Target Cross-Domain Recommendation, https://arxiv.org/pdf/2507.12871, 2025-08-07
2025-11-09 20:12:26,816 - INFO - root - Page:2, Index:7, DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training, https://arxiv.org/pdf/2507.07149, 2025-07-09
2025-11-09 20:12:26,828 - INFO - root - Page:2, Index:8, Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics, https://arxiv.org/pdf/2507.07044, 2025-07-15
2025-11-09 20:12:26,832 - INFO - root - Page:2, Index:9, QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models, https://arxiv.org/pdf/2507.06079, 2025-07-08
2025-11-09 20:12:26,836 - INFO - root - Page:2, Index:10, GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field, https://arxiv.org/pdf/2507.05594, 2025-07-07
2025-11-09 20:12:26,840 - INFO - root - Page:2, Index:11, Hita: Holistic Tokenizer for Autoregressive Image Generation, https://arxiv.org/pdf/2507.02358, 2025-07-11
2025-11-09 20:12:26,845 - INFO - root - Page:2, Index:12, QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference, https://arxiv.org/pdf/2506.23934, 2025-06-30
2025-11-09 20:12:26,849 - INFO - root - Page:2, Index:13, FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization, https://arxiv.org/pdf/2506.23516, 2025-07-21
2025-11-09 20:12:26,857 - INFO - root - Page:2, Index:14, Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models, https://arxiv.org/pdf/2506.23025, 2025-06-28
2025-11-09 20:12:26,860 - INFO - root - Page:2, Index:15, Joint Quantization and Pruning Neural Networks Approach: A Case Study on FSO Receivers, https://arxiv.org/pdf/2506.20084, 2025-06-24
2025-11-09 20:12:26,866 - INFO - root - Page:2, Index:16, Single-step Diffusion for Image Compression at Ultra-Low Bitrates, https://arxiv.org/pdf/2506.16572, 2025-09-22
2025-11-09 20:12:26,871 - INFO - root - Page:2, Index:17, LittleBit: Ultra Low-Bit Quantization via Latent Factorization, https://arxiv.org/pdf/2506.13771, 2025-10-28
2025-11-09 20:12:26,876 - INFO - root - Page:2, Index:18, MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering, https://arxiv.org/pdf/2506.13755, 2025-06-16
2025-11-09 20:12:26,879 - INFO - root - Page:2, Index:19, EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization, https://arxiv.org/pdf/2506.13329, 2025-07-04
2025-11-09 20:12:26,885 - INFO - root - Page:2, Index:20, Towards Neural Audio Codec Source Parsing, https://arxiv.org/pdf/2506.12627, 2025-06-14
2025-11-09 20:12:26,886 - INFO - root - Page:2, Index:21, Quantizing Small-Scale State-Space Models for Edge AI, https://arxiv.org/pdf/2506.12480, 2025-06-14
2025-11-09 20:12:26,892 - INFO - root - Page:2, Index:22, EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction, https://arxiv.org/pdf/2506.12015, 2025-06-13
2025-11-09 20:12:26,898 - INFO - root - Page:2, Index:23, Compression Aware Certified Training, https://arxiv.org/pdf/2506.11992, 2025-06-13
2025-11-09 20:12:26,902 - INFO - root - Page:2, Index:24, GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers, https://arxiv.org/pdf/2506.11784, 2025-06-13
2025-11-09 20:12:26,907 - INFO - root - Page:2, Index:25, TruncQuant: Truncation-Ready Quantization for DNNs with Flexible Weight Bit Precision, https://arxiv.org/pdf/2506.11431, 2025-06-12
2025-11-09 20:12:26,911 - INFO - root - Page:2, Index:26, EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices, https://arxiv.org/pdf/2506.11093, 2025-06-05
2025-11-09 20:12:26,916 - INFO - root - Page:2, Index:27, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 20:12:26,918 - INFO - root - Page:2, Index:28, AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent, https://arxiv.org/pdf/2506.10205, 2025-06-11
2025-11-09 20:12:26,924 - INFO - root - Page:2, Index:29, Q-SAM2: Accurate Quantization for Segment Anything Model 2, https://arxiv.org/pdf/2506.09782, 2025-06-11
2025-11-09 20:12:26,926 - INFO - root - Page:2, Index:30, Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs, https://arxiv.org/pdf/2506.09104, 2025-06-10
2025-11-09 20:12:26,934 - INFO - root - Page:2, Index:31, Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU, https://arxiv.org/pdf/2506.08911, 2025-06-10
2025-11-09 20:12:26,945 - INFO - root - Page:2, Index:32, POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration, https://arxiv.org/pdf/2506.08785, 2025-06-10
2025-11-09 20:12:26,987 - INFO - root - Page:2, Index:33, MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts, https://arxiv.org/pdf/2506.07533, 2025-06-09
2025-11-09 20:12:27,010 - INFO - root - Page:2, Index:34, BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation, https://arxiv.org/pdf/2506.07530, 2025-06-09
2025-11-09 20:12:27,033 - INFO - root - Page:2, Index:35, RecGPT: A Foundation Model for Sequential Recommendation, https://arxiv.org/pdf/2506.06270, 2025-06-12
2025-11-09 20:12:27,045 - INFO - root - Page:2, Index:36, FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion, https://arxiv.org/pdf/2506.04648, 2025-06-05
2025-11-09 20:12:27,052 - INFO - root - Page:2, Index:37, BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing, https://arxiv.org/pdf/2506.03515, 2025-06-03
2025-11-09 20:12:27,060 - INFO - root - Page:2, Index:38, ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding, https://arxiv.org/pdf/2506.01853, 2025-06-02
2025-11-09 20:12:27,062 - INFO - root - Page:2, Index:39, Movable Antenna Enhanced Federated Fine-Tuning of Large Language Models via Hybrid Client Selection Optimization, https://arxiv.org/pdf/2506.00011, 2025-10-26
2025-11-09 20:12:27,063 - INFO - root - Page:2, Index:40, Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach, https://arxiv.org/pdf/2505.24721, 2025-05-30
2025-11-09 20:12:27,063 - INFO - root - Page:2, Index:41, GARLIC: GAussian Representation LearnIng for spaCe partitioning, https://arxiv.org/pdf/2505.24608, 2025-10-02
2025-11-09 20:12:27,064 - INFO - root - Page:2, Index:42, AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up, https://arxiv.org/pdf/2505.24584, 2025-08-18
2025-11-09 20:12:27,065 - INFO - root - Page:2, Index:43, Model-Preserving Adaptive Rounding, https://arxiv.org/pdf/2505.22988, 2025-09-25
2025-11-09 20:12:27,066 - INFO - root - Page:2, Index:44, Highly Efficient and Effective LLMs with Multi-Boolean Architectures, https://arxiv.org/pdf/2505.22811, 2025-10-03
2025-11-09 20:12:27,066 - INFO - root - Page:2, Index:45, Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning, https://arxiv.org/pdf/2505.21591, 2025-05-27
2025-11-09 20:12:27,066 - INFO - root - Page:2, Index:46, Frequency Composition for Compressed and Domain-Adaptive Neural Networks, https://arxiv.org/pdf/2505.20890, 2025-05-27
2025-11-09 20:12:27,067 - INFO - root - Page:2, Index:47, MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition, https://arxiv.org/pdf/2505.20744, 2025-10-27
2025-11-09 20:12:27,067 - INFO - root - Page:2, Index:48, Optimizing edge AI models on HPC systems with the edge in the loop, https://arxiv.org/pdf/2505.19995, 2025-05-26
2025-11-09 20:12:27,067 - INFO - root - Fetching page 4 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=150
2025-11-09 20:12:34,212 - INFO - root - get_all_titles_from_web 
2025-11-09 20:12:34,213 - INFO - root - Page:3, Index:0, DPASyn: Mechanism-Aware Drug Synergy Prediction via Dual Attention and Precision-Aware Quantization, https://arxiv.org/pdf/2505.19144, 2025-09-20
2025-11-09 20:12:34,214 - INFO - root - Page:3, Index:1, Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding, https://arxiv.org/pdf/2505.18758, 2025-05-24
2025-11-09 20:12:34,215 - INFO - root - Page:3, Index:2, Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization, https://arxiv.org/pdf/2505.18113, 2025-05-23
2025-11-09 20:12:34,215 - INFO - root - Page:3, Index:3, Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs, https://arxiv.org/pdf/2505.17662, 2025-09-19
2025-11-09 20:12:34,215 - INFO - root - Page:3, Index:4, FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design, https://arxiv.org/pdf/2505.16335, 2025-05-22
2025-11-09 20:12:34,216 - INFO - root - Page:3, Index:5, Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control, https://arxiv.org/pdf/2505.15304, 2025-05-30
2025-11-09 20:12:34,216 - INFO - root - Page:3, Index:6, Scaling Law for Quantization-Aware Training, https://arxiv.org/pdf/2505.14302, 2025-05-20
2025-11-09 20:12:34,216 - INFO - root - Page:3, Index:7, Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs, https://arxiv.org/pdf/2505.13060, 2025-05-19
2025-11-09 20:12:34,216 - INFO - root - Page:3, Index:8, Deep Unfolding with Kernel-based Quantization in MIMO Detection, https://arxiv.org/pdf/2505.12736, 2025-05-19
2025-11-09 20:12:34,217 - INFO - root - Page:3, Index:9, FedHQ: Hybrid Runtime Quantization for Federated Learning, https://arxiv.org/pdf/2505.11982, 2025-05-17
2025-11-09 20:12:34,217 - INFO - root - Page:3, Index:10, QVGen: Pushing the Limit of Quantized Video Generative Models, https://arxiv.org/pdf/2505.11497, 2025-09-27
2025-11-09 20:12:34,218 - INFO - root - Page:3, Index:11, EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes, https://arxiv.org/pdf/2505.10787, 2025-05-15
2025-11-09 20:12:34,219 - INFO - root - Page:3, Index:12, ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition, https://arxiv.org/pdf/2505.08981, 2025-05-13
2025-11-09 20:12:34,220 - INFO - root - Page:3, Index:13, PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs, https://arxiv.org/pdf/2505.03254, 2025-08-06
2025-11-09 20:12:34,220 - INFO - root - Page:3, Index:14, ICQuant: Index Coding enables Low-bit LLM Quantization, https://arxiv.org/pdf/2505.00850, 2025-08-24
2025-11-09 20:12:34,222 - INFO - root - Page:3, Index:15, Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining, https://arxiv.org/pdf/2504.13932, 2025-07-30
2025-11-09 20:12:34,223 - INFO - root - Page:3, Index:16, Achieving binary weight and activation for LLMs using Post-Training Quantization, https://arxiv.org/pdf/2504.05352, 2025-06-30
2025-11-09 20:12:34,223 - INFO - root - Page:3, Index:17, Wireless Hearables With Programmable Speech AI Accelerators, https://arxiv.org/pdf/2503.18698, 2025-10-21
2025-11-09 20:12:34,224 - INFO - root - Page:3, Index:18, GranQ: Efficient Channel-wise Quantization via Vectorized Pre-Scaling for Zero-Shot QAT, https://arxiv.org/pdf/2503.18339, 2025-10-15
2025-11-09 20:12:34,224 - INFO - root - Page:3, Index:19, CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting, https://arxiv.org/pdf/2503.12836, 2025-09-29
2025-11-09 20:12:34,226 - INFO - root - Page:3, Index:20, AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model, https://arxiv.org/pdf/2503.03088, 2025-07-11
2025-11-09 20:12:34,226 - INFO - root - Page:3, Index:21, Teaching Metric Distance to Discrete Autoregressive Language Models, https://arxiv.org/pdf/2503.02379, 2025-10-07
2025-11-09 20:12:34,227 - INFO - root - Page:3, Index:22, Regularization-based Framework for Quantization-, Fault- and Variability-Aware Training, https://arxiv.org/pdf/2503.01297, 2025-07-12
2025-11-09 20:12:34,228 - INFO - root - Fetching page 5 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=200
2025-11-09 20:12:40,773 - INFO - root - get_all_titles_from_web 
2025-11-09 20:12:40,773 - INFO - root - Page:4, Index:0, Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models, https://arxiv.org/pdf/2502.15799, 2025-06-29
2025-11-09 20:12:40,774 - INFO - root - Page:4, Index:1, Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer, https://arxiv.org/pdf/2502.15779, 2025-09-01
2025-11-09 20:12:40,774 - INFO - root - Page:4, Index:2, RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models, https://arxiv.org/pdf/2502.09003, 2025-06-05
2025-11-09 20:12:40,774 - INFO - root - Page:4, Index:3, Spatial Degradation-Aware and Temporal Consistent Diffusion Model for Compressed Video Super-Resolution, https://arxiv.org/pdf/2502.07381, 2025-06-27
2025-11-09 20:12:40,774 - INFO - root - Page:4, Index:4, QuEST: Stable Training of LLMs with 1-Bit Weights and Activations, https://arxiv.org/pdf/2502.05003, 2025-06-10
2025-11-09 20:12:40,774 - INFO - root - Page:4, Index:5, HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs, https://arxiv.org/pdf/2501.02625, 2025-11-05
2025-11-09 20:12:40,775 - INFO - root - Page:4, Index:6, TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction, https://arxiv.org/pdf/2412.16919, 2025-08-08
2025-11-09 20:12:40,775 - INFO - root - Page:4, Index:7, Training Multi-Layer Binary Neural Networks With Local Binary Error Signals, https://arxiv.org/pdf/2412.00119, 2025-08-05
2025-11-09 20:12:40,775 - INFO - root - Page:4, Index:8, Scaling Large-scale GNN Training to Thousands of Processors on CPU-based Supercomputers, https://arxiv.org/pdf/2411.16025, 2025-05-26
2025-11-09 20:12:40,775 - INFO - root - Page:4, Index:9, HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization, https://arxiv.org/pdf/2411.06581, 2025-05-16
2025-11-09 20:12:40,776 - INFO - root - Fetching page 6 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=250
2025-11-09 20:12:47,553 - INFO - root - get_all_titles_from_web 
2025-11-09 20:12:47,553 - INFO - root - Page:5, Index:0, Gradient-Free Training of Quantized Neural Networks, https://arxiv.org/pdf/2410.09734, 2025-09-29
2025-11-09 20:12:47,554 - INFO - root - Page:5, Index:1, QT-DoG: Quantization-aware Training for Domain Generalization, https://arxiv.org/pdf/2410.06020, 2025-06-26
2025-11-09 20:12:47,554 - INFO - root - Page:5, Index:2, Constraint Guided Model Quantization of Neural Networks, https://arxiv.org/pdf/2409.20138, 2025-09-12
2025-11-09 20:12:47,556 - INFO - root - Page:5, Index:3, Accumulator-Aware Post-Training Quantization for Large Language Models, https://arxiv.org/pdf/2409.17092, 2025-07-30
2025-11-09 20:12:47,556 - INFO - root - Page:5, Index:4, DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation, https://arxiv.org/pdf/2409.14307, 2025-07-09
2025-11-09 20:12:47,556 - INFO - root - Page:5, Index:5, Robust Training of Neural Networks at Arbitrary Precision and Sparsity, https://arxiv.org/pdf/2409.09245, 2025-09-23
2025-11-09 20:12:47,557 - INFO - root - Page:5, Index:6, VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing, https://arxiv.org/pdf/2408.05758, 2025-05-27
2025-11-09 20:12:47,557 - INFO - root - Page:5, Index:7, DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers, https://arxiv.org/pdf/2408.03291, 2025-06-19
2025-11-09 20:12:47,558 - INFO - root - Page:5, Index:8, Temporal Feature Matters: A Framework for Diffusion Model Quantization, https://arxiv.org/pdf/2407.19547, 2025-07-11
2025-11-09 20:12:47,560 - INFO - root - Page:5, Index:9, EfficientQAT: Efficient Quantization-Aware Training for Large Language Models, https://arxiv.org/pdf/2407.11062, 2025-05-19
2025-11-09 20:12:47,563 - INFO - root - Page:5, Index:10, Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT, https://arxiv.org/pdf/2407.11041, 2025-10-31
2025-11-09 20:12:47,566 - INFO - root - Fetching page 7 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=300
2025-11-09 20:12:54,121 - INFO - root - get_all_titles_from_web 
2025-11-09 20:12:54,122 - INFO - root - Page:6, Index:0, BoA: Attention-aware Post-training Quantization without Backpropagation, https://arxiv.org/pdf/2406.13474, 2025-06-06
2025-11-09 20:12:54,123 - INFO - root - Page:6, Index:1, A Rescaling-Invariant Lipschitz Bound Based on Path-Metrics for Modern ReLU Network Parameterizations, https://arxiv.org/pdf/2405.15006, 2025-06-13
2025-11-09 20:12:54,123 - INFO - root - Page:6, Index:2, Scheduling Weight Transitions for Quantization-Aware Training, https://arxiv.org/pdf/2404.19248, 2025-10-01
2025-11-09 20:12:54,123 - INFO - root - Page:6, Index:3, GPTVQ: The Blessing of Dimensionality for LLM Quantization, https://arxiv.org/pdf/2402.15319, 2025-06-03
2025-11-09 20:12:54,123 - INFO - root - Fetching page 8 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=350
2025-11-09 20:13:00,649 - INFO - root - get_all_titles_from_web 
2025-11-09 20:13:00,649 - INFO - root - Page:7, Index:0, Squat: Quant Small Language Models on the Edge, https://arxiv.org/pdf/2402.10787, 2025-07-01
2025-11-09 20:13:00,649 - INFO - root - Page:7, Index:1, L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2402.04902, 2025-07-21
2025-11-09 20:13:00,650 - INFO - root - Fetching page 9 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=400
2025-11-09 20:13:07,499 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 20:13:21,213 - INFO - root - 跳过已处理论文 A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies：D:\ChatPaper\academic Papers\Quantization-Aware-Training\A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat-4.pdf
2025-11-09 20:13:21,214 - INFO - root - 跳过已处理论文 FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error：D:\ChatPaper\academic Papers\Quantization-Aware-Training\FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error-4.pdf
2025-11-09 20:13:21,215 - INFO - root - summary time: 80.50 seconds
2025-11-09 20:14:13,113 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 20:14:13,114 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 20:14:13,116 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 20:14:14,181 - INFO - root - LLMClientManager: 指定使用客户端: Gemini
2025-11-09 20:14:15,286 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 20:14:18,313 - INFO - root - GeminiClient: initialized model gemini-2.5-flash
2025-11-09 20:14:18,313 - INFO - root - LLMClientManager: Gemini 客户端初始化成功
2025-11-09 20:14:18,314 - INFO - root - LLMClientManager: switched to Gemini client
2025-11-09 20:14:18,315 - INFO - root - 已手动切换到 LLM 客户端: Gemini
2025-11-09 20:14:18,315 - INFO - root - 使用 LLM 模型: gemini-2.5-flash
2025-11-09 20:14:18,315 - INFO - root - 可用客户端: ['Gemini']
2025-11-09 20:14:18,316 - INFO - root - === 运行配置 ===
2025-11-09 20:14:18,317 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 20:14:18,317 - INFO - root - 关键词: QAT
2025-11-09 20:14:18,318 - INFO - root - 查询: Quantization-Aware-Training
2025-11-09 20:14:18,318 - INFO - root - 排序: None
2025-11-09 20:14:18,319 - INFO - root - 最近天数: 180
2025-11-09 20:14:18,320 - INFO - root - 最大处理数量: 2
2025-11-09 20:14:18,320 - INFO - root - 保存图片: 是
2025-11-09 20:14:18,321 - INFO - root - 输出语言: 中文
2025-11-09 20:14:18,321 - INFO - root - 强制重新处理: 否
2025-11-09 20:14:18,321 - INFO - root - ====================
2025-11-09 20:14:18,321 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 20:14:18,321 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 20:14:25,132 - INFO - root - get_all_titles_from_web 
2025-11-09 20:14:25,133 - INFO - root - Page:0, Index:0, A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies, https://arxiv.org/pdf/2511.03201, 2025-11-05
2025-11-09 20:14:25,133 - INFO - root - Page:0, Index:1, FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error, https://arxiv.org/pdf/2511.02302, 2025-11-04
2025-11-09 20:14:25,133 - INFO - root - Page:0, Index:2, Outlier-Aware Post-Training Quantization for Image Super-Resolution, https://arxiv.org/pdf/2511.00682, 2025-11-01
2025-11-09 20:14:25,133 - INFO - root - Page:0, Index:3, Evaluation of Wafer-Scale SOT-MRAM for Analog Crossbar Array Applications, https://arxiv.org/pdf/2510.25853, 2025-11-03
2025-11-09 20:14:25,134 - INFO - root - Page:0, Index:4, Improving the Straight-Through Estimator with Zeroth-Order Information, https://arxiv.org/pdf/2510.23926, 2025-10-27
2025-11-09 20:14:25,134 - INFO - root - Page:0, Index:5, Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework, https://arxiv.org/pdf/2510.22243, 2025-10-25
2025-11-09 20:14:25,134 - INFO - root - Page:0, Index:6, TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge, https://arxiv.org/pdf/2510.21879, 2025-10-23
2025-11-09 20:14:25,134 - INFO - root - Page:0, Index:7, KARIPAP: Quantum-Inspired Tensor Network Compression of Large Language Models Using Infinite Projected Entangled Pair States and Tensor Renormalization Group, https://arxiv.org/pdf/2510.21844, 2025-10-22
2025-11-09 20:14:25,136 - INFO - root - Page:0, Index:8, A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization, https://arxiv.org/pdf/2510.21314, 2025-10-24
2025-11-09 20:14:25,136 - INFO - root - Page:0, Index:9, Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks, https://arxiv.org/pdf/2510.19760, 2025-10-22
2025-11-09 20:14:25,136 - INFO - root - Page:0, Index:10, CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training, https://arxiv.org/pdf/2510.18784, 2025-10-21
2025-11-09 20:14:25,136 - INFO - root - Page:0, Index:11, Mixed-Precision Quantization for Language Models: Techniques and Prospects, https://arxiv.org/pdf/2510.16805, 2025-10-19
2025-11-09 20:14:25,136 - INFO - root - Page:0, Index:12, SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation, https://arxiv.org/pdf/2510.16396, 2025-10-30
2025-11-09 20:14:25,136 - INFO - root - Page:0, Index:13, CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models, https://arxiv.org/pdf/2510.15962, 2025-10-11
2025-11-09 20:14:25,136 - INFO - root - Page:0, Index:14, SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization, https://arxiv.org/pdf/2510.15775, 2025-10-17
2025-11-09 20:14:25,138 - INFO - root - Page:0, Index:15, SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware, https://arxiv.org/pdf/2510.15542, 2025-10-31
2025-11-09 20:14:25,138 - INFO - root - Page:0, Index:16, GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework, https://arxiv.org/pdf/2510.15299, 2025-10-17
2025-11-09 20:14:25,142 - INFO - root - Page:0, Index:17, SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images, https://arxiv.org/pdf/2510.15072, 2025-10-16
2025-11-09 20:14:25,144 - INFO - root - Page:0, Index:18, FraQAT: Quantization Aware Training with Fractional bits, https://arxiv.org/pdf/2510.14823, 2025-10-16
2025-11-09 20:14:25,144 - INFO - root - Page:0, Index:19, Computing-In-Memory Aware Model Adaption For Edge Devices, https://arxiv.org/pdf/2510.14379, 2025-10-16
2025-11-09 20:14:25,144 - INFO - root - Page:0, Index:20, Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge, https://arxiv.org/pdf/2510.13760, 2025-11-04
2025-11-09 20:14:25,145 - INFO - root - Page:0, Index:21, NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models, https://arxiv.org/pdf/2510.13068, 2025-10-14
2025-11-09 20:14:25,145 - INFO - root - Page:0, Index:22, Detect Anything via Next Point Prediction, https://arxiv.org/pdf/2510.12798, 2025-10-14
2025-11-09 20:14:25,146 - INFO - root - Page:0, Index:23, AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model, https://arxiv.org/pdf/2510.11496, 2025-10-14
2025-11-09 20:14:25,146 - INFO - root - Page:0, Index:24, Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware, https://arxiv.org/pdf/2510.11484, 2025-10-13
2025-11-09 20:14:25,146 - INFO - root - Page:0, Index:25, SASER: Stego attacks on open-source LLMs, https://arxiv.org/pdf/2510.10486, 2025-10-12
2025-11-09 20:14:25,146 - INFO - root - Page:0, Index:26, Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition, https://arxiv.org/pdf/2510.09653, 2025-10-15
2025-11-09 20:14:25,147 - INFO - root - Page:0, Index:27, A Theoretically-Grounded Codebook for Digital Semantic Communications, https://arxiv.org/pdf/2510.07108, 2025-10-14
2025-11-09 20:14:25,147 - INFO - root - Page:0, Index:28, QuantDemoire: Quantization with Outlier Aware for Image Demoiréing, https://arxiv.org/pdf/2510.04066, 2025-10-05
2025-11-09 20:14:25,147 - INFO - root - Page:0, Index:29, Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization, https://arxiv.org/pdf/2510.03763, 2025-10-04
2025-11-09 20:14:25,148 - INFO - root - Page:0, Index:30, Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models, https://arxiv.org/pdf/2510.03274, 2025-09-27
2025-11-09 20:14:25,148 - INFO - root - Page:0, Index:31, PT$^2$-LLM: Post-Training Ternarization for Large Language Models, https://arxiv.org/pdf/2510.03267, 2025-09-26
2025-11-09 20:14:25,148 - INFO - root - Page:0, Index:32, Purrception: Variational Flow Matching for Vector-Quantized Image Generation, https://arxiv.org/pdf/2510.01478, 2025-10-01
2025-11-09 20:14:25,148 - INFO - root - Page:0, Index:33, Post-Training Quantization for Audio Diffusion Transformers, https://arxiv.org/pdf/2510.00313, 2025-09-30
2025-11-09 20:14:25,148 - INFO - root - Page:0, Index:34, Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling, https://arxiv.org/pdf/2510.00028, 2025-09-25
2025-11-09 20:14:25,148 - INFO - root - Page:0, Index:35, Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models, https://arxiv.org/pdf/2509.26436, 2025-09-30
2025-11-09 20:14:25,149 - INFO - root - Page:0, Index:36, Cat: Post-Training Quantization Error Reduction via Cluster-based Affine Transformation, https://arxiv.org/pdf/2509.26277, 2025-10-07
2025-11-09 20:14:25,149 - INFO - root - Page:0, Index:37, CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models, https://arxiv.org/pdf/2509.25996, 2025-09-30
2025-11-09 20:14:25,149 - INFO - root - Page:0, Index:38, Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications, https://arxiv.org/pdf/2509.25439, 2025-09-29
2025-11-09 20:14:25,149 - INFO - root - Page:0, Index:39, On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs, https://arxiv.org/pdf/2509.25214, 2025-09-22
2025-11-09 20:14:25,149 - INFO - root - Page:0, Index:40, VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning, https://arxiv.org/pdf/2509.24650, 2025-09-29
2025-11-09 20:14:25,150 - INFO - root - Page:0, Index:41, S$^2$NN: Sub-bit Spiking Neural Networks, https://arxiv.org/pdf/2509.24266, 2025-10-24
2025-11-09 20:14:25,150 - INFO - root - Page:0, Index:42, Tequila: Trapping-free Ternary Quantization for Large Language Models, https://arxiv.org/pdf/2509.23809, 2025-10-17
2025-11-09 20:14:25,150 - INFO - root - Page:0, Index:43, Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution, https://arxiv.org/pdf/2509.23774, 2025-09-30
2025-11-09 20:14:25,150 - INFO - root - Page:0, Index:44, RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization, https://arxiv.org/pdf/2509.23582, 2025-09-27
2025-11-09 20:14:25,150 - INFO - root - Page:0, Index:45, Beyond Outliers: A Study of Optimizers Under Quantization, https://arxiv.org/pdf/2509.23500, 2025-10-02
2025-11-09 20:14:25,150 - INFO - root - Page:0, Index:46, Compute-Optimal Quantization-Aware Training, https://arxiv.org/pdf/2509.22935, 2025-09-26
2025-11-09 20:14:25,151 - INFO - root - Page:0, Index:47, COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning, https://arxiv.org/pdf/2509.22075, 2025-10-06
2025-11-09 20:14:25,151 - INFO - root - Page:0, Index:48, SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models, https://arxiv.org/pdf/2509.21498, 2025-09-25
2025-11-09 20:14:25,151 - INFO - root - Page:0, Index:49, Quantized Visual Geometry Grounded Transformer, https://arxiv.org/pdf/2509.21302, 2025-09-29
2025-11-09 20:14:25,151 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 20:14:31,601 - INFO - root - get_all_titles_from_web 
2025-11-09 20:14:31,601 - INFO - root - Page:1, Index:0, Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy, https://arxiv.org/pdf/2509.21173, 2025-10-27
2025-11-09 20:14:31,601 - INFO - root - Page:1, Index:1, Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer, https://arxiv.org/pdf/2509.20854, 2025-09-25
2025-11-09 20:14:31,603 - INFO - root - Page:1, Index:2, TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection, https://arxiv.org/pdf/2509.18193, 2025-09-19
2025-11-09 20:14:31,603 - INFO - root - Page:1, Index:3, SBVR: Summation of BitVector Representation for Efficient LLM Quantization, https://arxiv.org/pdf/2509.18172, 2025-09-17
2025-11-09 20:14:31,603 - INFO - root - Page:1, Index:4, QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2509.17428, 2025-09-26
2025-11-09 20:14:31,603 - INFO - root - Page:1, Index:5, PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models, https://arxiv.org/pdf/2509.16989, 2025-10-28
2025-11-09 20:14:31,604 - INFO - root - Page:1, Index:6, MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training, https://arxiv.org/pdf/2509.15514, 2025-09-18
2025-11-09 20:14:31,604 - INFO - root - Page:1, Index:7, Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs, https://arxiv.org/pdf/2509.14391, 2025-09-17
2025-11-09 20:14:31,604 - INFO - root - Page:1, Index:8, Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization, https://arxiv.org/pdf/2509.13786, 2025-09-19
2025-11-09 20:14:31,605 - INFO - root - Page:1, Index:9, Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees, https://arxiv.org/pdf/2509.11054, 2025-09-13
2025-11-09 20:14:31,605 - INFO - root - Page:1, Index:10, SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models, https://arxiv.org/pdf/2509.09090, 2025-09-10
2025-11-09 20:14:31,605 - INFO - root - Page:1, Index:11, CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks with Entropy Regularization, https://arxiv.org/pdf/2509.08776, 2025-09-10
2025-11-09 20:14:31,606 - INFO - root - Page:1, Index:12, Explaining How Quantization Disparately Skews a Model, https://arxiv.org/pdf/2509.07222, 2025-09-08
2025-11-09 20:14:31,606 - INFO - root - Page:1, Index:13, LoaQ: Layer-wise Output Approximation Quantization, https://arxiv.org/pdf/2509.06297, 2025-09-07
2025-11-09 20:14:31,606 - INFO - root - Page:1, Index:14, FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving, https://arxiv.org/pdf/2509.06261, 2025-09-14
2025-11-09 20:14:31,606 - INFO - root - Page:1, Index:15, Sensitivity-Aware Post-Training Quantization for Deep Neural Networks, https://arxiv.org/pdf/2509.05576, 2025-09-05
2025-11-09 20:14:31,607 - INFO - root - Page:1, Index:16, SuperSNN: A Hardware-Aware Framework for Physically Realizable, High-Performance Superconducting Spiking Neural Network Chips, https://arxiv.org/pdf/2509.05532, 2025-09-05
2025-11-09 20:14:31,607 - INFO - root - Page:1, Index:17, Data-Augmented Quantization-Aware Knowledge Distillation, https://arxiv.org/pdf/2509.03850, 2025-09-03
2025-11-09 20:14:31,608 - INFO - root - Page:1, Index:18, DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling, https://arxiv.org/pdf/2509.03472, 2025-09-03
2025-11-09 20:14:31,608 - INFO - root - Page:1, Index:19, Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs, https://arxiv.org/pdf/2509.02017, 2025-09-02
2025-11-09 20:14:31,608 - INFO - root - Page:1, Index:20, Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling, https://arxiv.org/pdf/2509.01624, 2025-09-01
2025-11-09 20:14:31,608 - INFO - root - Page:1, Index:21, Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective, https://arxiv.org/pdf/2509.00859, 2025-08-31
2025-11-09 20:14:31,609 - INFO - root - Page:1, Index:22, Progressive Element-wise Gradient Estimation for Neural Network Quantization, https://arxiv.org/pdf/2509.00097, 2025-08-27
2025-11-09 20:14:31,609 - INFO - root - Page:1, Index:23, End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost, https://arxiv.org/pdf/2509.00031, 2025-09-29
2025-11-09 20:14:31,609 - INFO - root - Page:1, Index:24, Quantization Robustness to Input Degradations for Object Detection, https://arxiv.org/pdf/2508.19600, 2025-08-27
2025-11-09 20:14:31,610 - INFO - root - Page:1, Index:25, Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models, https://arxiv.org/pdf/2508.18609, 2025-08-27
2025-11-09 20:14:31,610 - INFO - root - Page:1, Index:26, AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration, https://arxiv.org/pdf/2508.18025, 2025-08-25
2025-11-09 20:14:31,610 - INFO - root - Page:1, Index:27, TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling, https://arxiv.org/pdf/2508.16790, 2025-08-22
2025-11-09 20:14:31,611 - INFO - root - Page:1, Index:28, A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering, https://arxiv.org/pdf/2508.16516, 2025-08-22
2025-11-09 20:14:31,611 - INFO - root - Page:1, Index:29, JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs, https://arxiv.org/pdf/2508.15468, 2025-08-21
2025-11-09 20:14:31,611 - INFO - root - Page:1, Index:30, MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation, https://arxiv.org/pdf/2508.15281, 2025-08-21
2025-11-09 20:14:31,611 - INFO - root - Page:1, Index:31, DLLMQuant: Quantizing Diffusion-based Large Language Models, https://arxiv.org/pdf/2508.14090, 2025-08-25
2025-11-09 20:14:31,613 - INFO - root - Page:1, Index:32, Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management, https://arxiv.org/pdf/2508.13905, 2025-08-19
2025-11-09 20:14:31,614 - INFO - root - Page:1, Index:33, XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads, https://arxiv.org/pdf/2508.13049, 2025-08-18
2025-11-09 20:14:31,616 - INFO - root - Page:1, Index:34, Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection, https://arxiv.org/pdf/2508.12230, 2025-08-17
2025-11-09 20:14:31,618 - INFO - root - Page:1, Index:35, TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks, https://arxiv.org/pdf/2508.12132, 2025-08-16
2025-11-09 20:14:31,620 - INFO - root - Page:1, Index:36, Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion, https://arxiv.org/pdf/2508.12094, 2025-10-13
2025-11-09 20:14:31,620 - INFO - root - Page:1, Index:37, Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware, https://arxiv.org/pdf/2508.11940, 2025-08-16
2025-11-09 20:14:31,620 - INFO - root - Page:1, Index:38, PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks, https://arxiv.org/pdf/2508.10557, 2025-08-15
2025-11-09 20:14:31,620 - INFO - root - Page:1, Index:39, MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI, https://arxiv.org/pdf/2508.09500, 2025-08-13
2025-11-09 20:14:31,622 - INFO - root - Page:1, Index:40, SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via Codebooks for recommender system, https://arxiv.org/pdf/2508.09090, 2025-08-12
2025-11-09 20:14:31,622 - INFO - root - Page:1, Index:41, Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models, https://arxiv.org/pdf/2508.06974, 2025-08-09
2025-11-09 20:14:31,622 - INFO - root - Page:1, Index:42, Efficient Deep Neural Receiver with Post-Training Quantization, https://arxiv.org/pdf/2508.06275, 2025-08-08
2025-11-09 20:14:31,623 - INFO - root - Page:1, Index:43, iFairy: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$, https://arxiv.org/pdf/2508.05571, 2025-08-16
2025-11-09 20:14:31,623 - INFO - root - Page:1, Index:44, S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation, https://arxiv.org/pdf/2508.04016, 2025-10-27
2025-11-09 20:14:31,623 - INFO - root - Page:1, Index:45, LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation, https://arxiv.org/pdf/2508.03485, 2025-09-23
2025-11-09 20:14:31,624 - INFO - root - Page:1, Index:46, VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation, https://arxiv.org/pdf/2508.03351, 2025-08-05
2025-11-09 20:14:31,624 - INFO - root - Page:1, Index:47, TF-MLPNet: Tiny Real-Time Neural Speech Separation, https://arxiv.org/pdf/2508.03047, 2025-08-04
2025-11-09 20:14:31,624 - INFO - root - Page:1, Index:48, SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation, https://arxiv.org/pdf/2508.01375, 2025-08-02
2025-11-09 20:14:31,624 - INFO - root - Page:1, Index:49, MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective, https://arxiv.org/pdf/2507.19131, 2025-07-25
2025-11-09 20:14:31,625 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 20:14:38,390 - INFO - root - get_all_titles_from_web 
2025-11-09 20:14:38,390 - INFO - root - Page:2, Index:0, Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction, https://arxiv.org/pdf/2507.17768, 2025-07-16
2025-11-09 20:14:38,390 - INFO - root - Page:2, Index:1, SiLQ: Simple Large Language Model Quantization-Aware Training, https://arxiv.org/pdf/2507.16933, 2025-07-22
2025-11-09 20:14:38,390 - INFO - root - Page:2, Index:2, Task-Specific Zero-shot Quantization-Aware Training for Object Detection, https://arxiv.org/pdf/2507.16782, 2025-07-22
2025-11-09 20:14:38,391 - INFO - root - Page:2, Index:3, TorchAO: PyTorch-Native Training-to-Serving Model Optimization, https://arxiv.org/pdf/2507.16099, 2025-07-21
2025-11-09 20:14:38,391 - INFO - root - Page:2, Index:4, SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models, https://arxiv.org/pdf/2507.14811, 2025-08-27
2025-11-09 20:14:38,391 - INFO - root - Page:2, Index:5, Apple Intelligence Foundation Language Models: Tech Report 2025, https://arxiv.org/pdf/2507.13575, 2025-08-27
2025-11-09 20:14:38,391 - INFO - root - Page:2, Index:6, Generative Multi-Target Cross-Domain Recommendation, https://arxiv.org/pdf/2507.12871, 2025-08-07
2025-11-09 20:14:38,391 - INFO - root - Page:2, Index:7, DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training, https://arxiv.org/pdf/2507.07149, 2025-07-09
2025-11-09 20:14:38,391 - INFO - root - Page:2, Index:8, Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics, https://arxiv.org/pdf/2507.07044, 2025-07-15
2025-11-09 20:14:38,392 - INFO - root - Page:2, Index:9, QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models, https://arxiv.org/pdf/2507.06079, 2025-07-08
2025-11-09 20:14:38,392 - INFO - root - Page:2, Index:10, GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field, https://arxiv.org/pdf/2507.05594, 2025-07-07
2025-11-09 20:14:38,392 - INFO - root - Page:2, Index:11, Hita: Holistic Tokenizer for Autoregressive Image Generation, https://arxiv.org/pdf/2507.02358, 2025-07-11
2025-11-09 20:14:38,392 - INFO - root - Page:2, Index:12, QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference, https://arxiv.org/pdf/2506.23934, 2025-06-30
2025-11-09 20:14:38,392 - INFO - root - Page:2, Index:13, FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization, https://arxiv.org/pdf/2506.23516, 2025-07-21
2025-11-09 20:14:38,393 - INFO - root - Page:2, Index:14, Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models, https://arxiv.org/pdf/2506.23025, 2025-06-28
2025-11-09 20:14:38,393 - INFO - root - Page:2, Index:15, Joint Quantization and Pruning Neural Networks Approach: A Case Study on FSO Receivers, https://arxiv.org/pdf/2506.20084, 2025-06-24
2025-11-09 20:14:38,393 - INFO - root - Page:2, Index:16, Single-step Diffusion for Image Compression at Ultra-Low Bitrates, https://arxiv.org/pdf/2506.16572, 2025-09-22
2025-11-09 20:14:38,394 - INFO - root - Page:2, Index:17, LittleBit: Ultra Low-Bit Quantization via Latent Factorization, https://arxiv.org/pdf/2506.13771, 2025-10-28
2025-11-09 20:14:38,394 - INFO - root - Page:2, Index:18, MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering, https://arxiv.org/pdf/2506.13755, 2025-06-16
2025-11-09 20:14:38,395 - INFO - root - Page:2, Index:19, EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization, https://arxiv.org/pdf/2506.13329, 2025-07-04
2025-11-09 20:14:38,395 - INFO - root - Page:2, Index:20, Towards Neural Audio Codec Source Parsing, https://arxiv.org/pdf/2506.12627, 2025-06-14
2025-11-09 20:14:38,396 - INFO - root - Page:2, Index:21, Quantizing Small-Scale State-Space Models for Edge AI, https://arxiv.org/pdf/2506.12480, 2025-06-14
2025-11-09 20:14:38,396 - INFO - root - Page:2, Index:22, EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction, https://arxiv.org/pdf/2506.12015, 2025-06-13
2025-11-09 20:14:38,397 - INFO - root - Page:2, Index:23, Compression Aware Certified Training, https://arxiv.org/pdf/2506.11992, 2025-06-13
2025-11-09 20:14:38,397 - INFO - root - Page:2, Index:24, GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers, https://arxiv.org/pdf/2506.11784, 2025-06-13
2025-11-09 20:14:38,397 - INFO - root - Page:2, Index:25, TruncQuant: Truncation-Ready Quantization for DNNs with Flexible Weight Bit Precision, https://arxiv.org/pdf/2506.11431, 2025-06-12
2025-11-09 20:14:38,398 - INFO - root - Page:2, Index:26, EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices, https://arxiv.org/pdf/2506.11093, 2025-06-05
2025-11-09 20:14:38,399 - INFO - root - Page:2, Index:27, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 20:14:38,402 - INFO - root - Page:2, Index:28, AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent, https://arxiv.org/pdf/2506.10205, 2025-06-11
2025-11-09 20:14:38,403 - INFO - root - Page:2, Index:29, Q-SAM2: Accurate Quantization for Segment Anything Model 2, https://arxiv.org/pdf/2506.09782, 2025-06-11
2025-11-09 20:14:38,405 - INFO - root - Page:2, Index:30, Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs, https://arxiv.org/pdf/2506.09104, 2025-06-10
2025-11-09 20:14:38,406 - INFO - root - Page:2, Index:31, Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU, https://arxiv.org/pdf/2506.08911, 2025-06-10
2025-11-09 20:14:38,406 - INFO - root - Page:2, Index:32, POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration, https://arxiv.org/pdf/2506.08785, 2025-06-10
2025-11-09 20:14:38,407 - INFO - root - Page:2, Index:33, MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts, https://arxiv.org/pdf/2506.07533, 2025-06-09
2025-11-09 20:14:38,407 - INFO - root - Page:2, Index:34, BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation, https://arxiv.org/pdf/2506.07530, 2025-06-09
2025-11-09 20:14:38,407 - INFO - root - Page:2, Index:35, RecGPT: A Foundation Model for Sequential Recommendation, https://arxiv.org/pdf/2506.06270, 2025-06-12
2025-11-09 20:14:38,408 - INFO - root - Page:2, Index:36, FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion, https://arxiv.org/pdf/2506.04648, 2025-06-05
2025-11-09 20:14:38,408 - INFO - root - Page:2, Index:37, BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing, https://arxiv.org/pdf/2506.03515, 2025-06-03
2025-11-09 20:14:38,408 - INFO - root - Page:2, Index:38, ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding, https://arxiv.org/pdf/2506.01853, 2025-06-02
2025-11-09 20:14:38,408 - INFO - root - Page:2, Index:39, Movable Antenna Enhanced Federated Fine-Tuning of Large Language Models via Hybrid Client Selection Optimization, https://arxiv.org/pdf/2506.00011, 2025-10-26
2025-11-09 20:14:38,409 - INFO - root - Page:2, Index:40, Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach, https://arxiv.org/pdf/2505.24721, 2025-05-30
2025-11-09 20:14:38,411 - INFO - root - Page:2, Index:41, GARLIC: GAussian Representation LearnIng for spaCe partitioning, https://arxiv.org/pdf/2505.24608, 2025-10-02
2025-11-09 20:14:38,411 - INFO - root - Page:2, Index:42, AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up, https://arxiv.org/pdf/2505.24584, 2025-08-18
2025-11-09 20:14:38,414 - INFO - root - Page:2, Index:43, Model-Preserving Adaptive Rounding, https://arxiv.org/pdf/2505.22988, 2025-09-25
2025-11-09 20:14:38,414 - INFO - root - Page:2, Index:44, Highly Efficient and Effective LLMs with Multi-Boolean Architectures, https://arxiv.org/pdf/2505.22811, 2025-10-03
2025-11-09 20:14:38,414 - INFO - root - Page:2, Index:45, Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning, https://arxiv.org/pdf/2505.21591, 2025-05-27
2025-11-09 20:14:38,414 - INFO - root - Page:2, Index:46, Frequency Composition for Compressed and Domain-Adaptive Neural Networks, https://arxiv.org/pdf/2505.20890, 2025-05-27
2025-11-09 20:14:38,416 - INFO - root - Page:2, Index:47, MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition, https://arxiv.org/pdf/2505.20744, 2025-10-27
2025-11-09 20:14:38,416 - INFO - root - Page:2, Index:48, Optimizing edge AI models on HPC systems with the edge in the loop, https://arxiv.org/pdf/2505.19995, 2025-05-26
2025-11-09 20:14:38,416 - INFO - root - Fetching page 4 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=150
2025-11-09 20:14:45,563 - INFO - root - get_all_titles_from_web 
2025-11-09 20:14:45,564 - INFO - root - Page:3, Index:0, DPASyn: Mechanism-Aware Drug Synergy Prediction via Dual Attention and Precision-Aware Quantization, https://arxiv.org/pdf/2505.19144, 2025-09-20
2025-11-09 20:14:45,564 - INFO - root - Page:3, Index:1, Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding, https://arxiv.org/pdf/2505.18758, 2025-05-24
2025-11-09 20:14:45,564 - INFO - root - Page:3, Index:2, Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization, https://arxiv.org/pdf/2505.18113, 2025-05-23
2025-11-09 20:14:45,564 - INFO - root - Page:3, Index:3, Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs, https://arxiv.org/pdf/2505.17662, 2025-09-19
2025-11-09 20:14:45,565 - INFO - root - Page:3, Index:4, FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design, https://arxiv.org/pdf/2505.16335, 2025-05-22
2025-11-09 20:14:45,565 - INFO - root - Page:3, Index:5, Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control, https://arxiv.org/pdf/2505.15304, 2025-05-30
2025-11-09 20:14:45,565 - INFO - root - Page:3, Index:6, Scaling Law for Quantization-Aware Training, https://arxiv.org/pdf/2505.14302, 2025-05-20
2025-11-09 20:14:45,565 - INFO - root - Page:3, Index:7, Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs, https://arxiv.org/pdf/2505.13060, 2025-05-19
2025-11-09 20:14:45,566 - INFO - root - Page:3, Index:8, Deep Unfolding with Kernel-based Quantization in MIMO Detection, https://arxiv.org/pdf/2505.12736, 2025-05-19
2025-11-09 20:14:45,566 - INFO - root - Page:3, Index:9, FedHQ: Hybrid Runtime Quantization for Federated Learning, https://arxiv.org/pdf/2505.11982, 2025-05-17
2025-11-09 20:14:45,567 - INFO - root - Page:3, Index:10, QVGen: Pushing the Limit of Quantized Video Generative Models, https://arxiv.org/pdf/2505.11497, 2025-09-27
2025-11-09 20:14:45,567 - INFO - root - Page:3, Index:11, EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes, https://arxiv.org/pdf/2505.10787, 2025-05-15
2025-11-09 20:14:45,567 - INFO - root - Page:3, Index:12, ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition, https://arxiv.org/pdf/2505.08981, 2025-05-13
2025-11-09 20:14:45,567 - INFO - root - Page:3, Index:13, PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs, https://arxiv.org/pdf/2505.03254, 2025-08-06
2025-11-09 20:14:45,568 - INFO - root - Page:3, Index:14, ICQuant: Index Coding enables Low-bit LLM Quantization, https://arxiv.org/pdf/2505.00850, 2025-08-24
2025-11-09 20:14:45,568 - INFO - root - Page:3, Index:15, Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining, https://arxiv.org/pdf/2504.13932, 2025-07-30
2025-11-09 20:14:45,568 - INFO - root - Page:3, Index:16, Achieving binary weight and activation for LLMs using Post-Training Quantization, https://arxiv.org/pdf/2504.05352, 2025-06-30
2025-11-09 20:14:45,568 - INFO - root - Page:3, Index:17, Wireless Hearables With Programmable Speech AI Accelerators, https://arxiv.org/pdf/2503.18698, 2025-10-21
2025-11-09 20:14:45,568 - INFO - root - Page:3, Index:18, GranQ: Efficient Channel-wise Quantization via Vectorized Pre-Scaling for Zero-Shot QAT, https://arxiv.org/pdf/2503.18339, 2025-10-15
2025-11-09 20:14:45,569 - INFO - root - Page:3, Index:19, CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting, https://arxiv.org/pdf/2503.12836, 2025-09-29
2025-11-09 20:14:45,569 - INFO - root - Page:3, Index:20, AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model, https://arxiv.org/pdf/2503.03088, 2025-07-11
2025-11-09 20:14:45,569 - INFO - root - Page:3, Index:21, Teaching Metric Distance to Discrete Autoregressive Language Models, https://arxiv.org/pdf/2503.02379, 2025-10-07
2025-11-09 20:14:45,570 - INFO - root - Page:3, Index:22, Regularization-based Framework for Quantization-, Fault- and Variability-Aware Training, https://arxiv.org/pdf/2503.01297, 2025-07-12
2025-11-09 20:14:45,570 - INFO - root - Fetching page 5 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=200
2025-11-09 20:14:52,400 - INFO - root - get_all_titles_from_web 
2025-11-09 20:14:52,400 - INFO - root - Page:4, Index:0, Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models, https://arxiv.org/pdf/2502.15799, 2025-06-29
2025-11-09 20:14:52,401 - INFO - root - Page:4, Index:1, Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer, https://arxiv.org/pdf/2502.15779, 2025-09-01
2025-11-09 20:14:52,401 - INFO - root - Page:4, Index:2, RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models, https://arxiv.org/pdf/2502.09003, 2025-06-05
2025-11-09 20:14:52,401 - INFO - root - Page:4, Index:3, Spatial Degradation-Aware and Temporal Consistent Diffusion Model for Compressed Video Super-Resolution, https://arxiv.org/pdf/2502.07381, 2025-06-27
2025-11-09 20:14:52,402 - INFO - root - Page:4, Index:4, QuEST: Stable Training of LLMs with 1-Bit Weights and Activations, https://arxiv.org/pdf/2502.05003, 2025-06-10
2025-11-09 20:14:52,402 - INFO - root - Page:4, Index:5, HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs, https://arxiv.org/pdf/2501.02625, 2025-11-05
2025-11-09 20:14:52,402 - INFO - root - Page:4, Index:6, TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction, https://arxiv.org/pdf/2412.16919, 2025-08-08
2025-11-09 20:14:52,402 - INFO - root - Page:4, Index:7, Training Multi-Layer Binary Neural Networks With Local Binary Error Signals, https://arxiv.org/pdf/2412.00119, 2025-08-05
2025-11-09 20:14:52,404 - INFO - root - Page:4, Index:8, Scaling Large-scale GNN Training to Thousands of Processors on CPU-based Supercomputers, https://arxiv.org/pdf/2411.16025, 2025-05-26
2025-11-09 20:14:52,404 - INFO - root - Page:4, Index:9, HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization, https://arxiv.org/pdf/2411.06581, 2025-05-16
2025-11-09 20:14:52,404 - INFO - root - Fetching page 6 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=250
2025-11-09 20:14:59,509 - INFO - root - get_all_titles_from_web 
2025-11-09 20:14:59,509 - INFO - root - Page:5, Index:0, Gradient-Free Training of Quantized Neural Networks, https://arxiv.org/pdf/2410.09734, 2025-09-29
2025-11-09 20:14:59,510 - INFO - root - Page:5, Index:1, QT-DoG: Quantization-aware Training for Domain Generalization, https://arxiv.org/pdf/2410.06020, 2025-06-26
2025-11-09 20:14:59,510 - INFO - root - Page:5, Index:2, Constraint Guided Model Quantization of Neural Networks, https://arxiv.org/pdf/2409.20138, 2025-09-12
2025-11-09 20:14:59,510 - INFO - root - Page:5, Index:3, Accumulator-Aware Post-Training Quantization for Large Language Models, https://arxiv.org/pdf/2409.17092, 2025-07-30
2025-11-09 20:14:59,511 - INFO - root - Page:5, Index:4, DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation, https://arxiv.org/pdf/2409.14307, 2025-07-09
2025-11-09 20:14:59,511 - INFO - root - Page:5, Index:5, Robust Training of Neural Networks at Arbitrary Precision and Sparsity, https://arxiv.org/pdf/2409.09245, 2025-09-23
2025-11-09 20:14:59,511 - INFO - root - Page:5, Index:6, VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing, https://arxiv.org/pdf/2408.05758, 2025-05-27
2025-11-09 20:14:59,511 - INFO - root - Page:5, Index:7, DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers, https://arxiv.org/pdf/2408.03291, 2025-06-19
2025-11-09 20:14:59,513 - INFO - root - Page:5, Index:8, Temporal Feature Matters: A Framework for Diffusion Model Quantization, https://arxiv.org/pdf/2407.19547, 2025-07-11
2025-11-09 20:14:59,513 - INFO - root - Page:5, Index:9, EfficientQAT: Efficient Quantization-Aware Training for Large Language Models, https://arxiv.org/pdf/2407.11062, 2025-05-19
2025-11-09 20:14:59,513 - INFO - root - Page:5, Index:10, Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT, https://arxiv.org/pdf/2407.11041, 2025-10-31
2025-11-09 20:14:59,514 - INFO - root - Fetching page 7 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=300
2025-11-09 20:15:06,532 - INFO - root - get_all_titles_from_web 
2025-11-09 20:15:06,534 - INFO - root - Page:6, Index:0, BoA: Attention-aware Post-training Quantization without Backpropagation, https://arxiv.org/pdf/2406.13474, 2025-06-06
2025-11-09 20:15:06,535 - INFO - root - Page:6, Index:1, A Rescaling-Invariant Lipschitz Bound Based on Path-Metrics for Modern ReLU Network Parameterizations, https://arxiv.org/pdf/2405.15006, 2025-06-13
2025-11-09 20:15:06,536 - INFO - root - Page:6, Index:2, Scheduling Weight Transitions for Quantization-Aware Training, https://arxiv.org/pdf/2404.19248, 2025-10-01
2025-11-09 20:15:06,536 - INFO - root - Page:6, Index:3, GPTVQ: The Blessing of Dimensionality for LLM Quantization, https://arxiv.org/pdf/2402.15319, 2025-06-03
2025-11-09 20:15:06,537 - INFO - root - Fetching page 8 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=350
2025-11-09 20:15:13,032 - INFO - root - get_all_titles_from_web 
2025-11-09 20:15:13,032 - INFO - root - Page:7, Index:0, Squat: Quant Small Language Models on the Edge, https://arxiv.org/pdf/2402.10787, 2025-07-01
2025-11-09 20:15:13,032 - INFO - root - Page:7, Index:1, L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2402.04902, 2025-07-21
2025-11-09 20:15:13,034 - INFO - root - Fetching page 9 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=400
2025-11-09 20:15:19,488 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 20:15:33,034 - INFO - root - 跳过已处理论文 A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies：D:\ChatPaper\academic Papers\Quantization-Aware-Training\A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat-5.pdf
2025-11-09 20:15:33,035 - INFO - root - 跳过已处理论文 FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error：D:\ChatPaper\academic Papers\Quantization-Aware-Training\FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error-5.pdf
2025-11-09 20:15:33,035 - INFO - root - summary time: 79.92 seconds
2025-11-09 20:28:53,722 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 20:28:53,728 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 20:28:53,751 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 20:28:57,059 - INFO - root - LLMClientManager: 指定使用客户端: Deepseek
2025-11-09 20:28:57,069 - INFO - root - DeepSeekClient: API key found: 9f9e270e-b...
2025-11-09 20:28:57,110 - INFO - root - DeepSeekClient: 使用模式: 火山引擎
2025-11-09 20:28:57,126 - INFO - root - DeepSeekClient: 模型名称: deepseek-v3-1-terminus
2025-11-09 20:29:04,506 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 20:29:04,566 - INFO - root - DeepSeekClient: initialized successfully with model deepseek-v3-1-terminus
2025-11-09 20:29:04,566 - INFO - root - LLMClientManager: DeepSeek 客户端初始化成功
2025-11-09 20:29:04,567 - WARNING - root - LLMClientManager: client Deepseek not available
2025-11-09 20:29:04,568 - WARNING - root - 无法切换到指定的客户端 Deepseek，将使用默认客户端
2025-11-09 20:29:04,568 - INFO - root - 可用客户端: ['DeepSeek']
2025-11-09 20:29:04,568 - INFO - root - 使用 LLM 模型: deepseek-v3-1-terminus
2025-11-09 20:29:04,569 - INFO - root - 可用客户端: ['DeepSeek']
2025-11-09 20:29:04,574 - INFO - root - === 运行配置 ===
2025-11-09 20:29:04,575 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 20:29:04,575 - INFO - root - 关键词: QAT
2025-11-09 20:29:04,577 - INFO - root - 查询: Quantization-Aware-Training
2025-11-09 20:29:04,577 - INFO - root - 排序: None
2025-11-09 20:29:04,577 - INFO - root - 最近天数: 180
2025-11-09 20:29:04,578 - INFO - root - 最大处理数量: 2
2025-11-09 20:29:04,578 - INFO - root - 保存图片: 是
2025-11-09 20:29:04,578 - INFO - root - 输出语言: 中文
2025-11-09 20:29:04,581 - INFO - root - 强制重新处理: 否
2025-11-09 20:29:04,581 - INFO - root - ====================
2025-11-09 20:29:04,582 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 20:29:04,582 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 20:29:11,959 - INFO - root - get_all_titles_from_web 
2025-11-09 20:29:11,965 - INFO - root - Page:0, Index:0, A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies, https://arxiv.org/pdf/2511.03201, 2025-11-05
2025-11-09 20:29:11,966 - INFO - root - Page:0, Index:1, FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error, https://arxiv.org/pdf/2511.02302, 2025-11-04
2025-11-09 20:29:11,967 - INFO - root - Page:0, Index:2, Outlier-Aware Post-Training Quantization for Image Super-Resolution, https://arxiv.org/pdf/2511.00682, 2025-11-01
2025-11-09 20:29:11,975 - INFO - root - Page:0, Index:3, Evaluation of Wafer-Scale SOT-MRAM for Analog Crossbar Array Applications, https://arxiv.org/pdf/2510.25853, 2025-11-03
2025-11-09 20:29:11,991 - INFO - root - Page:0, Index:4, Improving the Straight-Through Estimator with Zeroth-Order Information, https://arxiv.org/pdf/2510.23926, 2025-10-27
2025-11-09 20:29:12,007 - INFO - root - Page:0, Index:5, Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework, https://arxiv.org/pdf/2510.22243, 2025-10-25
2025-11-09 20:29:12,015 - INFO - root - Page:0, Index:6, TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge, https://arxiv.org/pdf/2510.21879, 2025-10-23
2025-11-09 20:29:12,016 - INFO - root - Page:0, Index:7, KARIPAP: Quantum-Inspired Tensor Network Compression of Large Language Models Using Infinite Projected Entangled Pair States and Tensor Renormalization Group, https://arxiv.org/pdf/2510.21844, 2025-10-22
2025-11-09 20:29:12,016 - INFO - root - Page:0, Index:8, A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization, https://arxiv.org/pdf/2510.21314, 2025-10-24
2025-11-09 20:29:12,018 - INFO - root - Page:0, Index:9, Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks, https://arxiv.org/pdf/2510.19760, 2025-10-22
2025-11-09 20:29:12,019 - INFO - root - Page:0, Index:10, CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training, https://arxiv.org/pdf/2510.18784, 2025-10-21
2025-11-09 20:29:12,021 - INFO - root - Page:0, Index:11, Mixed-Precision Quantization for Language Models: Techniques and Prospects, https://arxiv.org/pdf/2510.16805, 2025-10-19
2025-11-09 20:29:12,021 - INFO - root - Page:0, Index:12, SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation, https://arxiv.org/pdf/2510.16396, 2025-10-30
2025-11-09 20:29:12,022 - INFO - root - Page:0, Index:13, CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models, https://arxiv.org/pdf/2510.15962, 2025-10-11
2025-11-09 20:29:12,022 - INFO - root - Page:0, Index:14, SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization, https://arxiv.org/pdf/2510.15775, 2025-10-17
2025-11-09 20:29:12,023 - INFO - root - Page:0, Index:15, SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware, https://arxiv.org/pdf/2510.15542, 2025-10-31
2025-11-09 20:29:12,023 - INFO - root - Page:0, Index:16, GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework, https://arxiv.org/pdf/2510.15299, 2025-10-17
2025-11-09 20:29:12,023 - INFO - root - Page:0, Index:17, SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images, https://arxiv.org/pdf/2510.15072, 2025-10-16
2025-11-09 20:29:12,023 - INFO - root - Page:0, Index:18, FraQAT: Quantization Aware Training with Fractional bits, https://arxiv.org/pdf/2510.14823, 2025-10-16
2025-11-09 20:29:12,024 - INFO - root - Page:0, Index:19, Computing-In-Memory Aware Model Adaption For Edge Devices, https://arxiv.org/pdf/2510.14379, 2025-10-16
2025-11-09 20:29:12,024 - INFO - root - Page:0, Index:20, Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge, https://arxiv.org/pdf/2510.13760, 2025-11-04
2025-11-09 20:29:12,024 - INFO - root - Page:0, Index:21, NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models, https://arxiv.org/pdf/2510.13068, 2025-10-14
2025-11-09 20:29:12,025 - INFO - root - Page:0, Index:22, Detect Anything via Next Point Prediction, https://arxiv.org/pdf/2510.12798, 2025-10-14
2025-11-09 20:29:12,025 - INFO - root - Page:0, Index:23, AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model, https://arxiv.org/pdf/2510.11496, 2025-10-14
2025-11-09 20:29:12,026 - INFO - root - Page:0, Index:24, Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware, https://arxiv.org/pdf/2510.11484, 2025-10-13
2025-11-09 20:29:12,035 - INFO - root - Page:0, Index:25, SASER: Stego attacks on open-source LLMs, https://arxiv.org/pdf/2510.10486, 2025-10-12
2025-11-09 20:29:12,036 - INFO - root - Page:0, Index:26, Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition, https://arxiv.org/pdf/2510.09653, 2025-10-15
2025-11-09 20:29:12,040 - INFO - root - Page:0, Index:27, A Theoretically-Grounded Codebook for Digital Semantic Communications, https://arxiv.org/pdf/2510.07108, 2025-10-14
2025-11-09 20:29:12,056 - INFO - root - Page:0, Index:28, QuantDemoire: Quantization with Outlier Aware for Image Demoiréing, https://arxiv.org/pdf/2510.04066, 2025-10-05
2025-11-09 20:29:12,066 - INFO - root - Page:0, Index:29, Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization, https://arxiv.org/pdf/2510.03763, 2025-10-04
2025-11-09 20:29:12,067 - INFO - root - Page:0, Index:30, Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models, https://arxiv.org/pdf/2510.03274, 2025-09-27
2025-11-09 20:29:12,068 - INFO - root - Page:0, Index:31, PT$^2$-LLM: Post-Training Ternarization for Large Language Models, https://arxiv.org/pdf/2510.03267, 2025-09-26
2025-11-09 20:29:12,071 - INFO - root - Page:0, Index:32, Purrception: Variational Flow Matching for Vector-Quantized Image Generation, https://arxiv.org/pdf/2510.01478, 2025-10-01
2025-11-09 20:29:12,072 - INFO - root - Page:0, Index:33, Post-Training Quantization for Audio Diffusion Transformers, https://arxiv.org/pdf/2510.00313, 2025-09-30
2025-11-09 20:29:12,074 - INFO - root - Page:0, Index:34, Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling, https://arxiv.org/pdf/2510.00028, 2025-09-25
2025-11-09 20:29:12,074 - INFO - root - Page:0, Index:35, Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models, https://arxiv.org/pdf/2509.26436, 2025-09-30
2025-11-09 20:29:12,076 - INFO - root - Page:0, Index:36, Cat: Post-Training Quantization Error Reduction via Cluster-based Affine Transformation, https://arxiv.org/pdf/2509.26277, 2025-10-07
2025-11-09 20:29:12,088 - INFO - root - Page:0, Index:37, CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models, https://arxiv.org/pdf/2509.25996, 2025-09-30
2025-11-09 20:29:12,107 - INFO - root - Page:0, Index:38, Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications, https://arxiv.org/pdf/2509.25439, 2025-09-29
2025-11-09 20:29:12,114 - INFO - root - Page:0, Index:39, On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs, https://arxiv.org/pdf/2509.25214, 2025-09-22
2025-11-09 20:29:12,117 - INFO - root - Page:0, Index:40, VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning, https://arxiv.org/pdf/2509.24650, 2025-09-29
2025-11-09 20:29:12,123 - INFO - root - Page:0, Index:41, S$^2$NN: Sub-bit Spiking Neural Networks, https://arxiv.org/pdf/2509.24266, 2025-10-24
2025-11-09 20:29:12,125 - INFO - root - Page:0, Index:42, Tequila: Trapping-free Ternary Quantization for Large Language Models, https://arxiv.org/pdf/2509.23809, 2025-10-17
2025-11-09 20:29:12,125 - INFO - root - Page:0, Index:43, Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution, https://arxiv.org/pdf/2509.23774, 2025-09-30
2025-11-09 20:29:12,125 - INFO - root - Page:0, Index:44, RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization, https://arxiv.org/pdf/2509.23582, 2025-09-27
2025-11-09 20:29:12,133 - INFO - root - Page:0, Index:45, Beyond Outliers: A Study of Optimizers Under Quantization, https://arxiv.org/pdf/2509.23500, 2025-10-02
2025-11-09 20:29:12,134 - INFO - root - Page:0, Index:46, Compute-Optimal Quantization-Aware Training, https://arxiv.org/pdf/2509.22935, 2025-09-26
2025-11-09 20:29:12,136 - INFO - root - Page:0, Index:47, COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning, https://arxiv.org/pdf/2509.22075, 2025-10-06
2025-11-09 20:29:12,142 - INFO - root - Page:0, Index:48, SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models, https://arxiv.org/pdf/2509.21498, 2025-09-25
2025-11-09 20:29:12,159 - INFO - root - Page:0, Index:49, Quantized Visual Geometry Grounded Transformer, https://arxiv.org/pdf/2509.21302, 2025-09-29
2025-11-09 20:29:12,185 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 20:29:19,926 - INFO - root - get_all_titles_from_web 
2025-11-09 20:29:19,932 - INFO - root - Page:1, Index:0, Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy, https://arxiv.org/pdf/2509.21173, 2025-10-27
2025-11-09 20:29:19,960 - INFO - root - Page:1, Index:1, Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer, https://arxiv.org/pdf/2509.20854, 2025-09-25
2025-11-09 20:29:19,976 - INFO - root - Page:1, Index:2, TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection, https://arxiv.org/pdf/2509.18193, 2025-09-19
2025-11-09 20:29:19,978 - INFO - root - Page:1, Index:3, SBVR: Summation of BitVector Representation for Efficient LLM Quantization, https://arxiv.org/pdf/2509.18172, 2025-09-17
2025-11-09 20:29:19,983 - INFO - root - Page:1, Index:4, QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2509.17428, 2025-09-26
2025-11-09 20:29:20,008 - INFO - root - Page:1, Index:5, PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models, https://arxiv.org/pdf/2509.16989, 2025-10-28
2025-11-09 20:29:20,030 - INFO - root - Page:1, Index:6, MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training, https://arxiv.org/pdf/2509.15514, 2025-09-18
2025-11-09 20:29:20,045 - INFO - root - Page:1, Index:7, Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs, https://arxiv.org/pdf/2509.14391, 2025-09-17
2025-11-09 20:29:20,078 - INFO - root - Page:1, Index:8, Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization, https://arxiv.org/pdf/2509.13786, 2025-09-19
2025-11-09 20:29:20,093 - INFO - root - Page:1, Index:9, Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees, https://arxiv.org/pdf/2509.11054, 2025-09-13
2025-11-09 20:29:20,156 - INFO - root - Page:1, Index:10, SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models, https://arxiv.org/pdf/2509.09090, 2025-09-10
2025-11-09 20:29:20,280 - INFO - root - Page:1, Index:11, CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks with Entropy Regularization, https://arxiv.org/pdf/2509.08776, 2025-09-10
2025-11-09 20:29:20,397 - INFO - root - Page:1, Index:12, Explaining How Quantization Disparately Skews a Model, https://arxiv.org/pdf/2509.07222, 2025-09-08
2025-11-09 20:29:20,433 - INFO - root - Page:1, Index:13, LoaQ: Layer-wise Output Approximation Quantization, https://arxiv.org/pdf/2509.06297, 2025-09-07
2025-11-09 20:29:20,501 - INFO - root - Page:1, Index:14, FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving, https://arxiv.org/pdf/2509.06261, 2025-09-14
2025-11-09 20:29:20,513 - INFO - root - Page:1, Index:15, Sensitivity-Aware Post-Training Quantization for Deep Neural Networks, https://arxiv.org/pdf/2509.05576, 2025-09-05
2025-11-09 20:29:20,529 - INFO - root - Page:1, Index:16, SuperSNN: A Hardware-Aware Framework for Physically Realizable, High-Performance Superconducting Spiking Neural Network Chips, https://arxiv.org/pdf/2509.05532, 2025-09-05
2025-11-09 20:29:20,533 - INFO - root - Page:1, Index:17, Data-Augmented Quantization-Aware Knowledge Distillation, https://arxiv.org/pdf/2509.03850, 2025-09-03
2025-11-09 20:29:20,540 - INFO - root - Page:1, Index:18, DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling, https://arxiv.org/pdf/2509.03472, 2025-09-03
2025-11-09 20:29:20,590 - INFO - root - Page:1, Index:19, Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs, https://arxiv.org/pdf/2509.02017, 2025-09-02
2025-11-09 20:29:20,597 - INFO - root - Page:1, Index:20, Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling, https://arxiv.org/pdf/2509.01624, 2025-09-01
2025-11-09 20:29:20,616 - INFO - root - Page:1, Index:21, Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective, https://arxiv.org/pdf/2509.00859, 2025-08-31
2025-11-09 20:29:20,627 - INFO - root - Page:1, Index:22, Progressive Element-wise Gradient Estimation for Neural Network Quantization, https://arxiv.org/pdf/2509.00097, 2025-08-27
2025-11-09 20:29:20,645 - INFO - root - Page:1, Index:23, End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost, https://arxiv.org/pdf/2509.00031, 2025-09-29
2025-11-09 20:29:20,646 - INFO - root - Page:1, Index:24, Quantization Robustness to Input Degradations for Object Detection, https://arxiv.org/pdf/2508.19600, 2025-08-27
2025-11-09 20:29:20,647 - INFO - root - Page:1, Index:25, Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models, https://arxiv.org/pdf/2508.18609, 2025-08-27
2025-11-09 20:29:20,648 - INFO - root - Page:1, Index:26, AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration, https://arxiv.org/pdf/2508.18025, 2025-08-25
2025-11-09 20:29:20,649 - INFO - root - Page:1, Index:27, TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling, https://arxiv.org/pdf/2508.16790, 2025-08-22
2025-11-09 20:29:20,649 - INFO - root - Page:1, Index:28, A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering, https://arxiv.org/pdf/2508.16516, 2025-08-22
2025-11-09 20:29:20,697 - INFO - root - Page:1, Index:29, JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs, https://arxiv.org/pdf/2508.15468, 2025-08-21
2025-11-09 20:29:20,698 - INFO - root - Page:1, Index:30, MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation, https://arxiv.org/pdf/2508.15281, 2025-08-21
2025-11-09 20:29:20,700 - INFO - root - Page:1, Index:31, DLLMQuant: Quantizing Diffusion-based Large Language Models, https://arxiv.org/pdf/2508.14090, 2025-08-25
2025-11-09 20:29:20,700 - INFO - root - Page:1, Index:32, Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management, https://arxiv.org/pdf/2508.13905, 2025-08-19
2025-11-09 20:29:20,707 - INFO - root - Page:1, Index:33, XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads, https://arxiv.org/pdf/2508.13049, 2025-08-18
2025-11-09 20:29:20,708 - INFO - root - Page:1, Index:34, Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection, https://arxiv.org/pdf/2508.12230, 2025-08-17
2025-11-09 20:29:20,712 - INFO - root - Page:1, Index:35, TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks, https://arxiv.org/pdf/2508.12132, 2025-08-16
2025-11-09 20:29:20,715 - INFO - root - Page:1, Index:36, Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion, https://arxiv.org/pdf/2508.12094, 2025-10-13
2025-11-09 20:29:20,724 - INFO - root - Page:1, Index:37, Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware, https://arxiv.org/pdf/2508.11940, 2025-08-16
2025-11-09 20:29:20,730 - INFO - root - Page:1, Index:38, PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks, https://arxiv.org/pdf/2508.10557, 2025-08-15
2025-11-09 20:29:20,731 - INFO - root - Page:1, Index:39, MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI, https://arxiv.org/pdf/2508.09500, 2025-08-13
2025-11-09 20:29:20,731 - INFO - root - Page:1, Index:40, SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via Codebooks for recommender system, https://arxiv.org/pdf/2508.09090, 2025-08-12
2025-11-09 20:29:20,734 - INFO - root - Page:1, Index:41, Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models, https://arxiv.org/pdf/2508.06974, 2025-08-09
2025-11-09 20:29:20,734 - INFO - root - Page:1, Index:42, Efficient Deep Neural Receiver with Post-Training Quantization, https://arxiv.org/pdf/2508.06275, 2025-08-08
2025-11-09 20:29:20,735 - INFO - root - Page:1, Index:43, iFairy: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$, https://arxiv.org/pdf/2508.05571, 2025-08-16
2025-11-09 20:29:20,767 - INFO - root - Page:1, Index:44, S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation, https://arxiv.org/pdf/2508.04016, 2025-10-27
2025-11-09 20:29:20,780 - INFO - root - Page:1, Index:45, LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation, https://arxiv.org/pdf/2508.03485, 2025-09-23
2025-11-09 20:29:20,795 - INFO - root - Page:1, Index:46, VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation, https://arxiv.org/pdf/2508.03351, 2025-08-05
2025-11-09 20:29:20,798 - INFO - root - Page:1, Index:47, TF-MLPNet: Tiny Real-Time Neural Speech Separation, https://arxiv.org/pdf/2508.03047, 2025-08-04
2025-11-09 20:29:20,811 - INFO - root - Page:1, Index:48, SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation, https://arxiv.org/pdf/2508.01375, 2025-08-02
2025-11-09 20:29:20,860 - INFO - root - Page:1, Index:49, MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective, https://arxiv.org/pdf/2507.19131, 2025-07-25
2025-11-09 20:29:20,893 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 20:29:28,070 - INFO - root - get_all_titles_from_web 
2025-11-09 20:29:28,071 - INFO - root - Page:2, Index:0, Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction, https://arxiv.org/pdf/2507.17768, 2025-07-16
2025-11-09 20:29:28,072 - INFO - root - Page:2, Index:1, SiLQ: Simple Large Language Model Quantization-Aware Training, https://arxiv.org/pdf/2507.16933, 2025-07-22
2025-11-09 20:29:28,072 - INFO - root - Page:2, Index:2, Task-Specific Zero-shot Quantization-Aware Training for Object Detection, https://arxiv.org/pdf/2507.16782, 2025-07-22
2025-11-09 20:29:28,072 - INFO - root - Page:2, Index:3, TorchAO: PyTorch-Native Training-to-Serving Model Optimization, https://arxiv.org/pdf/2507.16099, 2025-07-21
2025-11-09 20:29:28,072 - INFO - root - Page:2, Index:4, SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models, https://arxiv.org/pdf/2507.14811, 2025-08-27
2025-11-09 20:29:28,072 - INFO - root - Page:2, Index:5, Apple Intelligence Foundation Language Models: Tech Report 2025, https://arxiv.org/pdf/2507.13575, 2025-08-27
2025-11-09 20:29:28,073 - INFO - root - Page:2, Index:6, Generative Multi-Target Cross-Domain Recommendation, https://arxiv.org/pdf/2507.12871, 2025-08-07
2025-11-09 20:29:28,073 - INFO - root - Page:2, Index:7, DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training, https://arxiv.org/pdf/2507.07149, 2025-07-09
2025-11-09 20:29:28,073 - INFO - root - Page:2, Index:8, Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics, https://arxiv.org/pdf/2507.07044, 2025-07-15
2025-11-09 20:29:28,073 - INFO - root - Page:2, Index:9, QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models, https://arxiv.org/pdf/2507.06079, 2025-07-08
2025-11-09 20:29:28,073 - INFO - root - Page:2, Index:10, GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field, https://arxiv.org/pdf/2507.05594, 2025-07-07
2025-11-09 20:29:28,074 - INFO - root - Page:2, Index:11, Hita: Holistic Tokenizer for Autoregressive Image Generation, https://arxiv.org/pdf/2507.02358, 2025-07-11
2025-11-09 20:29:28,074 - INFO - root - Page:2, Index:12, QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference, https://arxiv.org/pdf/2506.23934, 2025-06-30
2025-11-09 20:29:28,074 - INFO - root - Page:2, Index:13, FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization, https://arxiv.org/pdf/2506.23516, 2025-07-21
2025-11-09 20:29:28,074 - INFO - root - Page:2, Index:14, Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models, https://arxiv.org/pdf/2506.23025, 2025-06-28
2025-11-09 20:29:28,074 - INFO - root - Page:2, Index:15, Joint Quantization and Pruning Neural Networks Approach: A Case Study on FSO Receivers, https://arxiv.org/pdf/2506.20084, 2025-06-24
2025-11-09 20:29:28,074 - INFO - root - Page:2, Index:16, Single-step Diffusion for Image Compression at Ultra-Low Bitrates, https://arxiv.org/pdf/2506.16572, 2025-09-22
2025-11-09 20:29:28,075 - INFO - root - Page:2, Index:17, LittleBit: Ultra Low-Bit Quantization via Latent Factorization, https://arxiv.org/pdf/2506.13771, 2025-10-28
2025-11-09 20:29:28,075 - INFO - root - Page:2, Index:18, MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering, https://arxiv.org/pdf/2506.13755, 2025-06-16
2025-11-09 20:29:28,075 - INFO - root - Page:2, Index:19, EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization, https://arxiv.org/pdf/2506.13329, 2025-07-04
2025-11-09 20:29:28,075 - INFO - root - Page:2, Index:20, Towards Neural Audio Codec Source Parsing, https://arxiv.org/pdf/2506.12627, 2025-06-14
2025-11-09 20:29:28,077 - INFO - root - Page:2, Index:21, Quantizing Small-Scale State-Space Models for Edge AI, https://arxiv.org/pdf/2506.12480, 2025-06-14
2025-11-09 20:29:28,078 - INFO - root - Page:2, Index:22, EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction, https://arxiv.org/pdf/2506.12015, 2025-06-13
2025-11-09 20:29:28,078 - INFO - root - Page:2, Index:23, Compression Aware Certified Training, https://arxiv.org/pdf/2506.11992, 2025-06-13
2025-11-09 20:29:28,078 - INFO - root - Page:2, Index:24, GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers, https://arxiv.org/pdf/2506.11784, 2025-06-13
2025-11-09 20:29:28,078 - INFO - root - Page:2, Index:25, TruncQuant: Truncation-Ready Quantization for DNNs with Flexible Weight Bit Precision, https://arxiv.org/pdf/2506.11431, 2025-06-12
2025-11-09 20:29:28,080 - INFO - root - Page:2, Index:26, EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices, https://arxiv.org/pdf/2506.11093, 2025-06-05
2025-11-09 20:29:28,081 - INFO - root - Page:2, Index:27, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 20:29:28,081 - INFO - root - Page:2, Index:28, AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent, https://arxiv.org/pdf/2506.10205, 2025-06-11
2025-11-09 20:29:28,081 - INFO - root - Page:2, Index:29, Q-SAM2: Accurate Quantization for Segment Anything Model 2, https://arxiv.org/pdf/2506.09782, 2025-06-11
2025-11-09 20:29:28,081 - INFO - root - Page:2, Index:30, Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs, https://arxiv.org/pdf/2506.09104, 2025-06-10
2025-11-09 20:29:28,081 - INFO - root - Page:2, Index:31, Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU, https://arxiv.org/pdf/2506.08911, 2025-06-10
2025-11-09 20:29:28,081 - INFO - root - Page:2, Index:32, POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration, https://arxiv.org/pdf/2506.08785, 2025-06-10
2025-11-09 20:29:28,083 - INFO - root - Page:2, Index:33, MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts, https://arxiv.org/pdf/2506.07533, 2025-06-09
2025-11-09 20:29:28,084 - INFO - root - Page:2, Index:34, BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation, https://arxiv.org/pdf/2506.07530, 2025-06-09
2025-11-09 20:29:28,084 - INFO - root - Page:2, Index:35, RecGPT: A Foundation Model for Sequential Recommendation, https://arxiv.org/pdf/2506.06270, 2025-06-12
2025-11-09 20:29:28,086 - INFO - root - Page:2, Index:36, FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion, https://arxiv.org/pdf/2506.04648, 2025-06-05
2025-11-09 20:29:28,086 - INFO - root - Page:2, Index:37, BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing, https://arxiv.org/pdf/2506.03515, 2025-06-03
2025-11-09 20:29:28,087 - INFO - root - Page:2, Index:38, ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding, https://arxiv.org/pdf/2506.01853, 2025-06-02
2025-11-09 20:29:28,087 - INFO - root - Page:2, Index:39, Movable Antenna Enhanced Federated Fine-Tuning of Large Language Models via Hybrid Client Selection Optimization, https://arxiv.org/pdf/2506.00011, 2025-10-26
2025-11-09 20:29:28,088 - INFO - root - Page:2, Index:40, Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach, https://arxiv.org/pdf/2505.24721, 2025-05-30
2025-11-09 20:29:28,088 - INFO - root - Page:2, Index:41, GARLIC: GAussian Representation LearnIng for spaCe partitioning, https://arxiv.org/pdf/2505.24608, 2025-10-02
2025-11-09 20:29:28,088 - INFO - root - Page:2, Index:42, AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up, https://arxiv.org/pdf/2505.24584, 2025-08-18
2025-11-09 20:29:28,089 - INFO - root - Page:2, Index:43, Model-Preserving Adaptive Rounding, https://arxiv.org/pdf/2505.22988, 2025-09-25
2025-11-09 20:29:28,089 - INFO - root - Page:2, Index:44, Highly Efficient and Effective LLMs with Multi-Boolean Architectures, https://arxiv.org/pdf/2505.22811, 2025-10-03
2025-11-09 20:29:28,089 - INFO - root - Page:2, Index:45, Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning, https://arxiv.org/pdf/2505.21591, 2025-05-27
2025-11-09 20:29:28,089 - INFO - root - Page:2, Index:46, Frequency Composition for Compressed and Domain-Adaptive Neural Networks, https://arxiv.org/pdf/2505.20890, 2025-05-27
2025-11-09 20:29:28,090 - INFO - root - Page:2, Index:47, MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition, https://arxiv.org/pdf/2505.20744, 2025-10-27
2025-11-09 20:29:28,091 - INFO - root - Page:2, Index:48, Optimizing edge AI models on HPC systems with the edge in the loop, https://arxiv.org/pdf/2505.19995, 2025-05-26
2025-11-09 20:29:28,091 - INFO - root - Fetching page 4 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=150
2025-11-09 20:29:36,313 - INFO - root - get_all_titles_from_web 
2025-11-09 20:29:36,313 - INFO - root - Page:3, Index:0, DPASyn: Mechanism-Aware Drug Synergy Prediction via Dual Attention and Precision-Aware Quantization, https://arxiv.org/pdf/2505.19144, 2025-09-20
2025-11-09 20:29:36,314 - INFO - root - Page:3, Index:1, Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding, https://arxiv.org/pdf/2505.18758, 2025-05-24
2025-11-09 20:29:36,315 - INFO - root - Page:3, Index:2, Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization, https://arxiv.org/pdf/2505.18113, 2025-05-23
2025-11-09 20:29:36,315 - INFO - root - Page:3, Index:3, Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs, https://arxiv.org/pdf/2505.17662, 2025-09-19
2025-11-09 20:29:36,317 - INFO - root - Page:3, Index:4, FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design, https://arxiv.org/pdf/2505.16335, 2025-05-22
2025-11-09 20:29:36,325 - INFO - root - Page:3, Index:5, Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control, https://arxiv.org/pdf/2505.15304, 2025-05-30
2025-11-09 20:29:36,330 - INFO - root - Page:3, Index:6, Scaling Law for Quantization-Aware Training, https://arxiv.org/pdf/2505.14302, 2025-05-20
2025-11-09 20:29:36,341 - INFO - root - Page:3, Index:7, Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs, https://arxiv.org/pdf/2505.13060, 2025-05-19
2025-11-09 20:29:36,344 - INFO - root - Page:3, Index:8, Deep Unfolding with Kernel-based Quantization in MIMO Detection, https://arxiv.org/pdf/2505.12736, 2025-05-19
2025-11-09 20:29:36,344 - INFO - root - Page:3, Index:9, FedHQ: Hybrid Runtime Quantization for Federated Learning, https://arxiv.org/pdf/2505.11982, 2025-05-17
2025-11-09 20:29:36,346 - INFO - root - Page:3, Index:10, QVGen: Pushing the Limit of Quantized Video Generative Models, https://arxiv.org/pdf/2505.11497, 2025-09-27
2025-11-09 20:29:36,347 - INFO - root - Page:3, Index:11, EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes, https://arxiv.org/pdf/2505.10787, 2025-05-15
2025-11-09 20:29:36,350 - INFO - root - Page:3, Index:12, ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition, https://arxiv.org/pdf/2505.08981, 2025-05-13
2025-11-09 20:29:36,356 - INFO - root - Page:3, Index:13, PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs, https://arxiv.org/pdf/2505.03254, 2025-08-06
2025-11-09 20:29:36,357 - INFO - root - Page:3, Index:14, ICQuant: Index Coding enables Low-bit LLM Quantization, https://arxiv.org/pdf/2505.00850, 2025-08-24
2025-11-09 20:29:36,360 - INFO - root - Page:3, Index:15, Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining, https://arxiv.org/pdf/2504.13932, 2025-07-30
2025-11-09 20:29:36,362 - INFO - root - Page:3, Index:16, Achieving binary weight and activation for LLMs using Post-Training Quantization, https://arxiv.org/pdf/2504.05352, 2025-06-30
2025-11-09 20:29:36,362 - INFO - root - Page:3, Index:17, Wireless Hearables With Programmable Speech AI Accelerators, https://arxiv.org/pdf/2503.18698, 2025-10-21
2025-11-09 20:29:36,373 - INFO - root - Page:3, Index:18, GranQ: Efficient Channel-wise Quantization via Vectorized Pre-Scaling for Zero-Shot QAT, https://arxiv.org/pdf/2503.18339, 2025-10-15
2025-11-09 20:29:36,376 - INFO - root - Page:3, Index:19, CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting, https://arxiv.org/pdf/2503.12836, 2025-09-29
2025-11-09 20:29:36,377 - INFO - root - Page:3, Index:20, AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model, https://arxiv.org/pdf/2503.03088, 2025-07-11
2025-11-09 20:29:36,379 - INFO - root - Page:3, Index:21, Teaching Metric Distance to Discrete Autoregressive Language Models, https://arxiv.org/pdf/2503.02379, 2025-10-07
2025-11-09 20:29:36,398 - INFO - root - Page:3, Index:22, Regularization-based Framework for Quantization-, Fault- and Variability-Aware Training, https://arxiv.org/pdf/2503.01297, 2025-07-12
2025-11-09 20:29:36,416 - INFO - root - Fetching page 5 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=200
2025-11-09 20:29:43,978 - INFO - root - get_all_titles_from_web 
2025-11-09 20:29:43,978 - INFO - root - Page:4, Index:0, Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models, https://arxiv.org/pdf/2502.15799, 2025-06-29
2025-11-09 20:29:43,979 - INFO - root - Page:4, Index:1, Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer, https://arxiv.org/pdf/2502.15779, 2025-09-01
2025-11-09 20:29:43,979 - INFO - root - Page:4, Index:2, RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models, https://arxiv.org/pdf/2502.09003, 2025-06-05
2025-11-09 20:29:43,979 - INFO - root - Page:4, Index:3, Spatial Degradation-Aware and Temporal Consistent Diffusion Model for Compressed Video Super-Resolution, https://arxiv.org/pdf/2502.07381, 2025-06-27
2025-11-09 20:29:43,979 - INFO - root - Page:4, Index:4, QuEST: Stable Training of LLMs with 1-Bit Weights and Activations, https://arxiv.org/pdf/2502.05003, 2025-06-10
2025-11-09 20:29:43,979 - INFO - root - Page:4, Index:5, HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs, https://arxiv.org/pdf/2501.02625, 2025-11-05
2025-11-09 20:29:43,981 - INFO - root - Page:4, Index:6, TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction, https://arxiv.org/pdf/2412.16919, 2025-08-08
2025-11-09 20:29:43,981 - INFO - root - Page:4, Index:7, Training Multi-Layer Binary Neural Networks With Local Binary Error Signals, https://arxiv.org/pdf/2412.00119, 2025-08-05
2025-11-09 20:29:43,981 - INFO - root - Page:4, Index:8, Scaling Large-scale GNN Training to Thousands of Processors on CPU-based Supercomputers, https://arxiv.org/pdf/2411.16025, 2025-05-26
2025-11-09 20:29:43,981 - INFO - root - Page:4, Index:9, HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization, https://arxiv.org/pdf/2411.06581, 2025-05-16
2025-11-09 20:29:43,982 - INFO - root - Fetching page 6 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=250
2025-11-09 20:29:50,445 - INFO - root - get_all_titles_from_web 
2025-11-09 20:29:50,446 - INFO - root - Page:5, Index:0, Gradient-Free Training of Quantized Neural Networks, https://arxiv.org/pdf/2410.09734, 2025-09-29
2025-11-09 20:29:50,446 - INFO - root - Page:5, Index:1, QT-DoG: Quantization-aware Training for Domain Generalization, https://arxiv.org/pdf/2410.06020, 2025-06-26
2025-11-09 20:29:50,447 - INFO - root - Page:5, Index:2, Constraint Guided Model Quantization of Neural Networks, https://arxiv.org/pdf/2409.20138, 2025-09-12
2025-11-09 20:29:50,447 - INFO - root - Page:5, Index:3, Accumulator-Aware Post-Training Quantization for Large Language Models, https://arxiv.org/pdf/2409.17092, 2025-07-30
2025-11-09 20:29:50,447 - INFO - root - Page:5, Index:4, DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation, https://arxiv.org/pdf/2409.14307, 2025-07-09
2025-11-09 20:29:50,448 - INFO - root - Page:5, Index:5, Robust Training of Neural Networks at Arbitrary Precision and Sparsity, https://arxiv.org/pdf/2409.09245, 2025-09-23
2025-11-09 20:29:50,448 - INFO - root - Page:5, Index:6, VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing, https://arxiv.org/pdf/2408.05758, 2025-05-27
2025-11-09 20:29:50,449 - INFO - root - Page:5, Index:7, DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers, https://arxiv.org/pdf/2408.03291, 2025-06-19
2025-11-09 20:29:50,451 - INFO - root - Page:5, Index:8, Temporal Feature Matters: A Framework for Diffusion Model Quantization, https://arxiv.org/pdf/2407.19547, 2025-07-11
2025-11-09 20:29:50,451 - INFO - root - Page:5, Index:9, EfficientQAT: Efficient Quantization-Aware Training for Large Language Models, https://arxiv.org/pdf/2407.11062, 2025-05-19
2025-11-09 20:29:50,452 - INFO - root - Page:5, Index:10, Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT, https://arxiv.org/pdf/2407.11041, 2025-10-31
2025-11-09 20:29:50,452 - INFO - root - Fetching page 7 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=300
2025-11-09 20:29:57,714 - INFO - root - get_all_titles_from_web 
2025-11-09 20:29:57,715 - INFO - root - Page:6, Index:0, BoA: Attention-aware Post-training Quantization without Backpropagation, https://arxiv.org/pdf/2406.13474, 2025-06-06
2025-11-09 20:29:57,716 - INFO - root - Page:6, Index:1, A Rescaling-Invariant Lipschitz Bound Based on Path-Metrics for Modern ReLU Network Parameterizations, https://arxiv.org/pdf/2405.15006, 2025-06-13
2025-11-09 20:29:57,716 - INFO - root - Page:6, Index:2, Scheduling Weight Transitions for Quantization-Aware Training, https://arxiv.org/pdf/2404.19248, 2025-10-01
2025-11-09 20:29:57,716 - INFO - root - Page:6, Index:3, GPTVQ: The Blessing of Dimensionality for LLM Quantization, https://arxiv.org/pdf/2402.15319, 2025-06-03
2025-11-09 20:29:57,717 - INFO - root - Fetching page 8 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=350
2025-11-09 20:30:06,889 - INFO - root - get_all_titles_from_web 
2025-11-09 20:30:06,891 - INFO - root - Page:7, Index:0, Squat: Quant Small Language Models on the Edge, https://arxiv.org/pdf/2402.10787, 2025-07-01
2025-11-09 20:30:06,892 - INFO - root - Page:7, Index:1, L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2402.04902, 2025-07-21
2025-11-09 20:30:06,893 - INFO - root - Fetching page 9 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=400
2025-11-09 20:30:18,691 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 20:30:33,568 - INFO - root - 跳过已处理论文 A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies：D:\ChatPaper\academic Papers\Quantization-Aware-Training\A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat-6.pdf
2025-11-09 20:30:33,570 - INFO - root - 跳过已处理论文 FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error：D:\ChatPaper\academic Papers\Quantization-Aware-Training\FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error-6.pdf
2025-11-09 20:30:33,570 - INFO - root - summary time: 99.85 seconds
2025-11-09 20:41:51,546 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=traffic+flow+prediction&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 20:41:57,937 - INFO - root - get_all_titles_from_web 
2025-11-09 20:41:57,937 - INFO - root - Page:0, Index:0, A Theoretical Framework for Environmental Similarity and Vessel Mobility as Coupled Predictors of Marine Invasive Species Pathways, https://arxiv.org/pdf/2511.03499, 2025-11-06
2025-11-09 20:41:57,939 - INFO - root - Page:0, Index:1, Towards Sub-millisecond Latency and Guaranteed Bit Rates in 5G User Plane, https://arxiv.org/pdf/2511.00196, 2025-10-31
2025-11-09 20:41:57,939 - INFO - root - Page:0, Index:2, A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration, https://arxiv.org/pdf/2510.27039, 2025-10-30
2025-11-09 20:41:57,939 - INFO - root - Page:0, Index:3, Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention, https://arxiv.org/pdf/2509.13361, 2025-11-04
2025-11-09 20:41:57,939 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=traffic+flow+prediction&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 20:45:14,569 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 20:45:14,571 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 20:45:14,572 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 20:45:15,662 - INFO - root - LLMClientManager: 指定使用客户端: DeepSeek
2025-11-09 20:45:15,663 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 20:45:15,663 - ERROR - root - LLMClientManager: 指定的客户端 DeepSeek 初始化失败
2025-11-09 20:45:15,663 - WARNING - root - LLMClientManager: 指定的客户端 DeepSeek 不可用，将尝试其他客户端
2025-11-09 20:45:16,564 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 20:45:19,999 - INFO - root - GeminiClient: initialized model gemini-2.5-flash
2025-11-09 20:45:20,000 - INFO - root - LLMClientManager: Gemini client initialized successfully
2025-11-09 20:45:20,000 - INFO - root - LLMClientManager: using Gemini as default client
2025-11-09 20:45:20,000 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 20:45:20,001 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 20:45:20,001 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 20:45:20,001 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 20:45:20,001 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 20:45:24,845 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 20:45:24,863 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 20:45:24,863 - INFO - root - LLMClientManager: Doubao client initialized successfully
2025-11-09 20:45:24,864 - WARNING - root - LLMClientManager: client DeepSeek not available
2025-11-09 20:45:24,864 - WARNING - root - 无法切换到指定的客户端 DeepSeek，将使用默认客户端
2025-11-09 20:45:24,864 - INFO - root - 可用客户端: ['Gemini', 'Doubao']
2025-11-09 20:45:24,864 - INFO - root - 使用 LLM 模型: gemini-2.5-flash
2025-11-09 20:45:24,865 - INFO - root - 可用客户端: ['Gemini', 'Doubao']
2025-11-09 20:45:24,866 - INFO - root - === 运行配置 ===
2025-11-09 20:45:24,866 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 20:45:24,866 - INFO - root - 关键词: QAT
2025-11-09 20:45:24,866 - INFO - root - 查询: Quantization-Aware-Training
2025-11-09 20:45:24,867 - INFO - root - 排序: None
2025-11-09 20:45:24,870 - INFO - root - 最近天数: 180
2025-11-09 20:45:24,871 - INFO - root - 最大处理数量: 2
2025-11-09 20:45:24,872 - INFO - root - 保存图片: 是
2025-11-09 20:45:24,872 - INFO - root - 输出语言: 中文
2025-11-09 20:45:24,872 - INFO - root - 强制重新处理: 否
2025-11-09 20:45:24,873 - INFO - root - ====================
2025-11-09 20:45:24,873 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 20:45:24,874 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 20:45:32,073 - INFO - root - get_all_titles_from_web 
2025-11-09 20:45:32,074 - INFO - root - Page:0, Index:0, A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies, https://arxiv.org/pdf/2511.03201, 2025-11-05
2025-11-09 20:45:32,074 - INFO - root - Page:0, Index:1, FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error, https://arxiv.org/pdf/2511.02302, 2025-11-04
2025-11-09 20:45:32,074 - INFO - root - Page:0, Index:2, Outlier-Aware Post-Training Quantization for Image Super-Resolution, https://arxiv.org/pdf/2511.00682, 2025-11-01
2025-11-09 20:45:32,074 - INFO - root - Page:0, Index:3, Evaluation of Wafer-Scale SOT-MRAM for Analog Crossbar Array Applications, https://arxiv.org/pdf/2510.25853, 2025-11-03
2025-11-09 20:45:32,074 - INFO - root - Page:0, Index:4, Improving the Straight-Through Estimator with Zeroth-Order Information, https://arxiv.org/pdf/2510.23926, 2025-10-27
2025-11-09 20:45:32,075 - INFO - root - Page:0, Index:5, Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework, https://arxiv.org/pdf/2510.22243, 2025-10-25
2025-11-09 20:45:32,075 - INFO - root - Page:0, Index:6, TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge, https://arxiv.org/pdf/2510.21879, 2025-10-23
2025-11-09 20:45:32,076 - INFO - root - Page:0, Index:7, KARIPAP: Quantum-Inspired Tensor Network Compression of Large Language Models Using Infinite Projected Entangled Pair States and Tensor Renormalization Group, https://arxiv.org/pdf/2510.21844, 2025-10-22
2025-11-09 20:45:32,076 - INFO - root - Page:0, Index:8, A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization, https://arxiv.org/pdf/2510.21314, 2025-10-24
2025-11-09 20:45:32,076 - INFO - root - Page:0, Index:9, Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks, https://arxiv.org/pdf/2510.19760, 2025-10-22
2025-11-09 20:45:32,076 - INFO - root - Page:0, Index:10, CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training, https://arxiv.org/pdf/2510.18784, 2025-10-21
2025-11-09 20:45:32,078 - INFO - root - Page:0, Index:11, Mixed-Precision Quantization for Language Models: Techniques and Prospects, https://arxiv.org/pdf/2510.16805, 2025-10-19
2025-11-09 20:45:32,078 - INFO - root - Page:0, Index:12, SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation, https://arxiv.org/pdf/2510.16396, 2025-10-30
2025-11-09 20:45:32,078 - INFO - root - Page:0, Index:13, CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models, https://arxiv.org/pdf/2510.15962, 2025-10-11
2025-11-09 20:45:32,078 - INFO - root - Page:0, Index:14, SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization, https://arxiv.org/pdf/2510.15775, 2025-10-17
2025-11-09 20:45:32,079 - INFO - root - Page:0, Index:15, SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware, https://arxiv.org/pdf/2510.15542, 2025-10-31
2025-11-09 20:45:32,079 - INFO - root - Page:0, Index:16, GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework, https://arxiv.org/pdf/2510.15299, 2025-10-17
2025-11-09 20:45:32,080 - INFO - root - Page:0, Index:17, SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images, https://arxiv.org/pdf/2510.15072, 2025-10-16
2025-11-09 20:45:32,080 - INFO - root - Page:0, Index:18, FraQAT: Quantization Aware Training with Fractional bits, https://arxiv.org/pdf/2510.14823, 2025-10-16
2025-11-09 20:45:32,080 - INFO - root - Page:0, Index:19, Computing-In-Memory Aware Model Adaption For Edge Devices, https://arxiv.org/pdf/2510.14379, 2025-10-16
2025-11-09 20:45:32,080 - INFO - root - Page:0, Index:20, Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge, https://arxiv.org/pdf/2510.13760, 2025-11-04
2025-11-09 20:45:32,082 - INFO - root - Page:0, Index:21, NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models, https://arxiv.org/pdf/2510.13068, 2025-10-14
2025-11-09 20:45:32,082 - INFO - root - Page:0, Index:22, Detect Anything via Next Point Prediction, https://arxiv.org/pdf/2510.12798, 2025-10-14
2025-11-09 20:45:32,082 - INFO - root - Page:0, Index:23, AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model, https://arxiv.org/pdf/2510.11496, 2025-10-14
2025-11-09 20:45:32,085 - INFO - root - Page:0, Index:24, Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware, https://arxiv.org/pdf/2510.11484, 2025-10-13
2025-11-09 20:45:32,085 - INFO - root - Page:0, Index:25, SASER: Stego attacks on open-source LLMs, https://arxiv.org/pdf/2510.10486, 2025-10-12
2025-11-09 20:45:32,085 - INFO - root - Page:0, Index:26, Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition, https://arxiv.org/pdf/2510.09653, 2025-10-15
2025-11-09 20:45:32,086 - INFO - root - Page:0, Index:27, A Theoretically-Grounded Codebook for Digital Semantic Communications, https://arxiv.org/pdf/2510.07108, 2025-10-14
2025-11-09 20:45:32,086 - INFO - root - Page:0, Index:28, QuantDemoire: Quantization with Outlier Aware for Image Demoiréing, https://arxiv.org/pdf/2510.04066, 2025-10-05
2025-11-09 20:45:32,087 - INFO - root - Page:0, Index:29, Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization, https://arxiv.org/pdf/2510.03763, 2025-10-04
2025-11-09 20:45:32,088 - INFO - root - Page:0, Index:30, Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models, https://arxiv.org/pdf/2510.03274, 2025-09-27
2025-11-09 20:45:32,090 - INFO - root - Page:0, Index:31, PT$^2$-LLM: Post-Training Ternarization for Large Language Models, https://arxiv.org/pdf/2510.03267, 2025-09-26
2025-11-09 20:45:32,095 - INFO - root - Page:0, Index:32, Purrception: Variational Flow Matching for Vector-Quantized Image Generation, https://arxiv.org/pdf/2510.01478, 2025-10-01
2025-11-09 20:45:32,095 - INFO - root - Page:0, Index:33, Post-Training Quantization for Audio Diffusion Transformers, https://arxiv.org/pdf/2510.00313, 2025-09-30
2025-11-09 20:45:32,096 - INFO - root - Page:0, Index:34, Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling, https://arxiv.org/pdf/2510.00028, 2025-09-25
2025-11-09 20:45:32,096 - INFO - root - Page:0, Index:35, Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models, https://arxiv.org/pdf/2509.26436, 2025-09-30
2025-11-09 20:45:32,096 - INFO - root - Page:0, Index:36, Cat: Post-Training Quantization Error Reduction via Cluster-based Affine Transformation, https://arxiv.org/pdf/2509.26277, 2025-10-07
2025-11-09 20:45:32,097 - INFO - root - Page:0, Index:37, CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models, https://arxiv.org/pdf/2509.25996, 2025-09-30
2025-11-09 20:45:32,097 - INFO - root - Page:0, Index:38, Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications, https://arxiv.org/pdf/2509.25439, 2025-09-29
2025-11-09 20:45:32,097 - INFO - root - Page:0, Index:39, On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs, https://arxiv.org/pdf/2509.25214, 2025-09-22
2025-11-09 20:45:32,098 - INFO - root - Page:0, Index:40, VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning, https://arxiv.org/pdf/2509.24650, 2025-09-29
2025-11-09 20:45:32,098 - INFO - root - Page:0, Index:41, S$^2$NN: Sub-bit Spiking Neural Networks, https://arxiv.org/pdf/2509.24266, 2025-10-24
2025-11-09 20:45:32,101 - INFO - root - Page:0, Index:42, Tequila: Trapping-free Ternary Quantization for Large Language Models, https://arxiv.org/pdf/2509.23809, 2025-10-17
2025-11-09 20:45:32,103 - INFO - root - Page:0, Index:43, Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution, https://arxiv.org/pdf/2509.23774, 2025-09-30
2025-11-09 20:45:32,103 - INFO - root - Page:0, Index:44, RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization, https://arxiv.org/pdf/2509.23582, 2025-09-27
2025-11-09 20:45:32,103 - INFO - root - Page:0, Index:45, Beyond Outliers: A Study of Optimizers Under Quantization, https://arxiv.org/pdf/2509.23500, 2025-10-02
2025-11-09 20:45:32,104 - INFO - root - Page:0, Index:46, Compute-Optimal Quantization-Aware Training, https://arxiv.org/pdf/2509.22935, 2025-09-26
2025-11-09 20:45:32,104 - INFO - root - Page:0, Index:47, COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning, https://arxiv.org/pdf/2509.22075, 2025-10-06
2025-11-09 20:45:32,105 - INFO - root - Page:0, Index:48, SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models, https://arxiv.org/pdf/2509.21498, 2025-09-25
2025-11-09 20:45:32,105 - INFO - root - Page:0, Index:49, Quantized Visual Geometry Grounded Transformer, https://arxiv.org/pdf/2509.21302, 2025-09-29
2025-11-09 20:45:32,105 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 20:45:39,008 - INFO - root - get_all_titles_from_web 
2025-11-09 20:45:39,009 - INFO - root - Page:1, Index:0, Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy, https://arxiv.org/pdf/2509.21173, 2025-10-27
2025-11-09 20:45:39,009 - INFO - root - Page:1, Index:1, Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer, https://arxiv.org/pdf/2509.20854, 2025-09-25
2025-11-09 20:45:39,009 - INFO - root - Page:1, Index:2, TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection, https://arxiv.org/pdf/2509.18193, 2025-09-19
2025-11-09 20:45:39,009 - INFO - root - Page:1, Index:3, SBVR: Summation of BitVector Representation for Efficient LLM Quantization, https://arxiv.org/pdf/2509.18172, 2025-09-17
2025-11-09 20:45:39,010 - INFO - root - Page:1, Index:4, QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2509.17428, 2025-09-26
2025-11-09 20:45:39,010 - INFO - root - Page:1, Index:5, PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models, https://arxiv.org/pdf/2509.16989, 2025-10-28
2025-11-09 20:45:39,010 - INFO - root - Page:1, Index:6, MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training, https://arxiv.org/pdf/2509.15514, 2025-09-18
2025-11-09 20:45:39,011 - INFO - root - Page:1, Index:7, Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs, https://arxiv.org/pdf/2509.14391, 2025-09-17
2025-11-09 20:45:39,011 - INFO - root - Page:1, Index:8, Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization, https://arxiv.org/pdf/2509.13786, 2025-09-19
2025-11-09 20:45:39,011 - INFO - root - Page:1, Index:9, Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees, https://arxiv.org/pdf/2509.11054, 2025-09-13
2025-11-09 20:45:39,012 - INFO - root - Page:1, Index:10, SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models, https://arxiv.org/pdf/2509.09090, 2025-09-10
2025-11-09 20:45:39,012 - INFO - root - Page:1, Index:11, CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks with Entropy Regularization, https://arxiv.org/pdf/2509.08776, 2025-09-10
2025-11-09 20:45:39,012 - INFO - root - Page:1, Index:12, Explaining How Quantization Disparately Skews a Model, https://arxiv.org/pdf/2509.07222, 2025-09-08
2025-11-09 20:45:39,012 - INFO - root - Page:1, Index:13, LoaQ: Layer-wise Output Approximation Quantization, https://arxiv.org/pdf/2509.06297, 2025-09-07
2025-11-09 20:45:39,012 - INFO - root - Page:1, Index:14, FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving, https://arxiv.org/pdf/2509.06261, 2025-09-14
2025-11-09 20:45:39,014 - INFO - root - Page:1, Index:15, Sensitivity-Aware Post-Training Quantization for Deep Neural Networks, https://arxiv.org/pdf/2509.05576, 2025-09-05
2025-11-09 20:45:39,014 - INFO - root - Page:1, Index:16, SuperSNN: A Hardware-Aware Framework for Physically Realizable, High-Performance Superconducting Spiking Neural Network Chips, https://arxiv.org/pdf/2509.05532, 2025-09-05
2025-11-09 20:45:39,014 - INFO - root - Page:1, Index:17, Data-Augmented Quantization-Aware Knowledge Distillation, https://arxiv.org/pdf/2509.03850, 2025-09-03
2025-11-09 20:45:39,015 - INFO - root - Page:1, Index:18, DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling, https://arxiv.org/pdf/2509.03472, 2025-09-03
2025-11-09 20:45:39,015 - INFO - root - Page:1, Index:19, Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs, https://arxiv.org/pdf/2509.02017, 2025-09-02
2025-11-09 20:45:39,016 - INFO - root - Page:1, Index:20, Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling, https://arxiv.org/pdf/2509.01624, 2025-09-01
2025-11-09 20:45:39,016 - INFO - root - Page:1, Index:21, Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective, https://arxiv.org/pdf/2509.00859, 2025-08-31
2025-11-09 20:45:39,017 - INFO - root - Page:1, Index:22, Progressive Element-wise Gradient Estimation for Neural Network Quantization, https://arxiv.org/pdf/2509.00097, 2025-08-27
2025-11-09 20:45:39,017 - INFO - root - Page:1, Index:23, End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost, https://arxiv.org/pdf/2509.00031, 2025-09-29
2025-11-09 20:45:39,018 - INFO - root - Page:1, Index:24, Quantization Robustness to Input Degradations for Object Detection, https://arxiv.org/pdf/2508.19600, 2025-08-27
2025-11-09 20:45:39,019 - INFO - root - Page:1, Index:25, Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models, https://arxiv.org/pdf/2508.18609, 2025-08-27
2025-11-09 20:45:39,019 - INFO - root - Page:1, Index:26, AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration, https://arxiv.org/pdf/2508.18025, 2025-08-25
2025-11-09 20:45:39,019 - INFO - root - Page:1, Index:27, TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling, https://arxiv.org/pdf/2508.16790, 2025-08-22
2025-11-09 20:45:39,020 - INFO - root - Page:1, Index:28, A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering, https://arxiv.org/pdf/2508.16516, 2025-08-22
2025-11-09 20:45:39,020 - INFO - root - Page:1, Index:29, JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs, https://arxiv.org/pdf/2508.15468, 2025-08-21
2025-11-09 20:45:39,020 - INFO - root - Page:1, Index:30, MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation, https://arxiv.org/pdf/2508.15281, 2025-08-21
2025-11-09 20:45:39,020 - INFO - root - Page:1, Index:31, DLLMQuant: Quantizing Diffusion-based Large Language Models, https://arxiv.org/pdf/2508.14090, 2025-08-25
2025-11-09 20:45:39,021 - INFO - root - Page:1, Index:32, Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management, https://arxiv.org/pdf/2508.13905, 2025-08-19
2025-11-09 20:45:39,021 - INFO - root - Page:1, Index:33, XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads, https://arxiv.org/pdf/2508.13049, 2025-08-18
2025-11-09 20:45:39,023 - INFO - root - Page:1, Index:34, Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection, https://arxiv.org/pdf/2508.12230, 2025-08-17
2025-11-09 20:45:39,025 - INFO - root - Page:1, Index:35, TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks, https://arxiv.org/pdf/2508.12132, 2025-08-16
2025-11-09 20:45:39,026 - INFO - root - Page:1, Index:36, Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion, https://arxiv.org/pdf/2508.12094, 2025-10-13
2025-11-09 20:45:39,026 - INFO - root - Page:1, Index:37, Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware, https://arxiv.org/pdf/2508.11940, 2025-08-16
2025-11-09 20:45:39,026 - INFO - root - Page:1, Index:38, PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks, https://arxiv.org/pdf/2508.10557, 2025-08-15
2025-11-09 20:45:39,027 - INFO - root - Page:1, Index:39, MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI, https://arxiv.org/pdf/2508.09500, 2025-08-13
2025-11-09 20:45:39,027 - INFO - root - Page:1, Index:40, SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via Codebooks for recommender system, https://arxiv.org/pdf/2508.09090, 2025-08-12
2025-11-09 20:45:39,027 - INFO - root - Page:1, Index:41, Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models, https://arxiv.org/pdf/2508.06974, 2025-08-09
2025-11-09 20:45:39,027 - INFO - root - Page:1, Index:42, Efficient Deep Neural Receiver with Post-Training Quantization, https://arxiv.org/pdf/2508.06275, 2025-08-08
2025-11-09 20:45:39,028 - INFO - root - Page:1, Index:43, iFairy: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$, https://arxiv.org/pdf/2508.05571, 2025-08-16
2025-11-09 20:45:39,028 - INFO - root - Page:1, Index:44, S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation, https://arxiv.org/pdf/2508.04016, 2025-10-27
2025-11-09 20:45:39,028 - INFO - root - Page:1, Index:45, LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation, https://arxiv.org/pdf/2508.03485, 2025-09-23
2025-11-09 20:45:39,030 - INFO - root - Page:1, Index:46, VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation, https://arxiv.org/pdf/2508.03351, 2025-08-05
2025-11-09 20:45:39,030 - INFO - root - Page:1, Index:47, TF-MLPNet: Tiny Real-Time Neural Speech Separation, https://arxiv.org/pdf/2508.03047, 2025-08-04
2025-11-09 20:45:39,030 - INFO - root - Page:1, Index:48, SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation, https://arxiv.org/pdf/2508.01375, 2025-08-02
2025-11-09 20:45:39,031 - INFO - root - Page:1, Index:49, MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective, https://arxiv.org/pdf/2507.19131, 2025-07-25
2025-11-09 20:45:39,031 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 20:45:46,283 - INFO - root - get_all_titles_from_web 
2025-11-09 20:45:46,283 - INFO - root - Page:2, Index:0, Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction, https://arxiv.org/pdf/2507.17768, 2025-07-16
2025-11-09 20:45:46,284 - INFO - root - Page:2, Index:1, SiLQ: Simple Large Language Model Quantization-Aware Training, https://arxiv.org/pdf/2507.16933, 2025-07-22
2025-11-09 20:45:46,284 - INFO - root - Page:2, Index:2, Task-Specific Zero-shot Quantization-Aware Training for Object Detection, https://arxiv.org/pdf/2507.16782, 2025-07-22
2025-11-09 20:45:46,284 - INFO - root - Page:2, Index:3, TorchAO: PyTorch-Native Training-to-Serving Model Optimization, https://arxiv.org/pdf/2507.16099, 2025-07-21
2025-11-09 20:45:46,284 - INFO - root - Page:2, Index:4, SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models, https://arxiv.org/pdf/2507.14811, 2025-08-27
2025-11-09 20:45:46,285 - INFO - root - Page:2, Index:5, Apple Intelligence Foundation Language Models: Tech Report 2025, https://arxiv.org/pdf/2507.13575, 2025-08-27
2025-11-09 20:45:46,285 - INFO - root - Page:2, Index:6, Generative Multi-Target Cross-Domain Recommendation, https://arxiv.org/pdf/2507.12871, 2025-08-07
2025-11-09 20:45:46,285 - INFO - root - Page:2, Index:7, DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training, https://arxiv.org/pdf/2507.07149, 2025-07-09
2025-11-09 20:45:46,285 - INFO - root - Page:2, Index:8, Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics, https://arxiv.org/pdf/2507.07044, 2025-07-15
2025-11-09 20:45:46,285 - INFO - root - Page:2, Index:9, QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models, https://arxiv.org/pdf/2507.06079, 2025-07-08
2025-11-09 20:45:46,286 - INFO - root - Page:2, Index:10, GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field, https://arxiv.org/pdf/2507.05594, 2025-07-07
2025-11-09 20:45:46,290 - INFO - root - Page:2, Index:11, Hita: Holistic Tokenizer for Autoregressive Image Generation, https://arxiv.org/pdf/2507.02358, 2025-07-11
2025-11-09 20:45:46,291 - INFO - root - Page:2, Index:12, QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference, https://arxiv.org/pdf/2506.23934, 2025-06-30
2025-11-09 20:45:46,291 - INFO - root - Page:2, Index:13, FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization, https://arxiv.org/pdf/2506.23516, 2025-07-21
2025-11-09 20:45:46,292 - INFO - root - Page:2, Index:14, Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models, https://arxiv.org/pdf/2506.23025, 2025-06-28
2025-11-09 20:45:46,296 - INFO - root - Page:2, Index:15, Joint Quantization and Pruning Neural Networks Approach: A Case Study on FSO Receivers, https://arxiv.org/pdf/2506.20084, 2025-06-24
2025-11-09 20:45:46,297 - INFO - root - Page:2, Index:16, Single-step Diffusion for Image Compression at Ultra-Low Bitrates, https://arxiv.org/pdf/2506.16572, 2025-09-22
2025-11-09 20:45:46,298 - INFO - root - Page:2, Index:17, LittleBit: Ultra Low-Bit Quantization via Latent Factorization, https://arxiv.org/pdf/2506.13771, 2025-10-28
2025-11-09 20:45:46,298 - INFO - root - Page:2, Index:18, MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering, https://arxiv.org/pdf/2506.13755, 2025-06-16
2025-11-09 20:45:46,298 - INFO - root - Page:2, Index:19, EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization, https://arxiv.org/pdf/2506.13329, 2025-07-04
2025-11-09 20:45:46,299 - INFO - root - Page:2, Index:20, Towards Neural Audio Codec Source Parsing, https://arxiv.org/pdf/2506.12627, 2025-06-14
2025-11-09 20:45:46,299 - INFO - root - Page:2, Index:21, Quantizing Small-Scale State-Space Models for Edge AI, https://arxiv.org/pdf/2506.12480, 2025-06-14
2025-11-09 20:45:46,299 - INFO - root - Page:2, Index:22, EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction, https://arxiv.org/pdf/2506.12015, 2025-06-13
2025-11-09 20:45:46,300 - INFO - root - Page:2, Index:23, Compression Aware Certified Training, https://arxiv.org/pdf/2506.11992, 2025-06-13
2025-11-09 20:45:46,300 - INFO - root - Page:2, Index:24, GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers, https://arxiv.org/pdf/2506.11784, 2025-06-13
2025-11-09 20:45:46,300 - INFO - root - Page:2, Index:25, TruncQuant: Truncation-Ready Quantization for DNNs with Flexible Weight Bit Precision, https://arxiv.org/pdf/2506.11431, 2025-06-12
2025-11-09 20:45:46,300 - INFO - root - Page:2, Index:26, EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices, https://arxiv.org/pdf/2506.11093, 2025-06-05
2025-11-09 20:45:46,302 - INFO - root - Page:2, Index:27, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 20:45:46,303 - INFO - root - Page:2, Index:28, AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent, https://arxiv.org/pdf/2506.10205, 2025-06-11
2025-11-09 20:45:46,304 - INFO - root - Page:2, Index:29, Q-SAM2: Accurate Quantization for Segment Anything Model 2, https://arxiv.org/pdf/2506.09782, 2025-06-11
2025-11-09 20:45:46,304 - INFO - root - Page:2, Index:30, Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs, https://arxiv.org/pdf/2506.09104, 2025-06-10
2025-11-09 20:45:46,304 - INFO - root - Page:2, Index:31, Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU, https://arxiv.org/pdf/2506.08911, 2025-06-10
2025-11-09 20:45:46,304 - INFO - root - Page:2, Index:32, POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration, https://arxiv.org/pdf/2506.08785, 2025-06-10
2025-11-09 20:45:46,306 - INFO - root - Page:2, Index:33, MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts, https://arxiv.org/pdf/2506.07533, 2025-06-09
2025-11-09 20:45:46,306 - INFO - root - Page:2, Index:34, BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation, https://arxiv.org/pdf/2506.07530, 2025-06-09
2025-11-09 20:45:46,308 - INFO - root - Page:2, Index:35, RecGPT: A Foundation Model for Sequential Recommendation, https://arxiv.org/pdf/2506.06270, 2025-06-12
2025-11-09 20:45:46,309 - INFO - root - Page:2, Index:36, FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion, https://arxiv.org/pdf/2506.04648, 2025-06-05
2025-11-09 20:45:46,309 - INFO - root - Page:2, Index:37, BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing, https://arxiv.org/pdf/2506.03515, 2025-06-03
2025-11-09 20:45:46,311 - INFO - root - Page:2, Index:38, ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding, https://arxiv.org/pdf/2506.01853, 2025-06-02
2025-11-09 20:45:46,311 - INFO - root - Page:2, Index:39, Movable Antenna Enhanced Federated Fine-Tuning of Large Language Models via Hybrid Client Selection Optimization, https://arxiv.org/pdf/2506.00011, 2025-10-26
2025-11-09 20:45:46,311 - INFO - root - Page:2, Index:40, Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach, https://arxiv.org/pdf/2505.24721, 2025-05-30
2025-11-09 20:45:46,312 - INFO - root - Page:2, Index:41, GARLIC: GAussian Representation LearnIng for spaCe partitioning, https://arxiv.org/pdf/2505.24608, 2025-10-02
2025-11-09 20:45:46,312 - INFO - root - Page:2, Index:42, AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up, https://arxiv.org/pdf/2505.24584, 2025-08-18
2025-11-09 20:45:46,312 - INFO - root - Page:2, Index:43, Model-Preserving Adaptive Rounding, https://arxiv.org/pdf/2505.22988, 2025-09-25
2025-11-09 20:45:46,312 - INFO - root - Page:2, Index:44, Highly Efficient and Effective LLMs with Multi-Boolean Architectures, https://arxiv.org/pdf/2505.22811, 2025-10-03
2025-11-09 20:45:46,314 - INFO - root - Page:2, Index:45, Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning, https://arxiv.org/pdf/2505.21591, 2025-05-27
2025-11-09 20:45:46,317 - INFO - root - Page:2, Index:46, Frequency Composition for Compressed and Domain-Adaptive Neural Networks, https://arxiv.org/pdf/2505.20890, 2025-05-27
2025-11-09 20:45:46,319 - INFO - root - Page:2, Index:47, MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition, https://arxiv.org/pdf/2505.20744, 2025-10-27
2025-11-09 20:45:46,320 - INFO - root - Page:2, Index:48, Optimizing edge AI models on HPC systems with the edge in the loop, https://arxiv.org/pdf/2505.19995, 2025-05-26
2025-11-09 20:45:46,320 - INFO - root - Fetching page 4 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=150
2025-11-09 20:45:53,377 - INFO - root - get_all_titles_from_web 
2025-11-09 20:45:53,377 - INFO - root - Page:3, Index:0, DPASyn: Mechanism-Aware Drug Synergy Prediction via Dual Attention and Precision-Aware Quantization, https://arxiv.org/pdf/2505.19144, 2025-09-20
2025-11-09 20:45:53,377 - INFO - root - Page:3, Index:1, Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding, https://arxiv.org/pdf/2505.18758, 2025-05-24
2025-11-09 20:45:53,380 - INFO - root - Page:3, Index:2, Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization, https://arxiv.org/pdf/2505.18113, 2025-05-23
2025-11-09 20:45:53,381 - INFO - root - Page:3, Index:3, Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs, https://arxiv.org/pdf/2505.17662, 2025-09-19
2025-11-09 20:45:53,382 - INFO - root - Page:3, Index:4, FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design, https://arxiv.org/pdf/2505.16335, 2025-05-22
2025-11-09 20:45:53,383 - INFO - root - Page:3, Index:5, Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control, https://arxiv.org/pdf/2505.15304, 2025-05-30
2025-11-09 20:45:53,383 - INFO - root - Page:3, Index:6, Scaling Law for Quantization-Aware Training, https://arxiv.org/pdf/2505.14302, 2025-05-20
2025-11-09 20:45:53,387 - INFO - root - Page:3, Index:7, Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs, https://arxiv.org/pdf/2505.13060, 2025-05-19
2025-11-09 20:45:53,393 - INFO - root - Page:3, Index:8, Deep Unfolding with Kernel-based Quantization in MIMO Detection, https://arxiv.org/pdf/2505.12736, 2025-05-19
2025-11-09 20:45:53,396 - INFO - root - Page:3, Index:9, FedHQ: Hybrid Runtime Quantization for Federated Learning, https://arxiv.org/pdf/2505.11982, 2025-05-17
2025-11-09 20:45:53,396 - INFO - root - Page:3, Index:10, QVGen: Pushing the Limit of Quantized Video Generative Models, https://arxiv.org/pdf/2505.11497, 2025-09-27
2025-11-09 20:45:53,401 - INFO - root - Page:3, Index:11, EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes, https://arxiv.org/pdf/2505.10787, 2025-05-15
2025-11-09 20:45:53,402 - INFO - root - Page:3, Index:12, ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition, https://arxiv.org/pdf/2505.08981, 2025-05-13
2025-11-09 20:45:53,403 - INFO - root - Page:3, Index:13, PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs, https://arxiv.org/pdf/2505.03254, 2025-08-06
2025-11-09 20:45:53,404 - INFO - root - Page:3, Index:14, ICQuant: Index Coding enables Low-bit LLM Quantization, https://arxiv.org/pdf/2505.00850, 2025-08-24
2025-11-09 20:45:53,404 - INFO - root - Page:3, Index:15, Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining, https://arxiv.org/pdf/2504.13932, 2025-07-30
2025-11-09 20:45:53,405 - INFO - root - Page:3, Index:16, Achieving binary weight and activation for LLMs using Post-Training Quantization, https://arxiv.org/pdf/2504.05352, 2025-06-30
2025-11-09 20:45:53,423 - INFO - root - Page:3, Index:17, Wireless Hearables With Programmable Speech AI Accelerators, https://arxiv.org/pdf/2503.18698, 2025-10-21
2025-11-09 20:45:53,424 - INFO - root - Page:3, Index:18, GranQ: Efficient Channel-wise Quantization via Vectorized Pre-Scaling for Zero-Shot QAT, https://arxiv.org/pdf/2503.18339, 2025-10-15
2025-11-09 20:45:53,425 - INFO - root - Page:3, Index:19, CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting, https://arxiv.org/pdf/2503.12836, 2025-09-29
2025-11-09 20:45:53,426 - INFO - root - Page:3, Index:20, AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model, https://arxiv.org/pdf/2503.03088, 2025-07-11
2025-11-09 20:45:53,427 - INFO - root - Page:3, Index:21, Teaching Metric Distance to Discrete Autoregressive Language Models, https://arxiv.org/pdf/2503.02379, 2025-10-07
2025-11-09 20:45:53,428 - INFO - root - Page:3, Index:22, Regularization-based Framework for Quantization-, Fault- and Variability-Aware Training, https://arxiv.org/pdf/2503.01297, 2025-07-12
2025-11-09 20:45:53,428 - INFO - root - Fetching page 5 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=200
2025-11-09 20:45:59,869 - INFO - root - get_all_titles_from_web 
2025-11-09 20:45:59,869 - INFO - root - Page:4, Index:0, Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models, https://arxiv.org/pdf/2502.15799, 2025-06-29
2025-11-09 20:45:59,872 - INFO - root - Page:4, Index:1, Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer, https://arxiv.org/pdf/2502.15779, 2025-09-01
2025-11-09 20:45:59,872 - INFO - root - Page:4, Index:2, RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models, https://arxiv.org/pdf/2502.09003, 2025-06-05
2025-11-09 20:45:59,873 - INFO - root - Page:4, Index:3, Spatial Degradation-Aware and Temporal Consistent Diffusion Model for Compressed Video Super-Resolution, https://arxiv.org/pdf/2502.07381, 2025-06-27
2025-11-09 20:45:59,873 - INFO - root - Page:4, Index:4, QuEST: Stable Training of LLMs with 1-Bit Weights and Activations, https://arxiv.org/pdf/2502.05003, 2025-06-10
2025-11-09 20:45:59,873 - INFO - root - Page:4, Index:5, HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs, https://arxiv.org/pdf/2501.02625, 2025-11-05
2025-11-09 20:45:59,874 - INFO - root - Page:4, Index:6, TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction, https://arxiv.org/pdf/2412.16919, 2025-08-08
2025-11-09 20:45:59,875 - INFO - root - Page:4, Index:7, Training Multi-Layer Binary Neural Networks With Local Binary Error Signals, https://arxiv.org/pdf/2412.00119, 2025-08-05
2025-11-09 20:45:59,876 - INFO - root - Page:4, Index:8, Scaling Large-scale GNN Training to Thousands of Processors on CPU-based Supercomputers, https://arxiv.org/pdf/2411.16025, 2025-05-26
2025-11-09 20:45:59,877 - INFO - root - Page:4, Index:9, HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization, https://arxiv.org/pdf/2411.06581, 2025-05-16
2025-11-09 20:45:59,877 - INFO - root - Fetching page 6 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=250
2025-11-09 20:46:06,656 - INFO - root - get_all_titles_from_web 
2025-11-09 20:46:06,657 - INFO - root - Page:5, Index:0, Gradient-Free Training of Quantized Neural Networks, https://arxiv.org/pdf/2410.09734, 2025-09-29
2025-11-09 20:46:06,657 - INFO - root - Page:5, Index:1, QT-DoG: Quantization-aware Training for Domain Generalization, https://arxiv.org/pdf/2410.06020, 2025-06-26
2025-11-09 20:46:06,657 - INFO - root - Page:5, Index:2, Constraint Guided Model Quantization of Neural Networks, https://arxiv.org/pdf/2409.20138, 2025-09-12
2025-11-09 20:46:06,657 - INFO - root - Page:5, Index:3, Accumulator-Aware Post-Training Quantization for Large Language Models, https://arxiv.org/pdf/2409.17092, 2025-07-30
2025-11-09 20:46:06,658 - INFO - root - Page:5, Index:4, DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation, https://arxiv.org/pdf/2409.14307, 2025-07-09
2025-11-09 20:46:06,658 - INFO - root - Page:5, Index:5, Robust Training of Neural Networks at Arbitrary Precision and Sparsity, https://arxiv.org/pdf/2409.09245, 2025-09-23
2025-11-09 20:46:06,658 - INFO - root - Page:5, Index:6, VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing, https://arxiv.org/pdf/2408.05758, 2025-05-27
2025-11-09 20:46:06,659 - INFO - root - Page:5, Index:7, DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers, https://arxiv.org/pdf/2408.03291, 2025-06-19
2025-11-09 20:46:06,659 - INFO - root - Page:5, Index:8, Temporal Feature Matters: A Framework for Diffusion Model Quantization, https://arxiv.org/pdf/2407.19547, 2025-07-11
2025-11-09 20:46:06,659 - INFO - root - Page:5, Index:9, EfficientQAT: Efficient Quantization-Aware Training for Large Language Models, https://arxiv.org/pdf/2407.11062, 2025-05-19
2025-11-09 20:46:06,660 - INFO - root - Page:5, Index:10, Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT, https://arxiv.org/pdf/2407.11041, 2025-10-31
2025-11-09 20:46:06,661 - INFO - root - Fetching page 7 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=300
2025-11-09 20:46:14,365 - INFO - root - get_all_titles_from_web 
2025-11-09 20:46:14,365 - INFO - root - Page:6, Index:0, BoA: Attention-aware Post-training Quantization without Backpropagation, https://arxiv.org/pdf/2406.13474, 2025-06-06
2025-11-09 20:46:14,366 - INFO - root - Page:6, Index:1, A Rescaling-Invariant Lipschitz Bound Based on Path-Metrics for Modern ReLU Network Parameterizations, https://arxiv.org/pdf/2405.15006, 2025-06-13
2025-11-09 20:46:14,366 - INFO - root - Page:6, Index:2, Scheduling Weight Transitions for Quantization-Aware Training, https://arxiv.org/pdf/2404.19248, 2025-10-01
2025-11-09 20:46:14,366 - INFO - root - Page:6, Index:3, GPTVQ: The Blessing of Dimensionality for LLM Quantization, https://arxiv.org/pdf/2402.15319, 2025-06-03
2025-11-09 20:46:14,367 - INFO - root - Fetching page 8 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=350
2025-11-09 20:46:20,769 - INFO - root - get_all_titles_from_web 
2025-11-09 20:46:20,769 - INFO - root - Page:7, Index:0, Squat: Quant Small Language Models on the Edge, https://arxiv.org/pdf/2402.10787, 2025-07-01
2025-11-09 20:46:20,770 - INFO - root - Page:7, Index:1, L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2402.04902, 2025-07-21
2025-11-09 20:46:20,770 - INFO - root - Fetching page 9 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=400
2025-11-09 20:46:27,236 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 20:46:27,237 - INFO - root - File already exists, skipping download: d:\ChatPaper\academic Papers\Quantization-Aware-Training\A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat.pdf
2025-11-09 20:46:27,244 - INFO - root - File already exists, skipping download: d:\ChatPaper\academic Papers\Quantization-Aware-Training\FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error.pdf
2025-11-09 20:46:27,245 - INFO - root - 跳过已处理论文 A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies：d:\ChatPaper\academic Papers\Quantization-Aware-Training\A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat.pdf
2025-11-09 20:46:27,246 - INFO - root - 跳过已处理论文 FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error：d:\ChatPaper\academic Papers\Quantization-Aware-Training\FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error.pdf
2025-11-09 20:46:27,247 - INFO - root - summary time: 72.68 seconds
2025-11-09 20:46:45,952 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 20:46:45,953 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 20:46:45,954 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 20:46:46,795 - INFO - root - LLMClientManager: 指定使用客户端: DeepSeek
2025-11-09 20:46:46,796 - WARNING - root - DeepSeekClient: API key not provided. LLM disabled.
2025-11-09 20:46:46,796 - ERROR - root - LLMClientManager: 指定的客户端 DeepSeek 初始化失败
2025-11-09 20:46:46,796 - WARNING - root - LLMClientManager: 指定的客户端 DeepSeek 不可用，将尝试其他客户端
2025-11-09 20:46:47,623 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 20:46:49,542 - INFO - root - GeminiClient: initialized model gemini-2.5-flash
2025-11-09 20:46:49,542 - INFO - root - LLMClientManager: Gemini client initialized successfully
2025-11-09 20:46:49,543 - INFO - root - LLMClientManager: using Gemini as default client
2025-11-09 20:46:49,543 - WARNING - root - KimiClient: API key not provided. LLM disabled.
2025-11-09 20:46:49,543 - WARNING - root - LLMClientManager: Kimi client initialization failed
2025-11-09 20:46:49,543 - WARNING - root - QwenClient: API key not provided. LLM disabled.
2025-11-09 20:46:49,544 - WARNING - root - LLMClientManager: Qwen client initialization failed
2025-11-09 20:46:49,544 - INFO - root - DoubaoClient: API key found: 9f9e270e-b...
2025-11-09 20:46:54,056 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 20:46:54,066 - INFO - root - DoubaoClient: initialized successfully with model doubao-seed-1-6-lite-251015
2025-11-09 20:46:54,066 - INFO - root - LLMClientManager: Doubao client initialized successfully
2025-11-09 20:46:54,068 - WARNING - root - LLMClientManager: client DeepSeek not available
2025-11-09 20:46:54,068 - WARNING - root - 无法切换到指定的客户端 DeepSeek，将使用默认客户端
2025-11-09 20:46:54,068 - INFO - root - 可用客户端: ['Gemini', 'Doubao']
2025-11-09 20:46:54,069 - INFO - root - 使用 LLM 模型: gemini-2.5-flash
2025-11-09 20:46:54,069 - INFO - root - 可用客户端: ['Gemini', 'Doubao']
2025-11-09 20:46:54,069 - INFO - root - === 运行配置 ===
2025-11-09 20:46:54,070 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 20:46:54,070 - INFO - root - 关键词: QAT
2025-11-09 20:46:54,070 - INFO - root - 查询: Quantization-Aware-Training
2025-11-09 20:46:54,070 - INFO - root - 排序: None
2025-11-09 20:46:54,071 - INFO - root - 最近天数: 180
2025-11-09 20:46:54,071 - INFO - root - 最大处理数量: 2
2025-11-09 20:46:54,071 - INFO - root - 保存图片: 是
2025-11-09 20:46:54,072 - INFO - root - 输出语言: 中文
2025-11-09 20:46:54,072 - INFO - root - 强制重新处理: 否
2025-11-09 20:46:54,072 - INFO - root - ====================
2025-11-09 20:46:54,072 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 20:46:54,073 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 20:47:01,095 - INFO - root - get_all_titles_from_web 
2025-11-09 20:47:01,095 - INFO - root - Page:0, Index:0, A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies, https://arxiv.org/pdf/2511.03201, 2025-11-05
2025-11-09 20:47:01,096 - INFO - root - Page:0, Index:1, FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error, https://arxiv.org/pdf/2511.02302, 2025-11-04
2025-11-09 20:47:01,096 - INFO - root - Page:0, Index:2, Outlier-Aware Post-Training Quantization for Image Super-Resolution, https://arxiv.org/pdf/2511.00682, 2025-11-01
2025-11-09 20:47:01,096 - INFO - root - Page:0, Index:3, Evaluation of Wafer-Scale SOT-MRAM for Analog Crossbar Array Applications, https://arxiv.org/pdf/2510.25853, 2025-11-03
2025-11-09 20:47:01,097 - INFO - root - Page:0, Index:4, Improving the Straight-Through Estimator with Zeroth-Order Information, https://arxiv.org/pdf/2510.23926, 2025-10-27
2025-11-09 20:47:01,097 - INFO - root - Page:0, Index:5, Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework, https://arxiv.org/pdf/2510.22243, 2025-10-25
2025-11-09 20:47:01,097 - INFO - root - Page:0, Index:6, TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge, https://arxiv.org/pdf/2510.21879, 2025-10-23
2025-11-09 20:47:01,097 - INFO - root - Page:0, Index:7, KARIPAP: Quantum-Inspired Tensor Network Compression of Large Language Models Using Infinite Projected Entangled Pair States and Tensor Renormalization Group, https://arxiv.org/pdf/2510.21844, 2025-10-22
2025-11-09 20:47:01,098 - INFO - root - Page:0, Index:8, A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization, https://arxiv.org/pdf/2510.21314, 2025-10-24
2025-11-09 20:47:01,099 - INFO - root - Page:0, Index:9, Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks, https://arxiv.org/pdf/2510.19760, 2025-10-22
2025-11-09 20:47:01,099 - INFO - root - Page:0, Index:10, CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training, https://arxiv.org/pdf/2510.18784, 2025-10-21
2025-11-09 20:47:01,099 - INFO - root - Page:0, Index:11, Mixed-Precision Quantization for Language Models: Techniques and Prospects, https://arxiv.org/pdf/2510.16805, 2025-10-19
2025-11-09 20:47:01,101 - INFO - root - Page:0, Index:12, SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation, https://arxiv.org/pdf/2510.16396, 2025-10-30
2025-11-09 20:47:01,102 - INFO - root - Page:0, Index:13, CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models, https://arxiv.org/pdf/2510.15962, 2025-10-11
2025-11-09 20:47:01,102 - INFO - root - Page:0, Index:14, SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization, https://arxiv.org/pdf/2510.15775, 2025-10-17
2025-11-09 20:47:01,102 - INFO - root - Page:0, Index:15, SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware, https://arxiv.org/pdf/2510.15542, 2025-10-31
2025-11-09 20:47:01,103 - INFO - root - Page:0, Index:16, GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework, https://arxiv.org/pdf/2510.15299, 2025-10-17
2025-11-09 20:47:01,103 - INFO - root - Page:0, Index:17, SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images, https://arxiv.org/pdf/2510.15072, 2025-10-16
2025-11-09 20:47:01,103 - INFO - root - Page:0, Index:18, FraQAT: Quantization Aware Training with Fractional bits, https://arxiv.org/pdf/2510.14823, 2025-10-16
2025-11-09 20:47:01,104 - INFO - root - Page:0, Index:19, Computing-In-Memory Aware Model Adaption For Edge Devices, https://arxiv.org/pdf/2510.14379, 2025-10-16
2025-11-09 20:47:01,104 - INFO - root - Page:0, Index:20, Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge, https://arxiv.org/pdf/2510.13760, 2025-11-04
2025-11-09 20:47:01,104 - INFO - root - Page:0, Index:21, NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models, https://arxiv.org/pdf/2510.13068, 2025-10-14
2025-11-09 20:47:01,105 - INFO - root - Page:0, Index:22, Detect Anything via Next Point Prediction, https://arxiv.org/pdf/2510.12798, 2025-10-14
2025-11-09 20:47:01,105 - INFO - root - Page:0, Index:23, AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model, https://arxiv.org/pdf/2510.11496, 2025-10-14
2025-11-09 20:47:01,105 - INFO - root - Page:0, Index:24, Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware, https://arxiv.org/pdf/2510.11484, 2025-10-13
2025-11-09 20:47:01,106 - INFO - root - Page:0, Index:25, SASER: Stego attacks on open-source LLMs, https://arxiv.org/pdf/2510.10486, 2025-10-12
2025-11-09 20:47:01,106 - INFO - root - Page:0, Index:26, Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition, https://arxiv.org/pdf/2510.09653, 2025-10-15
2025-11-09 20:47:01,106 - INFO - root - Page:0, Index:27, A Theoretically-Grounded Codebook for Digital Semantic Communications, https://arxiv.org/pdf/2510.07108, 2025-10-14
2025-11-09 20:47:01,107 - INFO - root - Page:0, Index:28, QuantDemoire: Quantization with Outlier Aware for Image Demoiréing, https://arxiv.org/pdf/2510.04066, 2025-10-05
2025-11-09 20:47:01,108 - INFO - root - Page:0, Index:29, Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization, https://arxiv.org/pdf/2510.03763, 2025-10-04
2025-11-09 20:47:01,109 - INFO - root - Page:0, Index:30, Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models, https://arxiv.org/pdf/2510.03274, 2025-09-27
2025-11-09 20:47:01,110 - INFO - root - Page:0, Index:31, PT$^2$-LLM: Post-Training Ternarization for Large Language Models, https://arxiv.org/pdf/2510.03267, 2025-09-26
2025-11-09 20:47:01,110 - INFO - root - Page:0, Index:32, Purrception: Variational Flow Matching for Vector-Quantized Image Generation, https://arxiv.org/pdf/2510.01478, 2025-10-01
2025-11-09 20:47:01,110 - INFO - root - Page:0, Index:33, Post-Training Quantization for Audio Diffusion Transformers, https://arxiv.org/pdf/2510.00313, 2025-09-30
2025-11-09 20:47:01,111 - INFO - root - Page:0, Index:34, Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling, https://arxiv.org/pdf/2510.00028, 2025-09-25
2025-11-09 20:47:01,111 - INFO - root - Page:0, Index:35, Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models, https://arxiv.org/pdf/2509.26436, 2025-09-30
2025-11-09 20:47:01,111 - INFO - root - Page:0, Index:36, Cat: Post-Training Quantization Error Reduction via Cluster-based Affine Transformation, https://arxiv.org/pdf/2509.26277, 2025-10-07
2025-11-09 20:47:01,112 - INFO - root - Page:0, Index:37, CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models, https://arxiv.org/pdf/2509.25996, 2025-09-30
2025-11-09 20:47:01,112 - INFO - root - Page:0, Index:38, Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications, https://arxiv.org/pdf/2509.25439, 2025-09-29
2025-11-09 20:47:01,112 - INFO - root - Page:0, Index:39, On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs, https://arxiv.org/pdf/2509.25214, 2025-09-22
2025-11-09 20:47:01,114 - INFO - root - Page:0, Index:40, VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning, https://arxiv.org/pdf/2509.24650, 2025-09-29
2025-11-09 20:47:01,114 - INFO - root - Page:0, Index:41, S$^2$NN: Sub-bit Spiking Neural Networks, https://arxiv.org/pdf/2509.24266, 2025-10-24
2025-11-09 20:47:01,114 - INFO - root - Page:0, Index:42, Tequila: Trapping-free Ternary Quantization for Large Language Models, https://arxiv.org/pdf/2509.23809, 2025-10-17
2025-11-09 20:47:01,114 - INFO - root - Page:0, Index:43, Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution, https://arxiv.org/pdf/2509.23774, 2025-09-30
2025-11-09 20:47:01,116 - INFO - root - Page:0, Index:44, RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization, https://arxiv.org/pdf/2509.23582, 2025-09-27
2025-11-09 20:47:01,117 - INFO - root - Page:0, Index:45, Beyond Outliers: A Study of Optimizers Under Quantization, https://arxiv.org/pdf/2509.23500, 2025-10-02
2025-11-09 20:47:01,117 - INFO - root - Page:0, Index:46, Compute-Optimal Quantization-Aware Training, https://arxiv.org/pdf/2509.22935, 2025-09-26
2025-11-09 20:47:01,117 - INFO - root - Page:0, Index:47, COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning, https://arxiv.org/pdf/2509.22075, 2025-10-06
2025-11-09 20:47:01,117 - INFO - root - Page:0, Index:48, SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models, https://arxiv.org/pdf/2509.21498, 2025-09-25
2025-11-09 20:47:01,119 - INFO - root - Page:0, Index:49, Quantized Visual Geometry Grounded Transformer, https://arxiv.org/pdf/2509.21302, 2025-09-29
2025-11-09 20:47:01,119 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 20:47:07,965 - INFO - root - get_all_titles_from_web 
2025-11-09 20:47:07,965 - INFO - root - Page:1, Index:0, Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy, https://arxiv.org/pdf/2509.21173, 2025-10-27
2025-11-09 20:47:07,967 - INFO - root - Page:1, Index:1, Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer, https://arxiv.org/pdf/2509.20854, 2025-09-25
2025-11-09 20:47:07,967 - INFO - root - Page:1, Index:2, TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection, https://arxiv.org/pdf/2509.18193, 2025-09-19
2025-11-09 20:47:07,967 - INFO - root - Page:1, Index:3, SBVR: Summation of BitVector Representation for Efficient LLM Quantization, https://arxiv.org/pdf/2509.18172, 2025-09-17
2025-11-09 20:47:07,967 - INFO - root - Page:1, Index:4, QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2509.17428, 2025-09-26
2025-11-09 20:47:07,968 - INFO - root - Page:1, Index:5, PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models, https://arxiv.org/pdf/2509.16989, 2025-10-28
2025-11-09 20:47:07,968 - INFO - root - Page:1, Index:6, MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training, https://arxiv.org/pdf/2509.15514, 2025-09-18
2025-11-09 20:47:07,968 - INFO - root - Page:1, Index:7, Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs, https://arxiv.org/pdf/2509.14391, 2025-09-17
2025-11-09 20:47:07,969 - INFO - root - Page:1, Index:8, Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization, https://arxiv.org/pdf/2509.13786, 2025-09-19
2025-11-09 20:47:07,969 - INFO - root - Page:1, Index:9, Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees, https://arxiv.org/pdf/2509.11054, 2025-09-13
2025-11-09 20:47:07,969 - INFO - root - Page:1, Index:10, SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models, https://arxiv.org/pdf/2509.09090, 2025-09-10
2025-11-09 20:47:07,970 - INFO - root - Page:1, Index:11, CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks with Entropy Regularization, https://arxiv.org/pdf/2509.08776, 2025-09-10
2025-11-09 20:47:07,970 - INFO - root - Page:1, Index:12, Explaining How Quantization Disparately Skews a Model, https://arxiv.org/pdf/2509.07222, 2025-09-08
2025-11-09 20:47:07,970 - INFO - root - Page:1, Index:13, LoaQ: Layer-wise Output Approximation Quantization, https://arxiv.org/pdf/2509.06297, 2025-09-07
2025-11-09 20:47:07,970 - INFO - root - Page:1, Index:14, FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving, https://arxiv.org/pdf/2509.06261, 2025-09-14
2025-11-09 20:47:07,971 - INFO - root - Page:1, Index:15, Sensitivity-Aware Post-Training Quantization for Deep Neural Networks, https://arxiv.org/pdf/2509.05576, 2025-09-05
2025-11-09 20:47:07,971 - INFO - root - Page:1, Index:16, SuperSNN: A Hardware-Aware Framework for Physically Realizable, High-Performance Superconducting Spiking Neural Network Chips, https://arxiv.org/pdf/2509.05532, 2025-09-05
2025-11-09 20:47:07,973 - INFO - root - Page:1, Index:17, Data-Augmented Quantization-Aware Knowledge Distillation, https://arxiv.org/pdf/2509.03850, 2025-09-03
2025-11-09 20:47:07,975 - INFO - root - Page:1, Index:18, DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling, https://arxiv.org/pdf/2509.03472, 2025-09-03
2025-11-09 20:47:07,976 - INFO - root - Page:1, Index:19, Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs, https://arxiv.org/pdf/2509.02017, 2025-09-02
2025-11-09 20:47:07,977 - INFO - root - Page:1, Index:20, Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling, https://arxiv.org/pdf/2509.01624, 2025-09-01
2025-11-09 20:47:07,977 - INFO - root - Page:1, Index:21, Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective, https://arxiv.org/pdf/2509.00859, 2025-08-31
2025-11-09 20:47:07,978 - INFO - root - Page:1, Index:22, Progressive Element-wise Gradient Estimation for Neural Network Quantization, https://arxiv.org/pdf/2509.00097, 2025-08-27
2025-11-09 20:47:07,978 - INFO - root - Page:1, Index:23, End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost, https://arxiv.org/pdf/2509.00031, 2025-09-29
2025-11-09 20:47:07,978 - INFO - root - Page:1, Index:24, Quantization Robustness to Input Degradations for Object Detection, https://arxiv.org/pdf/2508.19600, 2025-08-27
2025-11-09 20:47:07,978 - INFO - root - Page:1, Index:25, Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models, https://arxiv.org/pdf/2508.18609, 2025-08-27
2025-11-09 20:47:07,979 - INFO - root - Page:1, Index:26, AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration, https://arxiv.org/pdf/2508.18025, 2025-08-25
2025-11-09 20:47:07,979 - INFO - root - Page:1, Index:27, TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling, https://arxiv.org/pdf/2508.16790, 2025-08-22
2025-11-09 20:47:07,979 - INFO - root - Page:1, Index:28, A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering, https://arxiv.org/pdf/2508.16516, 2025-08-22
2025-11-09 20:47:07,979 - INFO - root - Page:1, Index:29, JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs, https://arxiv.org/pdf/2508.15468, 2025-08-21
2025-11-09 20:47:07,980 - INFO - root - Page:1, Index:30, MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation, https://arxiv.org/pdf/2508.15281, 2025-08-21
2025-11-09 20:47:07,980 - INFO - root - Page:1, Index:31, DLLMQuant: Quantizing Diffusion-based Large Language Models, https://arxiv.org/pdf/2508.14090, 2025-08-25
2025-11-09 20:47:07,980 - INFO - root - Page:1, Index:32, Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management, https://arxiv.org/pdf/2508.13905, 2025-08-19
2025-11-09 20:47:07,980 - INFO - root - Page:1, Index:33, XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads, https://arxiv.org/pdf/2508.13049, 2025-08-18
2025-11-09 20:47:07,981 - INFO - root - Page:1, Index:34, Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection, https://arxiv.org/pdf/2508.12230, 2025-08-17
2025-11-09 20:47:07,981 - INFO - root - Page:1, Index:35, TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks, https://arxiv.org/pdf/2508.12132, 2025-08-16
2025-11-09 20:47:07,981 - INFO - root - Page:1, Index:36, Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion, https://arxiv.org/pdf/2508.12094, 2025-10-13
2025-11-09 20:47:07,983 - INFO - root - Page:1, Index:37, Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware, https://arxiv.org/pdf/2508.11940, 2025-08-16
2025-11-09 20:47:07,983 - INFO - root - Page:1, Index:38, PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks, https://arxiv.org/pdf/2508.10557, 2025-08-15
2025-11-09 20:47:07,983 - INFO - root - Page:1, Index:39, MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI, https://arxiv.org/pdf/2508.09500, 2025-08-13
2025-11-09 20:47:07,985 - INFO - root - Page:1, Index:40, SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via Codebooks for recommender system, https://arxiv.org/pdf/2508.09090, 2025-08-12
2025-11-09 20:47:07,985 - INFO - root - Page:1, Index:41, Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models, https://arxiv.org/pdf/2508.06974, 2025-08-09
2025-11-09 20:47:07,985 - INFO - root - Page:1, Index:42, Efficient Deep Neural Receiver with Post-Training Quantization, https://arxiv.org/pdf/2508.06275, 2025-08-08
2025-11-09 20:47:07,985 - INFO - root - Page:1, Index:43, iFairy: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$, https://arxiv.org/pdf/2508.05571, 2025-08-16
2025-11-09 20:47:07,986 - INFO - root - Page:1, Index:44, S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation, https://arxiv.org/pdf/2508.04016, 2025-10-27
2025-11-09 20:47:07,986 - INFO - root - Page:1, Index:45, LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation, https://arxiv.org/pdf/2508.03485, 2025-09-23
2025-11-09 20:47:07,986 - INFO - root - Page:1, Index:46, VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation, https://arxiv.org/pdf/2508.03351, 2025-08-05
2025-11-09 20:47:07,986 - INFO - root - Page:1, Index:47, TF-MLPNet: Tiny Real-Time Neural Speech Separation, https://arxiv.org/pdf/2508.03047, 2025-08-04
2025-11-09 20:47:07,987 - INFO - root - Page:1, Index:48, SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation, https://arxiv.org/pdf/2508.01375, 2025-08-02
2025-11-09 20:47:07,987 - INFO - root - Page:1, Index:49, MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective, https://arxiv.org/pdf/2507.19131, 2025-07-25
2025-11-09 20:47:07,989 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 20:47:14,670 - INFO - root - get_all_titles_from_web 
2025-11-09 20:47:14,670 - INFO - root - Page:2, Index:0, Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction, https://arxiv.org/pdf/2507.17768, 2025-07-16
2025-11-09 20:47:14,671 - INFO - root - Page:2, Index:1, SiLQ: Simple Large Language Model Quantization-Aware Training, https://arxiv.org/pdf/2507.16933, 2025-07-22
2025-11-09 20:47:14,671 - INFO - root - Page:2, Index:2, Task-Specific Zero-shot Quantization-Aware Training for Object Detection, https://arxiv.org/pdf/2507.16782, 2025-07-22
2025-11-09 20:47:14,671 - INFO - root - Page:2, Index:3, TorchAO: PyTorch-Native Training-to-Serving Model Optimization, https://arxiv.org/pdf/2507.16099, 2025-07-21
2025-11-09 20:47:14,672 - INFO - root - Page:2, Index:4, SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models, https://arxiv.org/pdf/2507.14811, 2025-08-27
2025-11-09 20:47:14,672 - INFO - root - Page:2, Index:5, Apple Intelligence Foundation Language Models: Tech Report 2025, https://arxiv.org/pdf/2507.13575, 2025-08-27
2025-11-09 20:47:14,672 - INFO - root - Page:2, Index:6, Generative Multi-Target Cross-Domain Recommendation, https://arxiv.org/pdf/2507.12871, 2025-08-07
2025-11-09 20:47:14,672 - INFO - root - Page:2, Index:7, DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training, https://arxiv.org/pdf/2507.07149, 2025-07-09
2025-11-09 20:47:14,673 - INFO - root - Page:2, Index:8, Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics, https://arxiv.org/pdf/2507.07044, 2025-07-15
2025-11-09 20:47:14,673 - INFO - root - Page:2, Index:9, QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models, https://arxiv.org/pdf/2507.06079, 2025-07-08
2025-11-09 20:47:14,673 - INFO - root - Page:2, Index:10, GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field, https://arxiv.org/pdf/2507.05594, 2025-07-07
2025-11-09 20:47:14,674 - INFO - root - Page:2, Index:11, Hita: Holistic Tokenizer for Autoregressive Image Generation, https://arxiv.org/pdf/2507.02358, 2025-07-11
2025-11-09 20:47:14,674 - INFO - root - Page:2, Index:12, QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference, https://arxiv.org/pdf/2506.23934, 2025-06-30
2025-11-09 20:47:14,674 - INFO - root - Page:2, Index:13, FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization, https://arxiv.org/pdf/2506.23516, 2025-07-21
2025-11-09 20:47:14,674 - INFO - root - Page:2, Index:14, Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models, https://arxiv.org/pdf/2506.23025, 2025-06-28
2025-11-09 20:47:14,675 - INFO - root - Page:2, Index:15, Joint Quantization and Pruning Neural Networks Approach: A Case Study on FSO Receivers, https://arxiv.org/pdf/2506.20084, 2025-06-24
2025-11-09 20:47:14,675 - INFO - root - Page:2, Index:16, Single-step Diffusion for Image Compression at Ultra-Low Bitrates, https://arxiv.org/pdf/2506.16572, 2025-09-22
2025-11-09 20:47:14,675 - INFO - root - Page:2, Index:17, LittleBit: Ultra Low-Bit Quantization via Latent Factorization, https://arxiv.org/pdf/2506.13771, 2025-10-28
2025-11-09 20:47:14,676 - INFO - root - Page:2, Index:18, MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering, https://arxiv.org/pdf/2506.13755, 2025-06-16
2025-11-09 20:47:14,676 - INFO - root - Page:2, Index:19, EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization, https://arxiv.org/pdf/2506.13329, 2025-07-04
2025-11-09 20:47:14,677 - INFO - root - Page:2, Index:20, Towards Neural Audio Codec Source Parsing, https://arxiv.org/pdf/2506.12627, 2025-06-14
2025-11-09 20:47:14,677 - INFO - root - Page:2, Index:21, Quantizing Small-Scale State-Space Models for Edge AI, https://arxiv.org/pdf/2506.12480, 2025-06-14
2025-11-09 20:47:14,677 - INFO - root - Page:2, Index:22, EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction, https://arxiv.org/pdf/2506.12015, 2025-06-13
2025-11-09 20:47:14,678 - INFO - root - Page:2, Index:23, Compression Aware Certified Training, https://arxiv.org/pdf/2506.11992, 2025-06-13
2025-11-09 20:47:14,678 - INFO - root - Page:2, Index:24, GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers, https://arxiv.org/pdf/2506.11784, 2025-06-13
2025-11-09 20:47:14,680 - INFO - root - Page:2, Index:25, TruncQuant: Truncation-Ready Quantization for DNNs with Flexible Weight Bit Precision, https://arxiv.org/pdf/2506.11431, 2025-06-12
2025-11-09 20:47:14,680 - INFO - root - Page:2, Index:26, EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices, https://arxiv.org/pdf/2506.11093, 2025-06-05
2025-11-09 20:47:14,681 - INFO - root - Page:2, Index:27, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 20:47:14,682 - INFO - root - Page:2, Index:28, AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent, https://arxiv.org/pdf/2506.10205, 2025-06-11
2025-11-09 20:47:14,682 - INFO - root - Page:2, Index:29, Q-SAM2: Accurate Quantization for Segment Anything Model 2, https://arxiv.org/pdf/2506.09782, 2025-06-11
2025-11-09 20:47:14,682 - INFO - root - Page:2, Index:30, Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs, https://arxiv.org/pdf/2506.09104, 2025-06-10
2025-11-09 20:47:14,683 - INFO - root - Page:2, Index:31, Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU, https://arxiv.org/pdf/2506.08911, 2025-06-10
2025-11-09 20:47:14,683 - INFO - root - Page:2, Index:32, POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration, https://arxiv.org/pdf/2506.08785, 2025-06-10
2025-11-09 20:47:14,683 - INFO - root - Page:2, Index:33, MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts, https://arxiv.org/pdf/2506.07533, 2025-06-09
2025-11-09 20:47:14,683 - INFO - root - Page:2, Index:34, BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation, https://arxiv.org/pdf/2506.07530, 2025-06-09
2025-11-09 20:47:14,684 - INFO - root - Page:2, Index:35, RecGPT: A Foundation Model for Sequential Recommendation, https://arxiv.org/pdf/2506.06270, 2025-06-12
2025-11-09 20:47:14,687 - INFO - root - Page:2, Index:36, FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion, https://arxiv.org/pdf/2506.04648, 2025-06-05
2025-11-09 20:47:14,688 - INFO - root - Page:2, Index:37, BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing, https://arxiv.org/pdf/2506.03515, 2025-06-03
2025-11-09 20:47:14,689 - INFO - root - Page:2, Index:38, ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding, https://arxiv.org/pdf/2506.01853, 2025-06-02
2025-11-09 20:47:14,689 - INFO - root - Page:2, Index:39, Movable Antenna Enhanced Federated Fine-Tuning of Large Language Models via Hybrid Client Selection Optimization, https://arxiv.org/pdf/2506.00011, 2025-10-26
2025-11-09 20:47:14,690 - INFO - root - Page:2, Index:40, Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach, https://arxiv.org/pdf/2505.24721, 2025-05-30
2025-11-09 20:47:14,690 - INFO - root - Page:2, Index:41, GARLIC: GAussian Representation LearnIng for spaCe partitioning, https://arxiv.org/pdf/2505.24608, 2025-10-02
2025-11-09 20:47:14,690 - INFO - root - Page:2, Index:42, AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up, https://arxiv.org/pdf/2505.24584, 2025-08-18
2025-11-09 20:47:14,691 - INFO - root - Page:2, Index:43, Model-Preserving Adaptive Rounding, https://arxiv.org/pdf/2505.22988, 2025-09-25
2025-11-09 20:47:14,691 - INFO - root - Page:2, Index:44, Highly Efficient and Effective LLMs with Multi-Boolean Architectures, https://arxiv.org/pdf/2505.22811, 2025-10-03
2025-11-09 20:47:14,691 - INFO - root - Page:2, Index:45, Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning, https://arxiv.org/pdf/2505.21591, 2025-05-27
2025-11-09 20:47:14,692 - INFO - root - Page:2, Index:46, Frequency Composition for Compressed and Domain-Adaptive Neural Networks, https://arxiv.org/pdf/2505.20890, 2025-05-27
2025-11-09 20:47:14,693 - INFO - root - Page:2, Index:47, MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition, https://arxiv.org/pdf/2505.20744, 2025-10-27
2025-11-09 20:47:14,693 - INFO - root - Page:2, Index:48, Optimizing edge AI models on HPC systems with the edge in the loop, https://arxiv.org/pdf/2505.19995, 2025-05-26
2025-11-09 20:47:14,694 - INFO - root - Fetching page 4 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=150
2025-11-09 20:47:21,165 - INFO - root - get_all_titles_from_web 
2025-11-09 20:47:21,165 - INFO - root - Page:3, Index:0, DPASyn: Mechanism-Aware Drug Synergy Prediction via Dual Attention and Precision-Aware Quantization, https://arxiv.org/pdf/2505.19144, 2025-09-20
2025-11-09 20:47:21,166 - INFO - root - Page:3, Index:1, Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding, https://arxiv.org/pdf/2505.18758, 2025-05-24
2025-11-09 20:47:21,166 - INFO - root - Page:3, Index:2, Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization, https://arxiv.org/pdf/2505.18113, 2025-05-23
2025-11-09 20:47:21,166 - INFO - root - Page:3, Index:3, Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs, https://arxiv.org/pdf/2505.17662, 2025-09-19
2025-11-09 20:47:21,167 - INFO - root - Page:3, Index:4, FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design, https://arxiv.org/pdf/2505.16335, 2025-05-22
2025-11-09 20:47:21,167 - INFO - root - Page:3, Index:5, Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control, https://arxiv.org/pdf/2505.15304, 2025-05-30
2025-11-09 20:47:21,167 - INFO - root - Page:3, Index:6, Scaling Law for Quantization-Aware Training, https://arxiv.org/pdf/2505.14302, 2025-05-20
2025-11-09 20:47:21,167 - INFO - root - Page:3, Index:7, Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs, https://arxiv.org/pdf/2505.13060, 2025-05-19
2025-11-09 20:47:21,168 - INFO - root - Page:3, Index:8, Deep Unfolding with Kernel-based Quantization in MIMO Detection, https://arxiv.org/pdf/2505.12736, 2025-05-19
2025-11-09 20:47:21,168 - INFO - root - Page:3, Index:9, FedHQ: Hybrid Runtime Quantization for Federated Learning, https://arxiv.org/pdf/2505.11982, 2025-05-17
2025-11-09 20:47:21,169 - INFO - root - Page:3, Index:10, QVGen: Pushing the Limit of Quantized Video Generative Models, https://arxiv.org/pdf/2505.11497, 2025-09-27
2025-11-09 20:47:21,169 - INFO - root - Page:3, Index:11, EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes, https://arxiv.org/pdf/2505.10787, 2025-05-15
2025-11-09 20:47:21,169 - INFO - root - Page:3, Index:12, ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition, https://arxiv.org/pdf/2505.08981, 2025-05-13
2025-11-09 20:47:21,170 - INFO - root - Page:3, Index:13, PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs, https://arxiv.org/pdf/2505.03254, 2025-08-06
2025-11-09 20:47:21,170 - INFO - root - Page:3, Index:14, ICQuant: Index Coding enables Low-bit LLM Quantization, https://arxiv.org/pdf/2505.00850, 2025-08-24
2025-11-09 20:47:21,170 - INFO - root - Page:3, Index:15, Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining, https://arxiv.org/pdf/2504.13932, 2025-07-30
2025-11-09 20:47:21,171 - INFO - root - Page:3, Index:16, Achieving binary weight and activation for LLMs using Post-Training Quantization, https://arxiv.org/pdf/2504.05352, 2025-06-30
2025-11-09 20:47:21,171 - INFO - root - Page:3, Index:17, Wireless Hearables With Programmable Speech AI Accelerators, https://arxiv.org/pdf/2503.18698, 2025-10-21
2025-11-09 20:47:21,171 - INFO - root - Page:3, Index:18, GranQ: Efficient Channel-wise Quantization via Vectorized Pre-Scaling for Zero-Shot QAT, https://arxiv.org/pdf/2503.18339, 2025-10-15
2025-11-09 20:47:21,173 - INFO - root - Page:3, Index:19, CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting, https://arxiv.org/pdf/2503.12836, 2025-09-29
2025-11-09 20:47:21,173 - INFO - root - Page:3, Index:20, AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model, https://arxiv.org/pdf/2503.03088, 2025-07-11
2025-11-09 20:47:21,173 - INFO - root - Page:3, Index:21, Teaching Metric Distance to Discrete Autoregressive Language Models, https://arxiv.org/pdf/2503.02379, 2025-10-07
2025-11-09 20:47:21,173 - INFO - root - Page:3, Index:22, Regularization-based Framework for Quantization-, Fault- and Variability-Aware Training, https://arxiv.org/pdf/2503.01297, 2025-07-12
2025-11-09 20:47:21,174 - INFO - root - Fetching page 5 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=200
2025-11-09 20:47:28,424 - INFO - root - get_all_titles_from_web 
2025-11-09 20:47:28,424 - INFO - root - Page:4, Index:0, Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models, https://arxiv.org/pdf/2502.15799, 2025-06-29
2025-11-09 20:47:28,425 - INFO - root - Page:4, Index:1, Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer, https://arxiv.org/pdf/2502.15779, 2025-09-01
2025-11-09 20:47:28,425 - INFO - root - Page:4, Index:2, RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models, https://arxiv.org/pdf/2502.09003, 2025-06-05
2025-11-09 20:47:28,425 - INFO - root - Page:4, Index:3, Spatial Degradation-Aware and Temporal Consistent Diffusion Model for Compressed Video Super-Resolution, https://arxiv.org/pdf/2502.07381, 2025-06-27
2025-11-09 20:47:28,426 - INFO - root - Page:4, Index:4, QuEST: Stable Training of LLMs with 1-Bit Weights and Activations, https://arxiv.org/pdf/2502.05003, 2025-06-10
2025-11-09 20:47:28,426 - INFO - root - Page:4, Index:5, HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs, https://arxiv.org/pdf/2501.02625, 2025-11-05
2025-11-09 20:47:28,427 - INFO - root - Page:4, Index:6, TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction, https://arxiv.org/pdf/2412.16919, 2025-08-08
2025-11-09 20:47:28,427 - INFO - root - Page:4, Index:7, Training Multi-Layer Binary Neural Networks With Local Binary Error Signals, https://arxiv.org/pdf/2412.00119, 2025-08-05
2025-11-09 20:47:28,428 - INFO - root - Page:4, Index:8, Scaling Large-scale GNN Training to Thousands of Processors on CPU-based Supercomputers, https://arxiv.org/pdf/2411.16025, 2025-05-26
2025-11-09 20:47:28,428 - INFO - root - Page:4, Index:9, HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization, https://arxiv.org/pdf/2411.06581, 2025-05-16
2025-11-09 20:47:28,428 - INFO - root - Fetching page 6 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=250
2025-11-09 20:47:34,860 - INFO - root - get_all_titles_from_web 
2025-11-09 20:47:34,860 - INFO - root - Page:5, Index:0, Gradient-Free Training of Quantized Neural Networks, https://arxiv.org/pdf/2410.09734, 2025-09-29
2025-11-09 20:47:34,861 - INFO - root - Page:5, Index:1, QT-DoG: Quantization-aware Training for Domain Generalization, https://arxiv.org/pdf/2410.06020, 2025-06-26
2025-11-09 20:47:34,861 - INFO - root - Page:5, Index:2, Constraint Guided Model Quantization of Neural Networks, https://arxiv.org/pdf/2409.20138, 2025-09-12
2025-11-09 20:47:34,861 - INFO - root - Page:5, Index:3, Accumulator-Aware Post-Training Quantization for Large Language Models, https://arxiv.org/pdf/2409.17092, 2025-07-30
2025-11-09 20:47:34,861 - INFO - root - Page:5, Index:4, DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation, https://arxiv.org/pdf/2409.14307, 2025-07-09
2025-11-09 20:47:34,862 - INFO - root - Page:5, Index:5, Robust Training of Neural Networks at Arbitrary Precision and Sparsity, https://arxiv.org/pdf/2409.09245, 2025-09-23
2025-11-09 20:47:34,862 - INFO - root - Page:5, Index:6, VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing, https://arxiv.org/pdf/2408.05758, 2025-05-27
2025-11-09 20:47:34,862 - INFO - root - Page:5, Index:7, DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers, https://arxiv.org/pdf/2408.03291, 2025-06-19
2025-11-09 20:47:34,862 - INFO - root - Page:5, Index:8, Temporal Feature Matters: A Framework for Diffusion Model Quantization, https://arxiv.org/pdf/2407.19547, 2025-07-11
2025-11-09 20:47:34,863 - INFO - root - Page:5, Index:9, EfficientQAT: Efficient Quantization-Aware Training for Large Language Models, https://arxiv.org/pdf/2407.11062, 2025-05-19
2025-11-09 20:47:34,863 - INFO - root - Page:5, Index:10, Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT, https://arxiv.org/pdf/2407.11041, 2025-10-31
2025-11-09 20:47:34,863 - INFO - root - Fetching page 7 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=300
2025-11-09 20:47:41,801 - INFO - root - get_all_titles_from_web 
2025-11-09 20:47:41,801 - INFO - root - Page:6, Index:0, BoA: Attention-aware Post-training Quantization without Backpropagation, https://arxiv.org/pdf/2406.13474, 2025-06-06
2025-11-09 20:47:41,802 - INFO - root - Page:6, Index:1, A Rescaling-Invariant Lipschitz Bound Based on Path-Metrics for Modern ReLU Network Parameterizations, https://arxiv.org/pdf/2405.15006, 2025-06-13
2025-11-09 20:47:41,802 - INFO - root - Page:6, Index:2, Scheduling Weight Transitions for Quantization-Aware Training, https://arxiv.org/pdf/2404.19248, 2025-10-01
2025-11-09 20:47:41,802 - INFO - root - Page:6, Index:3, GPTVQ: The Blessing of Dimensionality for LLM Quantization, https://arxiv.org/pdf/2402.15319, 2025-06-03
2025-11-09 20:47:41,802 - INFO - root - Fetching page 8 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=350
2025-11-09 20:47:49,075 - INFO - root - get_all_titles_from_web 
2025-11-09 20:47:49,075 - INFO - root - Page:7, Index:0, Squat: Quant Small Language Models on the Edge, https://arxiv.org/pdf/2402.10787, 2025-07-01
2025-11-09 20:47:49,075 - INFO - root - Page:7, Index:1, L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2402.04902, 2025-07-21
2025-11-09 20:47:49,076 - INFO - root - Fetching page 9 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=400
2025-11-09 20:47:55,416 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 20:47:55,417 - INFO - root - File already exists, skipping download: d:\ChatPaper\academic Papers\Quantization-Aware-Training\A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat.pdf
2025-11-09 20:47:55,418 - INFO - root - File already exists, skipping download: d:\ChatPaper\academic Papers\Quantization-Aware-Training\FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error.pdf
2025-11-09 20:47:55,421 - INFO - root - 跳过已处理论文 A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies：d:\ChatPaper\academic Papers\Quantization-Aware-Training\A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat.pdf
2025-11-09 20:47:55,421 - INFO - root - 跳过已处理论文 FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error：d:\ChatPaper\academic Papers\Quantization-Aware-Training\FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error.pdf
2025-11-09 20:47:55,421 - INFO - root - summary time: 69.47 seconds
2025-11-09 20:51:41,855 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 20:51:41,857 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 20:51:41,858 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 20:51:42,908 - INFO - root - LLMClientManager: 指定使用客户端: DeepSeek
2025-11-09 20:51:42,908 - INFO - root - DeepSeekClient: API key found: 9f9e270e-b...
2025-11-09 20:51:42,909 - INFO - root - DeepSeekClient: 使用模式: 火山引擎
2025-11-09 20:51:42,909 - INFO - root - DeepSeekClient: 模型名称: deepseek-v3-1-terminus
2025-11-09 20:51:47,545 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 20:51:47,574 - INFO - root - DeepSeekClient: initialized successfully with model deepseek-v3-1-terminus
2025-11-09 20:51:47,574 - INFO - root - LLMClientManager: DeepSeek 客户端初始化成功
2025-11-09 20:51:47,576 - INFO - root - LLMClientManager: switched to DeepSeek client
2025-11-09 20:51:47,577 - INFO - root - 已手动切换到 LLM 客户端: DeepSeek
2025-11-09 20:51:47,578 - INFO - root - 使用 LLM 模型: deepseek-v3-1-terminus
2025-11-09 20:51:47,579 - INFO - root - 可用客户端: ['DeepSeek']
2025-11-09 20:51:47,579 - INFO - root - === 运行配置 ===
2025-11-09 20:51:47,581 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 20:51:47,583 - INFO - root - 关键词: QAT
2025-11-09 20:51:47,583 - INFO - root - 查询: Quantization-Aware-Training
2025-11-09 20:51:47,583 - INFO - root - 排序: None
2025-11-09 20:51:47,584 - INFO - root - 最近天数: 180
2025-11-09 20:51:47,584 - INFO - root - 最大处理数量: 2
2025-11-09 20:51:47,584 - INFO - root - 保存图片: 是
2025-11-09 20:51:47,591 - INFO - root - 输出语言: 中文
2025-11-09 20:51:47,591 - INFO - root - 强制重新处理: 否
2025-11-09 20:51:47,591 - INFO - root - ====================
2025-11-09 20:51:47,592 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 20:51:47,592 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 20:51:54,182 - INFO - root - get_all_titles_from_web 
2025-11-09 20:51:54,183 - INFO - root - Page:0, Index:0, A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies, https://arxiv.org/pdf/2511.03201, 2025-11-05
2025-11-09 20:51:54,183 - INFO - root - Page:0, Index:1, FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error, https://arxiv.org/pdf/2511.02302, 2025-11-04
2025-11-09 20:51:54,183 - INFO - root - Page:0, Index:2, Outlier-Aware Post-Training Quantization for Image Super-Resolution, https://arxiv.org/pdf/2511.00682, 2025-11-01
2025-11-09 20:51:54,183 - INFO - root - Page:0, Index:3, Evaluation of Wafer-Scale SOT-MRAM for Analog Crossbar Array Applications, https://arxiv.org/pdf/2510.25853, 2025-11-03
2025-11-09 20:51:54,184 - INFO - root - Page:0, Index:4, Improving the Straight-Through Estimator with Zeroth-Order Information, https://arxiv.org/pdf/2510.23926, 2025-10-27
2025-11-09 20:51:54,184 - INFO - root - Page:0, Index:5, Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework, https://arxiv.org/pdf/2510.22243, 2025-10-25
2025-11-09 20:51:54,184 - INFO - root - Page:0, Index:6, TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge, https://arxiv.org/pdf/2510.21879, 2025-10-23
2025-11-09 20:51:54,185 - INFO - root - Page:0, Index:7, KARIPAP: Quantum-Inspired Tensor Network Compression of Large Language Models Using Infinite Projected Entangled Pair States and Tensor Renormalization Group, https://arxiv.org/pdf/2510.21844, 2025-10-22
2025-11-09 20:51:54,185 - INFO - root - Page:0, Index:8, A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization, https://arxiv.org/pdf/2510.21314, 2025-10-24
2025-11-09 20:51:54,185 - INFO - root - Page:0, Index:9, Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks, https://arxiv.org/pdf/2510.19760, 2025-10-22
2025-11-09 20:51:54,185 - INFO - root - Page:0, Index:10, CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training, https://arxiv.org/pdf/2510.18784, 2025-10-21
2025-11-09 20:51:54,186 - INFO - root - Page:0, Index:11, Mixed-Precision Quantization for Language Models: Techniques and Prospects, https://arxiv.org/pdf/2510.16805, 2025-10-19
2025-11-09 20:51:54,186 - INFO - root - Page:0, Index:12, SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation, https://arxiv.org/pdf/2510.16396, 2025-10-30
2025-11-09 20:51:54,186 - INFO - root - Page:0, Index:13, CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models, https://arxiv.org/pdf/2510.15962, 2025-10-11
2025-11-09 20:51:54,186 - INFO - root - Page:0, Index:14, SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization, https://arxiv.org/pdf/2510.15775, 2025-10-17
2025-11-09 20:51:54,186 - INFO - root - Page:0, Index:15, SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware, https://arxiv.org/pdf/2510.15542, 2025-10-31
2025-11-09 20:51:54,187 - INFO - root - Page:0, Index:16, GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework, https://arxiv.org/pdf/2510.15299, 2025-10-17
2025-11-09 20:51:54,187 - INFO - root - Page:0, Index:17, SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images, https://arxiv.org/pdf/2510.15072, 2025-10-16
2025-11-09 20:51:54,187 - INFO - root - Page:0, Index:18, FraQAT: Quantization Aware Training with Fractional bits, https://arxiv.org/pdf/2510.14823, 2025-10-16
2025-11-09 20:51:54,188 - INFO - root - Page:0, Index:19, Computing-In-Memory Aware Model Adaption For Edge Devices, https://arxiv.org/pdf/2510.14379, 2025-10-16
2025-11-09 20:51:54,189 - INFO - root - Page:0, Index:20, Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge, https://arxiv.org/pdf/2510.13760, 2025-11-04
2025-11-09 20:51:54,189 - INFO - root - Page:0, Index:21, NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models, https://arxiv.org/pdf/2510.13068, 2025-10-14
2025-11-09 20:51:54,189 - INFO - root - Page:0, Index:22, Detect Anything via Next Point Prediction, https://arxiv.org/pdf/2510.12798, 2025-10-14
2025-11-09 20:51:54,190 - INFO - root - Page:0, Index:23, AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model, https://arxiv.org/pdf/2510.11496, 2025-10-14
2025-11-09 20:51:54,190 - INFO - root - Page:0, Index:24, Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware, https://arxiv.org/pdf/2510.11484, 2025-10-13
2025-11-09 20:51:54,190 - INFO - root - Page:0, Index:25, SASER: Stego attacks on open-source LLMs, https://arxiv.org/pdf/2510.10486, 2025-10-12
2025-11-09 20:51:54,195 - INFO - root - Page:0, Index:26, Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition, https://arxiv.org/pdf/2510.09653, 2025-10-15
2025-11-09 20:51:54,196 - INFO - root - Page:0, Index:27, A Theoretically-Grounded Codebook for Digital Semantic Communications, https://arxiv.org/pdf/2510.07108, 2025-10-14
2025-11-09 20:51:54,198 - INFO - root - Page:0, Index:28, QuantDemoire: Quantization with Outlier Aware for Image Demoiréing, https://arxiv.org/pdf/2510.04066, 2025-10-05
2025-11-09 20:51:54,198 - INFO - root - Page:0, Index:29, Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization, https://arxiv.org/pdf/2510.03763, 2025-10-04
2025-11-09 20:51:54,199 - INFO - root - Page:0, Index:30, Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models, https://arxiv.org/pdf/2510.03274, 2025-09-27
2025-11-09 20:51:54,199 - INFO - root - Page:0, Index:31, PT$^2$-LLM: Post-Training Ternarization for Large Language Models, https://arxiv.org/pdf/2510.03267, 2025-09-26
2025-11-09 20:51:54,199 - INFO - root - Page:0, Index:32, Purrception: Variational Flow Matching for Vector-Quantized Image Generation, https://arxiv.org/pdf/2510.01478, 2025-10-01
2025-11-09 20:51:54,199 - INFO - root - Page:0, Index:33, Post-Training Quantization for Audio Diffusion Transformers, https://arxiv.org/pdf/2510.00313, 2025-09-30
2025-11-09 20:51:54,200 - INFO - root - Page:0, Index:34, Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling, https://arxiv.org/pdf/2510.00028, 2025-09-25
2025-11-09 20:51:54,200 - INFO - root - Page:0, Index:35, Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models, https://arxiv.org/pdf/2509.26436, 2025-09-30
2025-11-09 20:51:54,200 - INFO - root - Page:0, Index:36, Cat: Post-Training Quantization Error Reduction via Cluster-based Affine Transformation, https://arxiv.org/pdf/2509.26277, 2025-10-07
2025-11-09 20:51:54,200 - INFO - root - Page:0, Index:37, CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models, https://arxiv.org/pdf/2509.25996, 2025-09-30
2025-11-09 20:51:54,201 - INFO - root - Page:0, Index:38, Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications, https://arxiv.org/pdf/2509.25439, 2025-09-29
2025-11-09 20:51:54,201 - INFO - root - Page:0, Index:39, On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs, https://arxiv.org/pdf/2509.25214, 2025-09-22
2025-11-09 20:51:54,201 - INFO - root - Page:0, Index:40, VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning, https://arxiv.org/pdf/2509.24650, 2025-09-29
2025-11-09 20:51:54,202 - INFO - root - Page:0, Index:41, S$^2$NN: Sub-bit Spiking Neural Networks, https://arxiv.org/pdf/2509.24266, 2025-10-24
2025-11-09 20:51:54,202 - INFO - root - Page:0, Index:42, Tequila: Trapping-free Ternary Quantization for Large Language Models, https://arxiv.org/pdf/2509.23809, 2025-10-17
2025-11-09 20:51:54,202 - INFO - root - Page:0, Index:43, Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution, https://arxiv.org/pdf/2509.23774, 2025-09-30
2025-11-09 20:51:54,203 - INFO - root - Page:0, Index:44, RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization, https://arxiv.org/pdf/2509.23582, 2025-09-27
2025-11-09 20:51:54,203 - INFO - root - Page:0, Index:45, Beyond Outliers: A Study of Optimizers Under Quantization, https://arxiv.org/pdf/2509.23500, 2025-10-02
2025-11-09 20:51:54,203 - INFO - root - Page:0, Index:46, Compute-Optimal Quantization-Aware Training, https://arxiv.org/pdf/2509.22935, 2025-09-26
2025-11-09 20:51:54,203 - INFO - root - Page:0, Index:47, COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning, https://arxiv.org/pdf/2509.22075, 2025-10-06
2025-11-09 20:51:54,204 - INFO - root - Page:0, Index:48, SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models, https://arxiv.org/pdf/2509.21498, 2025-09-25
2025-11-09 20:51:54,205 - INFO - root - Page:0, Index:49, Quantized Visual Geometry Grounded Transformer, https://arxiv.org/pdf/2509.21302, 2025-09-29
2025-11-09 20:51:54,205 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 20:52:01,885 - INFO - root - get_all_titles_from_web 
2025-11-09 20:52:01,886 - INFO - root - Page:1, Index:0, Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy, https://arxiv.org/pdf/2509.21173, 2025-10-27
2025-11-09 20:52:01,886 - INFO - root - Page:1, Index:1, Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer, https://arxiv.org/pdf/2509.20854, 2025-09-25
2025-11-09 20:52:01,886 - INFO - root - Page:1, Index:2, TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection, https://arxiv.org/pdf/2509.18193, 2025-09-19
2025-11-09 20:52:01,886 - INFO - root - Page:1, Index:3, SBVR: Summation of BitVector Representation for Efficient LLM Quantization, https://arxiv.org/pdf/2509.18172, 2025-09-17
2025-11-09 20:52:01,888 - INFO - root - Page:1, Index:4, QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2509.17428, 2025-09-26
2025-11-09 20:52:01,888 - INFO - root - Page:1, Index:5, PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models, https://arxiv.org/pdf/2509.16989, 2025-10-28
2025-11-09 20:52:01,888 - INFO - root - Page:1, Index:6, MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training, https://arxiv.org/pdf/2509.15514, 2025-09-18
2025-11-09 20:52:01,888 - INFO - root - Page:1, Index:7, Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs, https://arxiv.org/pdf/2509.14391, 2025-09-17
2025-11-09 20:52:01,888 - INFO - root - Page:1, Index:8, Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization, https://arxiv.org/pdf/2509.13786, 2025-09-19
2025-11-09 20:52:01,889 - INFO - root - Page:1, Index:9, Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees, https://arxiv.org/pdf/2509.11054, 2025-09-13
2025-11-09 20:52:01,890 - INFO - root - Page:1, Index:10, SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models, https://arxiv.org/pdf/2509.09090, 2025-09-10
2025-11-09 20:52:01,890 - INFO - root - Page:1, Index:11, CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks with Entropy Regularization, https://arxiv.org/pdf/2509.08776, 2025-09-10
2025-11-09 20:52:01,890 - INFO - root - Page:1, Index:12, Explaining How Quantization Disparately Skews a Model, https://arxiv.org/pdf/2509.07222, 2025-09-08
2025-11-09 20:52:01,890 - INFO - root - Page:1, Index:13, LoaQ: Layer-wise Output Approximation Quantization, https://arxiv.org/pdf/2509.06297, 2025-09-07
2025-11-09 20:52:01,891 - INFO - root - Page:1, Index:14, FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving, https://arxiv.org/pdf/2509.06261, 2025-09-14
2025-11-09 20:52:01,891 - INFO - root - Page:1, Index:15, Sensitivity-Aware Post-Training Quantization for Deep Neural Networks, https://arxiv.org/pdf/2509.05576, 2025-09-05
2025-11-09 20:52:01,892 - INFO - root - Page:1, Index:16, SuperSNN: A Hardware-Aware Framework for Physically Realizable, High-Performance Superconducting Spiking Neural Network Chips, https://arxiv.org/pdf/2509.05532, 2025-09-05
2025-11-09 20:52:01,892 - INFO - root - Page:1, Index:17, Data-Augmented Quantization-Aware Knowledge Distillation, https://arxiv.org/pdf/2509.03850, 2025-09-03
2025-11-09 20:52:01,893 - INFO - root - Page:1, Index:18, DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling, https://arxiv.org/pdf/2509.03472, 2025-09-03
2025-11-09 20:52:01,893 - INFO - root - Page:1, Index:19, Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs, https://arxiv.org/pdf/2509.02017, 2025-09-02
2025-11-09 20:52:01,893 - INFO - root - Page:1, Index:20, Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling, https://arxiv.org/pdf/2509.01624, 2025-09-01
2025-11-09 20:52:01,893 - INFO - root - Page:1, Index:21, Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective, https://arxiv.org/pdf/2509.00859, 2025-08-31
2025-11-09 20:52:01,894 - INFO - root - Page:1, Index:22, Progressive Element-wise Gradient Estimation for Neural Network Quantization, https://arxiv.org/pdf/2509.00097, 2025-08-27
2025-11-09 20:52:01,894 - INFO - root - Page:1, Index:23, End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost, https://arxiv.org/pdf/2509.00031, 2025-09-29
2025-11-09 20:52:01,894 - INFO - root - Page:1, Index:24, Quantization Robustness to Input Degradations for Object Detection, https://arxiv.org/pdf/2508.19600, 2025-08-27
2025-11-09 20:52:01,894 - INFO - root - Page:1, Index:25, Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models, https://arxiv.org/pdf/2508.18609, 2025-08-27
2025-11-09 20:52:01,896 - INFO - root - Page:1, Index:26, AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration, https://arxiv.org/pdf/2508.18025, 2025-08-25
2025-11-09 20:52:01,896 - INFO - root - Page:1, Index:27, TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling, https://arxiv.org/pdf/2508.16790, 2025-08-22
2025-11-09 20:52:01,896 - INFO - root - Page:1, Index:28, A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering, https://arxiv.org/pdf/2508.16516, 2025-08-22
2025-11-09 20:52:01,897 - INFO - root - Page:1, Index:29, JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs, https://arxiv.org/pdf/2508.15468, 2025-08-21
2025-11-09 20:52:01,897 - INFO - root - Page:1, Index:30, MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation, https://arxiv.org/pdf/2508.15281, 2025-08-21
2025-11-09 20:52:01,897 - INFO - root - Page:1, Index:31, DLLMQuant: Quantizing Diffusion-based Large Language Models, https://arxiv.org/pdf/2508.14090, 2025-08-25
2025-11-09 20:52:01,897 - INFO - root - Page:1, Index:32, Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management, https://arxiv.org/pdf/2508.13905, 2025-08-19
2025-11-09 20:52:01,909 - INFO - root - Page:1, Index:33, XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads, https://arxiv.org/pdf/2508.13049, 2025-08-18
2025-11-09 20:52:01,909 - INFO - root - Page:1, Index:34, Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection, https://arxiv.org/pdf/2508.12230, 2025-08-17
2025-11-09 20:52:01,910 - INFO - root - Page:1, Index:35, TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks, https://arxiv.org/pdf/2508.12132, 2025-08-16
2025-11-09 20:52:01,911 - INFO - root - Page:1, Index:36, Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion, https://arxiv.org/pdf/2508.12094, 2025-10-13
2025-11-09 20:52:01,912 - INFO - root - Page:1, Index:37, Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware, https://arxiv.org/pdf/2508.11940, 2025-08-16
2025-11-09 20:52:01,912 - INFO - root - Page:1, Index:38, PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks, https://arxiv.org/pdf/2508.10557, 2025-08-15
2025-11-09 20:52:01,912 - INFO - root - Page:1, Index:39, MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI, https://arxiv.org/pdf/2508.09500, 2025-08-13
2025-11-09 20:52:01,912 - INFO - root - Page:1, Index:40, SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via Codebooks for recommender system, https://arxiv.org/pdf/2508.09090, 2025-08-12
2025-11-09 20:52:01,914 - INFO - root - Page:1, Index:41, Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models, https://arxiv.org/pdf/2508.06974, 2025-08-09
2025-11-09 20:52:01,915 - INFO - root - Page:1, Index:42, Efficient Deep Neural Receiver with Post-Training Quantization, https://arxiv.org/pdf/2508.06275, 2025-08-08
2025-11-09 20:52:01,915 - INFO - root - Page:1, Index:43, iFairy: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$, https://arxiv.org/pdf/2508.05571, 2025-08-16
2025-11-09 20:52:01,915 - INFO - root - Page:1, Index:44, S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation, https://arxiv.org/pdf/2508.04016, 2025-10-27
2025-11-09 20:52:01,920 - INFO - root - Page:1, Index:45, LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation, https://arxiv.org/pdf/2508.03485, 2025-09-23
2025-11-09 20:52:01,920 - INFO - root - Page:1, Index:46, VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation, https://arxiv.org/pdf/2508.03351, 2025-08-05
2025-11-09 20:52:01,920 - INFO - root - Page:1, Index:47, TF-MLPNet: Tiny Real-Time Neural Speech Separation, https://arxiv.org/pdf/2508.03047, 2025-08-04
2025-11-09 20:52:01,922 - INFO - root - Page:1, Index:48, SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation, https://arxiv.org/pdf/2508.01375, 2025-08-02
2025-11-09 20:52:01,922 - INFO - root - Page:1, Index:49, MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective, https://arxiv.org/pdf/2507.19131, 2025-07-25
2025-11-09 20:52:01,923 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 20:52:08,418 - INFO - root - get_all_titles_from_web 
2025-11-09 20:52:08,418 - INFO - root - Page:2, Index:0, Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction, https://arxiv.org/pdf/2507.17768, 2025-07-16
2025-11-09 20:52:08,419 - INFO - root - Page:2, Index:1, SiLQ: Simple Large Language Model Quantization-Aware Training, https://arxiv.org/pdf/2507.16933, 2025-07-22
2025-11-09 20:52:08,419 - INFO - root - Page:2, Index:2, Task-Specific Zero-shot Quantization-Aware Training for Object Detection, https://arxiv.org/pdf/2507.16782, 2025-07-22
2025-11-09 20:52:08,419 - INFO - root - Page:2, Index:3, TorchAO: PyTorch-Native Training-to-Serving Model Optimization, https://arxiv.org/pdf/2507.16099, 2025-07-21
2025-11-09 20:52:08,420 - INFO - root - Page:2, Index:4, SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models, https://arxiv.org/pdf/2507.14811, 2025-08-27
2025-11-09 20:52:08,420 - INFO - root - Page:2, Index:5, Apple Intelligence Foundation Language Models: Tech Report 2025, https://arxiv.org/pdf/2507.13575, 2025-08-27
2025-11-09 20:52:08,420 - INFO - root - Page:2, Index:6, Generative Multi-Target Cross-Domain Recommendation, https://arxiv.org/pdf/2507.12871, 2025-08-07
2025-11-09 20:52:08,420 - INFO - root - Page:2, Index:7, DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training, https://arxiv.org/pdf/2507.07149, 2025-07-09
2025-11-09 20:52:08,422 - INFO - root - Page:2, Index:8, Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics, https://arxiv.org/pdf/2507.07044, 2025-07-15
2025-11-09 20:52:08,423 - INFO - root - Page:2, Index:9, QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models, https://arxiv.org/pdf/2507.06079, 2025-07-08
2025-11-09 20:52:08,423 - INFO - root - Page:2, Index:10, GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field, https://arxiv.org/pdf/2507.05594, 2025-07-07
2025-11-09 20:52:08,425 - INFO - root - Page:2, Index:11, Hita: Holistic Tokenizer for Autoregressive Image Generation, https://arxiv.org/pdf/2507.02358, 2025-07-11
2025-11-09 20:52:08,425 - INFO - root - Page:2, Index:12, QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference, https://arxiv.org/pdf/2506.23934, 2025-06-30
2025-11-09 20:52:08,426 - INFO - root - Page:2, Index:13, FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization, https://arxiv.org/pdf/2506.23516, 2025-07-21
2025-11-09 20:52:08,426 - INFO - root - Page:2, Index:14, Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models, https://arxiv.org/pdf/2506.23025, 2025-06-28
2025-11-09 20:52:08,426 - INFO - root - Page:2, Index:15, Joint Quantization and Pruning Neural Networks Approach: A Case Study on FSO Receivers, https://arxiv.org/pdf/2506.20084, 2025-06-24
2025-11-09 20:52:08,427 - INFO - root - Page:2, Index:16, Single-step Diffusion for Image Compression at Ultra-Low Bitrates, https://arxiv.org/pdf/2506.16572, 2025-09-22
2025-11-09 20:52:08,427 - INFO - root - Page:2, Index:17, LittleBit: Ultra Low-Bit Quantization via Latent Factorization, https://arxiv.org/pdf/2506.13771, 2025-10-28
2025-11-09 20:52:08,427 - INFO - root - Page:2, Index:18, MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering, https://arxiv.org/pdf/2506.13755, 2025-06-16
2025-11-09 20:52:08,427 - INFO - root - Page:2, Index:19, EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization, https://arxiv.org/pdf/2506.13329, 2025-07-04
2025-11-09 20:52:08,427 - INFO - root - Page:2, Index:20, Towards Neural Audio Codec Source Parsing, https://arxiv.org/pdf/2506.12627, 2025-06-14
2025-11-09 20:52:08,429 - INFO - root - Page:2, Index:21, Quantizing Small-Scale State-Space Models for Edge AI, https://arxiv.org/pdf/2506.12480, 2025-06-14
2025-11-09 20:52:08,429 - INFO - root - Page:2, Index:22, EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction, https://arxiv.org/pdf/2506.12015, 2025-06-13
2025-11-09 20:52:08,429 - INFO - root - Page:2, Index:23, Compression Aware Certified Training, https://arxiv.org/pdf/2506.11992, 2025-06-13
2025-11-09 20:52:08,430 - INFO - root - Page:2, Index:24, GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers, https://arxiv.org/pdf/2506.11784, 2025-06-13
2025-11-09 20:52:08,430 - INFO - root - Page:2, Index:25, TruncQuant: Truncation-Ready Quantization for DNNs with Flexible Weight Bit Precision, https://arxiv.org/pdf/2506.11431, 2025-06-12
2025-11-09 20:52:08,431 - INFO - root - Page:2, Index:26, EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices, https://arxiv.org/pdf/2506.11093, 2025-06-05
2025-11-09 20:52:08,431 - INFO - root - Page:2, Index:27, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 20:52:08,431 - INFO - root - Page:2, Index:28, AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent, https://arxiv.org/pdf/2506.10205, 2025-06-11
2025-11-09 20:52:08,431 - INFO - root - Page:2, Index:29, Q-SAM2: Accurate Quantization for Segment Anything Model 2, https://arxiv.org/pdf/2506.09782, 2025-06-11
2025-11-09 20:52:08,432 - INFO - root - Page:2, Index:30, Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs, https://arxiv.org/pdf/2506.09104, 2025-06-10
2025-11-09 20:52:08,432 - INFO - root - Page:2, Index:31, Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU, https://arxiv.org/pdf/2506.08911, 2025-06-10
2025-11-09 20:52:08,432 - INFO - root - Page:2, Index:32, POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration, https://arxiv.org/pdf/2506.08785, 2025-06-10
2025-11-09 20:52:08,433 - INFO - root - Page:2, Index:33, MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts, https://arxiv.org/pdf/2506.07533, 2025-06-09
2025-11-09 20:52:08,433 - INFO - root - Page:2, Index:34, BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation, https://arxiv.org/pdf/2506.07530, 2025-06-09
2025-11-09 20:52:08,433 - INFO - root - Page:2, Index:35, RecGPT: A Foundation Model for Sequential Recommendation, https://arxiv.org/pdf/2506.06270, 2025-06-12
2025-11-09 20:52:08,433 - INFO - root - Page:2, Index:36, FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion, https://arxiv.org/pdf/2506.04648, 2025-06-05
2025-11-09 20:52:08,434 - INFO - root - Page:2, Index:37, BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing, https://arxiv.org/pdf/2506.03515, 2025-06-03
2025-11-09 20:52:08,434 - INFO - root - Page:2, Index:38, ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding, https://arxiv.org/pdf/2506.01853, 2025-06-02
2025-11-09 20:52:08,434 - INFO - root - Page:2, Index:39, Movable Antenna Enhanced Federated Fine-Tuning of Large Language Models via Hybrid Client Selection Optimization, https://arxiv.org/pdf/2506.00011, 2025-10-26
2025-11-09 20:52:08,434 - INFO - root - Page:2, Index:40, Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach, https://arxiv.org/pdf/2505.24721, 2025-05-30
2025-11-09 20:52:08,434 - INFO - root - Page:2, Index:41, GARLIC: GAussian Representation LearnIng for spaCe partitioning, https://arxiv.org/pdf/2505.24608, 2025-10-02
2025-11-09 20:52:08,435 - INFO - root - Page:2, Index:42, AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up, https://arxiv.org/pdf/2505.24584, 2025-08-18
2025-11-09 20:52:08,435 - INFO - root - Page:2, Index:43, Model-Preserving Adaptive Rounding, https://arxiv.org/pdf/2505.22988, 2025-09-25
2025-11-09 20:52:08,435 - INFO - root - Page:2, Index:44, Highly Efficient and Effective LLMs with Multi-Boolean Architectures, https://arxiv.org/pdf/2505.22811, 2025-10-03
2025-11-09 20:52:08,435 - INFO - root - Page:2, Index:45, Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning, https://arxiv.org/pdf/2505.21591, 2025-05-27
2025-11-09 20:52:08,435 - INFO - root - Page:2, Index:46, Frequency Composition for Compressed and Domain-Adaptive Neural Networks, https://arxiv.org/pdf/2505.20890, 2025-05-27
2025-11-09 20:52:08,437 - INFO - root - Page:2, Index:47, MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition, https://arxiv.org/pdf/2505.20744, 2025-10-27
2025-11-09 20:52:08,438 - INFO - root - Page:2, Index:48, Optimizing edge AI models on HPC systems with the edge in the loop, https://arxiv.org/pdf/2505.19995, 2025-05-26
2025-11-09 20:52:08,438 - INFO - root - Fetching page 4 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=150
2025-11-09 20:52:15,383 - INFO - root - get_all_titles_from_web 
2025-11-09 20:52:15,383 - INFO - root - Page:3, Index:0, DPASyn: Mechanism-Aware Drug Synergy Prediction via Dual Attention and Precision-Aware Quantization, https://arxiv.org/pdf/2505.19144, 2025-09-20
2025-11-09 20:52:15,383 - INFO - root - Page:3, Index:1, Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding, https://arxiv.org/pdf/2505.18758, 2025-05-24
2025-11-09 20:52:15,385 - INFO - root - Page:3, Index:2, Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization, https://arxiv.org/pdf/2505.18113, 2025-05-23
2025-11-09 20:52:15,385 - INFO - root - Page:3, Index:3, Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs, https://arxiv.org/pdf/2505.17662, 2025-09-19
2025-11-09 20:52:15,385 - INFO - root - Page:3, Index:4, FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design, https://arxiv.org/pdf/2505.16335, 2025-05-22
2025-11-09 20:52:15,385 - INFO - root - Page:3, Index:5, Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control, https://arxiv.org/pdf/2505.15304, 2025-05-30
2025-11-09 20:52:15,386 - INFO - root - Page:3, Index:6, Scaling Law for Quantization-Aware Training, https://arxiv.org/pdf/2505.14302, 2025-05-20
2025-11-09 20:52:15,386 - INFO - root - Page:3, Index:7, Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs, https://arxiv.org/pdf/2505.13060, 2025-05-19
2025-11-09 20:52:15,386 - INFO - root - Page:3, Index:8, Deep Unfolding with Kernel-based Quantization in MIMO Detection, https://arxiv.org/pdf/2505.12736, 2025-05-19
2025-11-09 20:52:15,386 - INFO - root - Page:3, Index:9, FedHQ: Hybrid Runtime Quantization for Federated Learning, https://arxiv.org/pdf/2505.11982, 2025-05-17
2025-11-09 20:52:15,386 - INFO - root - Page:3, Index:10, QVGen: Pushing the Limit of Quantized Video Generative Models, https://arxiv.org/pdf/2505.11497, 2025-09-27
2025-11-09 20:52:15,387 - INFO - root - Page:3, Index:11, EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes, https://arxiv.org/pdf/2505.10787, 2025-05-15
2025-11-09 20:52:15,387 - INFO - root - Page:3, Index:12, ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition, https://arxiv.org/pdf/2505.08981, 2025-05-13
2025-11-09 20:52:15,387 - INFO - root - Page:3, Index:13, PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs, https://arxiv.org/pdf/2505.03254, 2025-08-06
2025-11-09 20:52:15,387 - INFO - root - Page:3, Index:14, ICQuant: Index Coding enables Low-bit LLM Quantization, https://arxiv.org/pdf/2505.00850, 2025-08-24
2025-11-09 20:52:15,388 - INFO - root - Page:3, Index:15, Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining, https://arxiv.org/pdf/2504.13932, 2025-07-30
2025-11-09 20:52:15,388 - INFO - root - Page:3, Index:16, Achieving binary weight and activation for LLMs using Post-Training Quantization, https://arxiv.org/pdf/2504.05352, 2025-06-30
2025-11-09 20:52:15,388 - INFO - root - Page:3, Index:17, Wireless Hearables With Programmable Speech AI Accelerators, https://arxiv.org/pdf/2503.18698, 2025-10-21
2025-11-09 20:52:15,388 - INFO - root - Page:3, Index:18, GranQ: Efficient Channel-wise Quantization via Vectorized Pre-Scaling for Zero-Shot QAT, https://arxiv.org/pdf/2503.18339, 2025-10-15
2025-11-09 20:52:15,390 - INFO - root - Page:3, Index:19, CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting, https://arxiv.org/pdf/2503.12836, 2025-09-29
2025-11-09 20:52:15,390 - INFO - root - Page:3, Index:20, AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model, https://arxiv.org/pdf/2503.03088, 2025-07-11
2025-11-09 20:52:15,390 - INFO - root - Page:3, Index:21, Teaching Metric Distance to Discrete Autoregressive Language Models, https://arxiv.org/pdf/2503.02379, 2025-10-07
2025-11-09 20:52:15,391 - INFO - root - Page:3, Index:22, Regularization-based Framework for Quantization-, Fault- and Variability-Aware Training, https://arxiv.org/pdf/2503.01297, 2025-07-12
2025-11-09 20:52:15,391 - INFO - root - Fetching page 5 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=200
2025-11-09 20:52:21,697 - INFO - root - get_all_titles_from_web 
2025-11-09 20:52:21,697 - INFO - root - Page:4, Index:0, Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models, https://arxiv.org/pdf/2502.15799, 2025-06-29
2025-11-09 20:52:21,697 - INFO - root - Page:4, Index:1, Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer, https://arxiv.org/pdf/2502.15779, 2025-09-01
2025-11-09 20:52:21,697 - INFO - root - Page:4, Index:2, RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models, https://arxiv.org/pdf/2502.09003, 2025-06-05
2025-11-09 20:52:21,697 - INFO - root - Page:4, Index:3, Spatial Degradation-Aware and Temporal Consistent Diffusion Model for Compressed Video Super-Resolution, https://arxiv.org/pdf/2502.07381, 2025-06-27
2025-11-09 20:52:21,699 - INFO - root - Page:4, Index:4, QuEST: Stable Training of LLMs with 1-Bit Weights and Activations, https://arxiv.org/pdf/2502.05003, 2025-06-10
2025-11-09 20:52:21,699 - INFO - root - Page:4, Index:5, HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs, https://arxiv.org/pdf/2501.02625, 2025-11-05
2025-11-09 20:52:21,699 - INFO - root - Page:4, Index:6, TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction, https://arxiv.org/pdf/2412.16919, 2025-08-08
2025-11-09 20:52:21,699 - INFO - root - Page:4, Index:7, Training Multi-Layer Binary Neural Networks With Local Binary Error Signals, https://arxiv.org/pdf/2412.00119, 2025-08-05
2025-11-09 20:52:21,700 - INFO - root - Page:4, Index:8, Scaling Large-scale GNN Training to Thousands of Processors on CPU-based Supercomputers, https://arxiv.org/pdf/2411.16025, 2025-05-26
2025-11-09 20:52:21,700 - INFO - root - Page:4, Index:9, HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization, https://arxiv.org/pdf/2411.06581, 2025-05-16
2025-11-09 20:52:21,700 - INFO - root - Fetching page 6 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=250
2025-11-09 20:52:29,866 - INFO - root - get_all_titles_from_web 
2025-11-09 20:52:29,867 - INFO - root - Page:5, Index:0, Gradient-Free Training of Quantized Neural Networks, https://arxiv.org/pdf/2410.09734, 2025-09-29
2025-11-09 20:52:29,867 - INFO - root - Page:5, Index:1, QT-DoG: Quantization-aware Training for Domain Generalization, https://arxiv.org/pdf/2410.06020, 2025-06-26
2025-11-09 20:52:29,867 - INFO - root - Page:5, Index:2, Constraint Guided Model Quantization of Neural Networks, https://arxiv.org/pdf/2409.20138, 2025-09-12
2025-11-09 20:52:29,868 - INFO - root - Page:5, Index:3, Accumulator-Aware Post-Training Quantization for Large Language Models, https://arxiv.org/pdf/2409.17092, 2025-07-30
2025-11-09 20:52:29,868 - INFO - root - Page:5, Index:4, DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation, https://arxiv.org/pdf/2409.14307, 2025-07-09
2025-11-09 20:52:29,868 - INFO - root - Page:5, Index:5, Robust Training of Neural Networks at Arbitrary Precision and Sparsity, https://arxiv.org/pdf/2409.09245, 2025-09-23
2025-11-09 20:52:29,868 - INFO - root - Page:5, Index:6, VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing, https://arxiv.org/pdf/2408.05758, 2025-05-27
2025-11-09 20:52:29,873 - INFO - root - Page:5, Index:7, DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers, https://arxiv.org/pdf/2408.03291, 2025-06-19
2025-11-09 20:52:29,874 - INFO - root - Page:5, Index:8, Temporal Feature Matters: A Framework for Diffusion Model Quantization, https://arxiv.org/pdf/2407.19547, 2025-07-11
2025-11-09 20:52:29,874 - INFO - root - Page:5, Index:9, EfficientQAT: Efficient Quantization-Aware Training for Large Language Models, https://arxiv.org/pdf/2407.11062, 2025-05-19
2025-11-09 20:52:29,874 - INFO - root - Page:5, Index:10, Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT, https://arxiv.org/pdf/2407.11041, 2025-10-31
2025-11-09 20:52:29,875 - INFO - root - Fetching page 7 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=300
2025-11-09 20:52:36,995 - INFO - root - get_all_titles_from_web 
2025-11-09 20:52:36,995 - INFO - root - Page:6, Index:0, BoA: Attention-aware Post-training Quantization without Backpropagation, https://arxiv.org/pdf/2406.13474, 2025-06-06
2025-11-09 20:52:36,995 - INFO - root - Page:6, Index:1, A Rescaling-Invariant Lipschitz Bound Based on Path-Metrics for Modern ReLU Network Parameterizations, https://arxiv.org/pdf/2405.15006, 2025-06-13
2025-11-09 20:52:36,996 - INFO - root - Page:6, Index:2, Scheduling Weight Transitions for Quantization-Aware Training, https://arxiv.org/pdf/2404.19248, 2025-10-01
2025-11-09 20:52:36,996 - INFO - root - Page:6, Index:3, GPTVQ: The Blessing of Dimensionality for LLM Quantization, https://arxiv.org/pdf/2402.15319, 2025-06-03
2025-11-09 20:52:36,997 - INFO - root - Fetching page 8 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=350
2025-11-09 20:52:43,870 - INFO - root - get_all_titles_from_web 
2025-11-09 20:52:43,870 - INFO - root - Page:7, Index:0, Squat: Quant Small Language Models on the Edge, https://arxiv.org/pdf/2402.10787, 2025-07-01
2025-11-09 20:52:43,871 - INFO - root - Page:7, Index:1, L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2402.04902, 2025-07-21
2025-11-09 20:52:43,871 - INFO - root - Fetching page 9 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=400
2025-11-09 20:52:50,584 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 20:52:50,585 - INFO - root - File already exists, skipping download: d:\ChatPaper\academic Papers\Quantization-Aware-Training\A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat.pdf
2025-11-09 20:52:50,592 - INFO - root - File already exists, skipping download: d:\ChatPaper\academic Papers\Quantization-Aware-Training\FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error.pdf
2025-11-09 20:52:50,593 - INFO - root - 跳过已处理论文 A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies：d:\ChatPaper\academic Papers\Quantization-Aware-Training\A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat.pdf
2025-11-09 20:52:50,593 - INFO - root - 跳过已处理论文 FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error：d:\ChatPaper\academic Papers\Quantization-Aware-Training\FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error.pdf
2025-11-09 20:52:50,595 - INFO - root - summary time: 68.74 seconds
2025-11-09 20:53:28,383 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 20:53:28,385 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 20:53:28,391 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 20:53:29,228 - INFO - root - LLMClientManager: 指定使用客户端: DeepSeek
2025-11-09 20:53:29,229 - INFO - root - DeepSeekClient: API key found: 9f9e270e-b...
2025-11-09 20:53:29,229 - INFO - root - DeepSeekClient: 使用模式: 火山引擎
2025-11-09 20:53:29,230 - INFO - root - DeepSeekClient: 模型名称: deepseek-v3-1-terminus
2025-11-09 20:53:32,466 - INFO - httpx - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-11-09 20:53:32,475 - INFO - root - DeepSeekClient: initialized successfully with model deepseek-v3-1-terminus
2025-11-09 20:53:32,477 - INFO - root - LLMClientManager: DeepSeek 客户端初始化成功
2025-11-09 20:53:32,477 - INFO - root - LLMClientManager: switched to DeepSeek client
2025-11-09 20:53:32,477 - INFO - root - 已手动切换到 LLM 客户端: DeepSeek
2025-11-09 20:53:32,478 - INFO - root - 使用 LLM 模型: deepseek-v3-1-terminus
2025-11-09 20:53:32,478 - INFO - root - 可用客户端: ['DeepSeek']
2025-11-09 20:53:32,478 - INFO - root - === 运行配置 ===
2025-11-09 20:53:32,479 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 20:53:32,479 - INFO - root - 关键词: QAT
2025-11-09 20:53:32,479 - INFO - root - 查询: Quantization-Aware-Training
2025-11-09 20:53:32,479 - INFO - root - 排序: None
2025-11-09 20:53:32,480 - INFO - root - 最近天数: 180
2025-11-09 20:53:32,480 - INFO - root - 最大处理数量: 2
2025-11-09 20:53:32,481 - INFO - root - 保存图片: 是
2025-11-09 20:53:32,481 - INFO - root - 输出语言: 中文
2025-11-09 20:53:32,482 - INFO - root - 强制重新处理: 否
2025-11-09 20:53:32,482 - INFO - root - ====================
2025-11-09 20:53:32,483 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 20:53:32,483 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 20:53:59,745 - INFO - root - 尝试使用 utf-8 编码读取配置文件...
2025-11-09 20:53:59,747 - INFO - root - 成功使用 utf-8 编码读取配置文件
2025-11-09 20:53:59,748 - INFO - root - 已将配置文件转换为 UTF-8 编码
2025-11-09 20:54:00,555 - INFO - root - LLMClientManager: 指定使用客户端: Gemini
2025-11-09 20:54:01,669 - INFO - root - GeminiClient: trying model gemini-2.5-flash
2025-11-09 20:54:04,999 - INFO - root - GeminiClient: initialized model gemini-2.5-flash
2025-11-09 20:54:04,999 - INFO - root - LLMClientManager: Gemini 客户端初始化成功
2025-11-09 20:54:04,999 - INFO - root - LLMClientManager: switched to Gemini client
2025-11-09 20:54:05,000 - INFO - root - 已手动切换到 LLM 客户端: Gemini
2025-11-09 20:54:05,000 - INFO - root - 使用 LLM 模型: gemini-2.5-flash
2025-11-09 20:54:05,000 - INFO - root - 可用客户端: ['Gemini']
2025-11-09 20:54:05,001 - INFO - root - === 运行配置 ===
2025-11-09 20:54:05,001 - INFO - root - 处理模式: arxiv在线搜索
2025-11-09 20:54:05,001 - INFO - root - 关键词: QAT
2025-11-09 20:54:05,001 - INFO - root - 查询: Quantization-Aware-Training
2025-11-09 20:54:05,002 - INFO - root - 排序: None
2025-11-09 20:54:05,003 - INFO - root - 最近天数: 180
2025-11-09 20:54:05,003 - INFO - root - 最大处理数量: 2
2025-11-09 20:54:05,003 - INFO - root - 保存图片: 是
2025-11-09 20:54:05,003 - INFO - root - 输出语言: 中文
2025-11-09 20:54:05,004 - INFO - root - 强制重新处理: 否
2025-11-09 20:54:05,004 - INFO - root - ====================
2025-11-09 20:54:05,004 - INFO - root - 使用 arXiv 搜索模式（通过 chat_arxiv 模块获取）
2025-11-09 20:54:05,005 - INFO - root - Fetching page 1 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50
2025-11-09 20:54:12,438 - INFO - root - get_all_titles_from_web 
2025-11-09 20:54:12,440 - INFO - root - Page:0, Index:0, A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies, https://arxiv.org/pdf/2511.03201, 2025-11-05
2025-11-09 20:54:12,440 - INFO - root - Page:0, Index:1, FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error, https://arxiv.org/pdf/2511.02302, 2025-11-04
2025-11-09 20:54:12,440 - INFO - root - Page:0, Index:2, Outlier-Aware Post-Training Quantization for Image Super-Resolution, https://arxiv.org/pdf/2511.00682, 2025-11-01
2025-11-09 20:54:12,442 - INFO - root - Page:0, Index:3, Evaluation of Wafer-Scale SOT-MRAM for Analog Crossbar Array Applications, https://arxiv.org/pdf/2510.25853, 2025-11-03
2025-11-09 20:54:12,443 - INFO - root - Page:0, Index:4, Improving the Straight-Through Estimator with Zeroth-Order Information, https://arxiv.org/pdf/2510.23926, 2025-10-27
2025-11-09 20:54:12,444 - INFO - root - Page:0, Index:5, Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework, https://arxiv.org/pdf/2510.22243, 2025-10-25
2025-11-09 20:54:12,445 - INFO - root - Page:0, Index:6, TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge, https://arxiv.org/pdf/2510.21879, 2025-10-23
2025-11-09 20:54:12,446 - INFO - root - Page:0, Index:7, KARIPAP: Quantum-Inspired Tensor Network Compression of Large Language Models Using Infinite Projected Entangled Pair States and Tensor Renormalization Group, https://arxiv.org/pdf/2510.21844, 2025-10-22
2025-11-09 20:54:12,452 - INFO - root - Page:0, Index:8, A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization, https://arxiv.org/pdf/2510.21314, 2025-10-24
2025-11-09 20:54:12,452 - INFO - root - Page:0, Index:9, Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks, https://arxiv.org/pdf/2510.19760, 2025-10-22
2025-11-09 20:54:12,454 - INFO - root - Page:0, Index:10, CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training, https://arxiv.org/pdf/2510.18784, 2025-10-21
2025-11-09 20:54:12,455 - INFO - root - Page:0, Index:11, Mixed-Precision Quantization for Language Models: Techniques and Prospects, https://arxiv.org/pdf/2510.16805, 2025-10-19
2025-11-09 20:54:12,455 - INFO - root - Page:0, Index:12, SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation, https://arxiv.org/pdf/2510.16396, 2025-10-30
2025-11-09 20:54:12,455 - INFO - root - Page:0, Index:13, CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models, https://arxiv.org/pdf/2510.15962, 2025-10-11
2025-11-09 20:54:12,456 - INFO - root - Page:0, Index:14, SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization, https://arxiv.org/pdf/2510.15775, 2025-10-17
2025-11-09 20:54:12,456 - INFO - root - Page:0, Index:15, SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware, https://arxiv.org/pdf/2510.15542, 2025-10-31
2025-11-09 20:54:12,458 - INFO - root - Page:0, Index:16, GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework, https://arxiv.org/pdf/2510.15299, 2025-10-17
2025-11-09 20:54:12,460 - INFO - root - Page:0, Index:17, SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images, https://arxiv.org/pdf/2510.15072, 2025-10-16
2025-11-09 20:54:12,461 - INFO - root - Page:0, Index:18, FraQAT: Quantization Aware Training with Fractional bits, https://arxiv.org/pdf/2510.14823, 2025-10-16
2025-11-09 20:54:12,462 - INFO - root - Page:0, Index:19, Computing-In-Memory Aware Model Adaption For Edge Devices, https://arxiv.org/pdf/2510.14379, 2025-10-16
2025-11-09 20:54:12,463 - INFO - root - Page:0, Index:20, Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge, https://arxiv.org/pdf/2510.13760, 2025-11-04
2025-11-09 20:54:12,467 - INFO - root - Page:0, Index:21, NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models, https://arxiv.org/pdf/2510.13068, 2025-10-14
2025-11-09 20:54:12,467 - INFO - root - Page:0, Index:22, Detect Anything via Next Point Prediction, https://arxiv.org/pdf/2510.12798, 2025-10-14
2025-11-09 20:54:12,469 - INFO - root - Page:0, Index:23, AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model, https://arxiv.org/pdf/2510.11496, 2025-10-14
2025-11-09 20:54:12,469 - INFO - root - Page:0, Index:24, Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware, https://arxiv.org/pdf/2510.11484, 2025-10-13
2025-11-09 20:54:12,471 - INFO - root - Page:0, Index:25, SASER: Stego attacks on open-source LLMs, https://arxiv.org/pdf/2510.10486, 2025-10-12
2025-11-09 20:54:12,472 - INFO - root - Page:0, Index:26, Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition, https://arxiv.org/pdf/2510.09653, 2025-10-15
2025-11-09 20:54:12,472 - INFO - root - Page:0, Index:27, A Theoretically-Grounded Codebook for Digital Semantic Communications, https://arxiv.org/pdf/2510.07108, 2025-10-14
2025-11-09 20:54:12,472 - INFO - root - Page:0, Index:28, QuantDemoire: Quantization with Outlier Aware for Image Demoiréing, https://arxiv.org/pdf/2510.04066, 2025-10-05
2025-11-09 20:54:12,473 - INFO - root - Page:0, Index:29, Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization, https://arxiv.org/pdf/2510.03763, 2025-10-04
2025-11-09 20:54:12,474 - INFO - root - Page:0, Index:30, Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models, https://arxiv.org/pdf/2510.03274, 2025-09-27
2025-11-09 20:54:12,474 - INFO - root - Page:0, Index:31, PT$^2$-LLM: Post-Training Ternarization for Large Language Models, https://arxiv.org/pdf/2510.03267, 2025-09-26
2025-11-09 20:54:12,475 - INFO - root - Page:0, Index:32, Purrception: Variational Flow Matching for Vector-Quantized Image Generation, https://arxiv.org/pdf/2510.01478, 2025-10-01
2025-11-09 20:54:12,479 - INFO - root - Page:0, Index:33, Post-Training Quantization for Audio Diffusion Transformers, https://arxiv.org/pdf/2510.00313, 2025-09-30
2025-11-09 20:54:12,488 - INFO - root - Page:0, Index:34, Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling, https://arxiv.org/pdf/2510.00028, 2025-09-25
2025-11-09 20:54:12,488 - INFO - root - Page:0, Index:35, Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models, https://arxiv.org/pdf/2509.26436, 2025-09-30
2025-11-09 20:54:12,489 - INFO - root - Page:0, Index:36, Cat: Post-Training Quantization Error Reduction via Cluster-based Affine Transformation, https://arxiv.org/pdf/2509.26277, 2025-10-07
2025-11-09 20:54:12,489 - INFO - root - Page:0, Index:37, CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models, https://arxiv.org/pdf/2509.25996, 2025-09-30
2025-11-09 20:54:12,490 - INFO - root - Page:0, Index:38, Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications, https://arxiv.org/pdf/2509.25439, 2025-09-29
2025-11-09 20:54:12,491 - INFO - root - Page:0, Index:39, On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs, https://arxiv.org/pdf/2509.25214, 2025-09-22
2025-11-09 20:54:12,494 - INFO - root - Page:0, Index:40, VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning, https://arxiv.org/pdf/2509.24650, 2025-09-29
2025-11-09 20:54:12,496 - INFO - root - Page:0, Index:41, S$^2$NN: Sub-bit Spiking Neural Networks, https://arxiv.org/pdf/2509.24266, 2025-10-24
2025-11-09 20:54:12,503 - INFO - root - Page:0, Index:42, Tequila: Trapping-free Ternary Quantization for Large Language Models, https://arxiv.org/pdf/2509.23809, 2025-10-17
2025-11-09 20:54:12,503 - INFO - root - Page:0, Index:43, Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution, https://arxiv.org/pdf/2509.23774, 2025-09-30
2025-11-09 20:54:12,504 - INFO - root - Page:0, Index:44, RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization, https://arxiv.org/pdf/2509.23582, 2025-09-27
2025-11-09 20:54:12,507 - INFO - root - Page:0, Index:45, Beyond Outliers: A Study of Optimizers Under Quantization, https://arxiv.org/pdf/2509.23500, 2025-10-02
2025-11-09 20:54:12,529 - INFO - root - Page:0, Index:46, Compute-Optimal Quantization-Aware Training, https://arxiv.org/pdf/2509.22935, 2025-09-26
2025-11-09 20:54:12,538 - INFO - root - Page:0, Index:47, COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning, https://arxiv.org/pdf/2509.22075, 2025-10-06
2025-11-09 20:54:12,538 - INFO - root - Page:0, Index:48, SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models, https://arxiv.org/pdf/2509.21498, 2025-09-25
2025-11-09 20:54:12,539 - INFO - root - Page:0, Index:49, Quantized Visual Geometry Grounded Transformer, https://arxiv.org/pdf/2509.21302, 2025-09-29
2025-11-09 20:54:12,540 - INFO - root - Fetching page 2 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50
2025-11-09 20:54:19,046 - INFO - root - get_all_titles_from_web 
2025-11-09 20:54:19,046 - INFO - root - Page:1, Index:0, Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy, https://arxiv.org/pdf/2509.21173, 2025-10-27
2025-11-09 20:54:19,046 - INFO - root - Page:1, Index:1, Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer, https://arxiv.org/pdf/2509.20854, 2025-09-25
2025-11-09 20:54:19,046 - INFO - root - Page:1, Index:2, TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection, https://arxiv.org/pdf/2509.18193, 2025-09-19
2025-11-09 20:54:19,048 - INFO - root - Page:1, Index:3, SBVR: Summation of BitVector Representation for Efficient LLM Quantization, https://arxiv.org/pdf/2509.18172, 2025-09-17
2025-11-09 20:54:19,048 - INFO - root - Page:1, Index:4, QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2509.17428, 2025-09-26
2025-11-09 20:54:19,048 - INFO - root - Page:1, Index:5, PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models, https://arxiv.org/pdf/2509.16989, 2025-10-28
2025-11-09 20:54:19,048 - INFO - root - Page:1, Index:6, MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training, https://arxiv.org/pdf/2509.15514, 2025-09-18
2025-11-09 20:54:19,048 - INFO - root - Page:1, Index:7, Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs, https://arxiv.org/pdf/2509.14391, 2025-09-17
2025-11-09 20:54:19,049 - INFO - root - Page:1, Index:8, Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization, https://arxiv.org/pdf/2509.13786, 2025-09-19
2025-11-09 20:54:19,049 - INFO - root - Page:1, Index:9, Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees, https://arxiv.org/pdf/2509.11054, 2025-09-13
2025-11-09 20:54:19,049 - INFO - root - Page:1, Index:10, SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models, https://arxiv.org/pdf/2509.09090, 2025-09-10
2025-11-09 20:54:19,049 - INFO - root - Page:1, Index:11, CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks with Entropy Regularization, https://arxiv.org/pdf/2509.08776, 2025-09-10
2025-11-09 20:54:19,050 - INFO - root - Page:1, Index:12, Explaining How Quantization Disparately Skews a Model, https://arxiv.org/pdf/2509.07222, 2025-09-08
2025-11-09 20:54:19,050 - INFO - root - Page:1, Index:13, LoaQ: Layer-wise Output Approximation Quantization, https://arxiv.org/pdf/2509.06297, 2025-09-07
2025-11-09 20:54:19,050 - INFO - root - Page:1, Index:14, FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving, https://arxiv.org/pdf/2509.06261, 2025-09-14
2025-11-09 20:54:19,051 - INFO - root - Page:1, Index:15, Sensitivity-Aware Post-Training Quantization for Deep Neural Networks, https://arxiv.org/pdf/2509.05576, 2025-09-05
2025-11-09 20:54:19,051 - INFO - root - Page:1, Index:16, SuperSNN: A Hardware-Aware Framework for Physically Realizable, High-Performance Superconducting Spiking Neural Network Chips, https://arxiv.org/pdf/2509.05532, 2025-09-05
2025-11-09 20:54:19,051 - INFO - root - Page:1, Index:17, Data-Augmented Quantization-Aware Knowledge Distillation, https://arxiv.org/pdf/2509.03850, 2025-09-03
2025-11-09 20:54:19,051 - INFO - root - Page:1, Index:18, DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling, https://arxiv.org/pdf/2509.03472, 2025-09-03
2025-11-09 20:54:19,052 - INFO - root - Page:1, Index:19, Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs, https://arxiv.org/pdf/2509.02017, 2025-09-02
2025-11-09 20:54:19,052 - INFO - root - Page:1, Index:20, Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling, https://arxiv.org/pdf/2509.01624, 2025-09-01
2025-11-09 20:54:19,052 - INFO - root - Page:1, Index:21, Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective, https://arxiv.org/pdf/2509.00859, 2025-08-31
2025-11-09 20:54:19,054 - INFO - root - Page:1, Index:22, Progressive Element-wise Gradient Estimation for Neural Network Quantization, https://arxiv.org/pdf/2509.00097, 2025-08-27
2025-11-09 20:54:19,056 - INFO - root - Page:1, Index:23, End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost, https://arxiv.org/pdf/2509.00031, 2025-09-29
2025-11-09 20:54:19,056 - INFO - root - Page:1, Index:24, Quantization Robustness to Input Degradations for Object Detection, https://arxiv.org/pdf/2508.19600, 2025-08-27
2025-11-09 20:54:19,056 - INFO - root - Page:1, Index:25, Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models, https://arxiv.org/pdf/2508.18609, 2025-08-27
2025-11-09 20:54:19,057 - INFO - root - Page:1, Index:26, AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration, https://arxiv.org/pdf/2508.18025, 2025-08-25
2025-11-09 20:54:19,059 - INFO - root - Page:1, Index:27, TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling, https://arxiv.org/pdf/2508.16790, 2025-08-22
2025-11-09 20:54:19,060 - INFO - root - Page:1, Index:28, A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering, https://arxiv.org/pdf/2508.16516, 2025-08-22
2025-11-09 20:54:19,061 - INFO - root - Page:1, Index:29, JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs, https://arxiv.org/pdf/2508.15468, 2025-08-21
2025-11-09 20:54:19,061 - INFO - root - Page:1, Index:30, MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation, https://arxiv.org/pdf/2508.15281, 2025-08-21
2025-11-09 20:54:19,062 - INFO - root - Page:1, Index:31, DLLMQuant: Quantizing Diffusion-based Large Language Models, https://arxiv.org/pdf/2508.14090, 2025-08-25
2025-11-09 20:54:19,062 - INFO - root - Page:1, Index:32, Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management, https://arxiv.org/pdf/2508.13905, 2025-08-19
2025-11-09 20:54:19,062 - INFO - root - Page:1, Index:33, XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads, https://arxiv.org/pdf/2508.13049, 2025-08-18
2025-11-09 20:54:19,064 - INFO - root - Page:1, Index:34, Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection, https://arxiv.org/pdf/2508.12230, 2025-08-17
2025-11-09 20:54:19,064 - INFO - root - Page:1, Index:35, TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks, https://arxiv.org/pdf/2508.12132, 2025-08-16
2025-11-09 20:54:19,064 - INFO - root - Page:1, Index:36, Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion, https://arxiv.org/pdf/2508.12094, 2025-10-13
2025-11-09 20:54:19,064 - INFO - root - Page:1, Index:37, Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware, https://arxiv.org/pdf/2508.11940, 2025-08-16
2025-11-09 20:54:19,064 - INFO - root - Page:1, Index:38, PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks, https://arxiv.org/pdf/2508.10557, 2025-08-15
2025-11-09 20:54:19,065 - INFO - root - Page:1, Index:39, MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI, https://arxiv.org/pdf/2508.09500, 2025-08-13
2025-11-09 20:54:19,065 - INFO - root - Page:1, Index:40, SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via Codebooks for recommender system, https://arxiv.org/pdf/2508.09090, 2025-08-12
2025-11-09 20:54:19,066 - INFO - root - Page:1, Index:41, Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models, https://arxiv.org/pdf/2508.06974, 2025-08-09
2025-11-09 20:54:19,066 - INFO - root - Page:1, Index:42, Efficient Deep Neural Receiver with Post-Training Quantization, https://arxiv.org/pdf/2508.06275, 2025-08-08
2025-11-09 20:54:19,066 - INFO - root - Page:1, Index:43, iFairy: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$, https://arxiv.org/pdf/2508.05571, 2025-08-16
2025-11-09 20:54:19,067 - INFO - root - Page:1, Index:44, S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation, https://arxiv.org/pdf/2508.04016, 2025-10-27
2025-11-09 20:54:19,067 - INFO - root - Page:1, Index:45, LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation, https://arxiv.org/pdf/2508.03485, 2025-09-23
2025-11-09 20:54:19,067 - INFO - root - Page:1, Index:46, VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation, https://arxiv.org/pdf/2508.03351, 2025-08-05
2025-11-09 20:54:19,068 - INFO - root - Page:1, Index:47, TF-MLPNet: Tiny Real-Time Neural Speech Separation, https://arxiv.org/pdf/2508.03047, 2025-08-04
2025-11-09 20:54:19,068 - INFO - root - Page:1, Index:48, SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation, https://arxiv.org/pdf/2508.01375, 2025-08-02
2025-11-09 20:54:19,069 - INFO - root - Page:1, Index:49, MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective, https://arxiv.org/pdf/2507.19131, 2025-07-25
2025-11-09 20:54:19,069 - INFO - root - Fetching page 3 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=100
2025-11-09 20:54:25,722 - INFO - root - get_all_titles_from_web 
2025-11-09 20:54:25,722 - INFO - root - Page:2, Index:0, Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction, https://arxiv.org/pdf/2507.17768, 2025-07-16
2025-11-09 20:54:25,724 - INFO - root - Page:2, Index:1, SiLQ: Simple Large Language Model Quantization-Aware Training, https://arxiv.org/pdf/2507.16933, 2025-07-22
2025-11-09 20:54:25,724 - INFO - root - Page:2, Index:2, Task-Specific Zero-shot Quantization-Aware Training for Object Detection, https://arxiv.org/pdf/2507.16782, 2025-07-22
2025-11-09 20:54:25,724 - INFO - root - Page:2, Index:3, TorchAO: PyTorch-Native Training-to-Serving Model Optimization, https://arxiv.org/pdf/2507.16099, 2025-07-21
2025-11-09 20:54:25,725 - INFO - root - Page:2, Index:4, SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models, https://arxiv.org/pdf/2507.14811, 2025-08-27
2025-11-09 20:54:25,725 - INFO - root - Page:2, Index:5, Apple Intelligence Foundation Language Models: Tech Report 2025, https://arxiv.org/pdf/2507.13575, 2025-08-27
2025-11-09 20:54:25,725 - INFO - root - Page:2, Index:6, Generative Multi-Target Cross-Domain Recommendation, https://arxiv.org/pdf/2507.12871, 2025-08-07
2025-11-09 20:54:25,725 - INFO - root - Page:2, Index:7, DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training, https://arxiv.org/pdf/2507.07149, 2025-07-09
2025-11-09 20:54:25,726 - INFO - root - Page:2, Index:8, Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics, https://arxiv.org/pdf/2507.07044, 2025-07-15
2025-11-09 20:54:25,727 - INFO - root - Page:2, Index:9, QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models, https://arxiv.org/pdf/2507.06079, 2025-07-08
2025-11-09 20:54:25,729 - INFO - root - Page:2, Index:10, GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field, https://arxiv.org/pdf/2507.05594, 2025-07-07
2025-11-09 20:54:25,732 - INFO - root - Page:2, Index:11, Hita: Holistic Tokenizer for Autoregressive Image Generation, https://arxiv.org/pdf/2507.02358, 2025-07-11
2025-11-09 20:54:25,733 - INFO - root - Page:2, Index:12, QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference, https://arxiv.org/pdf/2506.23934, 2025-06-30
2025-11-09 20:54:25,733 - INFO - root - Page:2, Index:13, FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization, https://arxiv.org/pdf/2506.23516, 2025-07-21
2025-11-09 20:54:25,735 - INFO - root - Page:2, Index:14, Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models, https://arxiv.org/pdf/2506.23025, 2025-06-28
2025-11-09 20:54:25,735 - INFO - root - Page:2, Index:15, Joint Quantization and Pruning Neural Networks Approach: A Case Study on FSO Receivers, https://arxiv.org/pdf/2506.20084, 2025-06-24
2025-11-09 20:54:25,736 - INFO - root - Page:2, Index:16, Single-step Diffusion for Image Compression at Ultra-Low Bitrates, https://arxiv.org/pdf/2506.16572, 2025-09-22
2025-11-09 20:54:25,736 - INFO - root - Page:2, Index:17, LittleBit: Ultra Low-Bit Quantization via Latent Factorization, https://arxiv.org/pdf/2506.13771, 2025-10-28
2025-11-09 20:54:25,736 - INFO - root - Page:2, Index:18, MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering, https://arxiv.org/pdf/2506.13755, 2025-06-16
2025-11-09 20:54:25,737 - INFO - root - Page:2, Index:19, EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization, https://arxiv.org/pdf/2506.13329, 2025-07-04
2025-11-09 20:54:25,737 - INFO - root - Page:2, Index:20, Towards Neural Audio Codec Source Parsing, https://arxiv.org/pdf/2506.12627, 2025-06-14
2025-11-09 20:54:25,737 - INFO - root - Page:2, Index:21, Quantizing Small-Scale State-Space Models for Edge AI, https://arxiv.org/pdf/2506.12480, 2025-06-14
2025-11-09 20:54:25,738 - INFO - root - Page:2, Index:22, EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction, https://arxiv.org/pdf/2506.12015, 2025-06-13
2025-11-09 20:54:25,738 - INFO - root - Page:2, Index:23, Compression Aware Certified Training, https://arxiv.org/pdf/2506.11992, 2025-06-13
2025-11-09 20:54:25,738 - INFO - root - Page:2, Index:24, GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers, https://arxiv.org/pdf/2506.11784, 2025-06-13
2025-11-09 20:54:25,738 - INFO - root - Page:2, Index:25, TruncQuant: Truncation-Ready Quantization for DNNs with Flexible Weight Bit Precision, https://arxiv.org/pdf/2506.11431, 2025-06-12
2025-11-09 20:54:25,739 - INFO - root - Page:2, Index:26, EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices, https://arxiv.org/pdf/2506.11093, 2025-06-05
2025-11-09 20:54:25,739 - INFO - root - Page:2, Index:27, Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization, https://arxiv.org/pdf/2506.10463, 2025-06-12
2025-11-09 20:54:25,741 - INFO - root - Page:2, Index:28, AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent, https://arxiv.org/pdf/2506.10205, 2025-06-11
2025-11-09 20:54:25,741 - INFO - root - Page:2, Index:29, Q-SAM2: Accurate Quantization for Segment Anything Model 2, https://arxiv.org/pdf/2506.09782, 2025-06-11
2025-11-09 20:54:25,741 - INFO - root - Page:2, Index:30, Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs, https://arxiv.org/pdf/2506.09104, 2025-06-10
2025-11-09 20:54:25,742 - INFO - root - Page:2, Index:31, Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU, https://arxiv.org/pdf/2506.08911, 2025-06-10
2025-11-09 20:54:25,742 - INFO - root - Page:2, Index:32, POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration, https://arxiv.org/pdf/2506.08785, 2025-06-10
2025-11-09 20:54:25,743 - INFO - root - Page:2, Index:33, MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts, https://arxiv.org/pdf/2506.07533, 2025-06-09
2025-11-09 20:54:25,743 - INFO - root - Page:2, Index:34, BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation, https://arxiv.org/pdf/2506.07530, 2025-06-09
2025-11-09 20:54:25,744 - INFO - root - Page:2, Index:35, RecGPT: A Foundation Model for Sequential Recommendation, https://arxiv.org/pdf/2506.06270, 2025-06-12
2025-11-09 20:54:25,748 - INFO - root - Page:2, Index:36, FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion, https://arxiv.org/pdf/2506.04648, 2025-06-05
2025-11-09 20:54:25,748 - INFO - root - Page:2, Index:37, BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing, https://arxiv.org/pdf/2506.03515, 2025-06-03
2025-11-09 20:54:25,749 - INFO - root - Page:2, Index:38, ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding, https://arxiv.org/pdf/2506.01853, 2025-06-02
2025-11-09 20:54:25,749 - INFO - root - Page:2, Index:39, Movable Antenna Enhanced Federated Fine-Tuning of Large Language Models via Hybrid Client Selection Optimization, https://arxiv.org/pdf/2506.00011, 2025-10-26
2025-11-09 20:54:25,750 - INFO - root - Page:2, Index:40, Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach, https://arxiv.org/pdf/2505.24721, 2025-05-30
2025-11-09 20:54:25,750 - INFO - root - Page:2, Index:41, GARLIC: GAussian Representation LearnIng for spaCe partitioning, https://arxiv.org/pdf/2505.24608, 2025-10-02
2025-11-09 20:54:25,751 - INFO - root - Page:2, Index:42, AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up, https://arxiv.org/pdf/2505.24584, 2025-08-18
2025-11-09 20:54:25,751 - INFO - root - Page:2, Index:43, Model-Preserving Adaptive Rounding, https://arxiv.org/pdf/2505.22988, 2025-09-25
2025-11-09 20:54:25,753 - INFO - root - Page:2, Index:44, Highly Efficient and Effective LLMs with Multi-Boolean Architectures, https://arxiv.org/pdf/2505.22811, 2025-10-03
2025-11-09 20:54:25,774 - INFO - root - Page:2, Index:45, Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning, https://arxiv.org/pdf/2505.21591, 2025-05-27
2025-11-09 20:54:25,774 - INFO - root - Page:2, Index:46, Frequency Composition for Compressed and Domain-Adaptive Neural Networks, https://arxiv.org/pdf/2505.20890, 2025-05-27
2025-11-09 20:54:25,774 - INFO - root - Page:2, Index:47, MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition, https://arxiv.org/pdf/2505.20744, 2025-10-27
2025-11-09 20:54:25,775 - INFO - root - Page:2, Index:48, Optimizing edge AI models on HPC systems with the edge in the loop, https://arxiv.org/pdf/2505.19995, 2025-05-26
2025-11-09 20:54:25,775 - INFO - root - Fetching page 4 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=150
2025-11-09 20:54:33,499 - INFO - root - get_all_titles_from_web 
2025-11-09 20:54:33,500 - INFO - root - Page:3, Index:0, DPASyn: Mechanism-Aware Drug Synergy Prediction via Dual Attention and Precision-Aware Quantization, https://arxiv.org/pdf/2505.19144, 2025-09-20
2025-11-09 20:54:33,500 - INFO - root - Page:3, Index:1, Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding, https://arxiv.org/pdf/2505.18758, 2025-05-24
2025-11-09 20:54:33,500 - INFO - root - Page:3, Index:2, Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization, https://arxiv.org/pdf/2505.18113, 2025-05-23
2025-11-09 20:54:33,501 - INFO - root - Page:3, Index:3, Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs, https://arxiv.org/pdf/2505.17662, 2025-09-19
2025-11-09 20:54:33,501 - INFO - root - Page:3, Index:4, FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design, https://arxiv.org/pdf/2505.16335, 2025-05-22
2025-11-09 20:54:33,501 - INFO - root - Page:3, Index:5, Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control, https://arxiv.org/pdf/2505.15304, 2025-05-30
2025-11-09 20:54:33,502 - INFO - root - Page:3, Index:6, Scaling Law for Quantization-Aware Training, https://arxiv.org/pdf/2505.14302, 2025-05-20
2025-11-09 20:54:33,503 - INFO - root - Page:3, Index:7, Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs, https://arxiv.org/pdf/2505.13060, 2025-05-19
2025-11-09 20:54:33,503 - INFO - root - Page:3, Index:8, Deep Unfolding with Kernel-based Quantization in MIMO Detection, https://arxiv.org/pdf/2505.12736, 2025-05-19
2025-11-09 20:54:33,504 - INFO - root - Page:3, Index:9, FedHQ: Hybrid Runtime Quantization for Federated Learning, https://arxiv.org/pdf/2505.11982, 2025-05-17
2025-11-09 20:54:33,504 - INFO - root - Page:3, Index:10, QVGen: Pushing the Limit of Quantized Video Generative Models, https://arxiv.org/pdf/2505.11497, 2025-09-27
2025-11-09 20:54:33,504 - INFO - root - Page:3, Index:11, EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes, https://arxiv.org/pdf/2505.10787, 2025-05-15
2025-11-09 20:54:33,505 - INFO - root - Page:3, Index:12, ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition, https://arxiv.org/pdf/2505.08981, 2025-05-13
2025-11-09 20:54:33,505 - INFO - root - Page:3, Index:13, PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs, https://arxiv.org/pdf/2505.03254, 2025-08-06
2025-11-09 20:54:33,505 - INFO - root - Page:3, Index:14, ICQuant: Index Coding enables Low-bit LLM Quantization, https://arxiv.org/pdf/2505.00850, 2025-08-24
2025-11-09 20:54:33,507 - INFO - root - Page:3, Index:15, Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining, https://arxiv.org/pdf/2504.13932, 2025-07-30
2025-11-09 20:54:33,508 - INFO - root - Page:3, Index:16, Achieving binary weight and activation for LLMs using Post-Training Quantization, https://arxiv.org/pdf/2504.05352, 2025-06-30
2025-11-09 20:54:33,508 - INFO - root - Page:3, Index:17, Wireless Hearables With Programmable Speech AI Accelerators, https://arxiv.org/pdf/2503.18698, 2025-10-21
2025-11-09 20:54:33,509 - INFO - root - Page:3, Index:18, GranQ: Efficient Channel-wise Quantization via Vectorized Pre-Scaling for Zero-Shot QAT, https://arxiv.org/pdf/2503.18339, 2025-10-15
2025-11-09 20:54:33,509 - INFO - root - Page:3, Index:19, CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting, https://arxiv.org/pdf/2503.12836, 2025-09-29
2025-11-09 20:54:33,509 - INFO - root - Page:3, Index:20, AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model, https://arxiv.org/pdf/2503.03088, 2025-07-11
2025-11-09 20:54:33,510 - INFO - root - Page:3, Index:21, Teaching Metric Distance to Discrete Autoregressive Language Models, https://arxiv.org/pdf/2503.02379, 2025-10-07
2025-11-09 20:54:33,512 - INFO - root - Page:3, Index:22, Regularization-based Framework for Quantization-, Fault- and Variability-Aware Training, https://arxiv.org/pdf/2503.01297, 2025-07-12
2025-11-09 20:54:33,513 - INFO - root - Fetching page 5 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=200
2025-11-09 20:54:39,936 - INFO - root - get_all_titles_from_web 
2025-11-09 20:54:39,937 - INFO - root - Page:4, Index:0, Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models, https://arxiv.org/pdf/2502.15799, 2025-06-29
2025-11-09 20:54:39,937 - INFO - root - Page:4, Index:1, Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer, https://arxiv.org/pdf/2502.15779, 2025-09-01
2025-11-09 20:54:39,937 - INFO - root - Page:4, Index:2, RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models, https://arxiv.org/pdf/2502.09003, 2025-06-05
2025-11-09 20:54:39,937 - INFO - root - Page:4, Index:3, Spatial Degradation-Aware and Temporal Consistent Diffusion Model for Compressed Video Super-Resolution, https://arxiv.org/pdf/2502.07381, 2025-06-27
2025-11-09 20:54:39,938 - INFO - root - Page:4, Index:4, QuEST: Stable Training of LLMs with 1-Bit Weights and Activations, https://arxiv.org/pdf/2502.05003, 2025-06-10
2025-11-09 20:54:39,938 - INFO - root - Page:4, Index:5, HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs, https://arxiv.org/pdf/2501.02625, 2025-11-05
2025-11-09 20:54:39,938 - INFO - root - Page:4, Index:6, TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction, https://arxiv.org/pdf/2412.16919, 2025-08-08
2025-11-09 20:54:39,939 - INFO - root - Page:4, Index:7, Training Multi-Layer Binary Neural Networks With Local Binary Error Signals, https://arxiv.org/pdf/2412.00119, 2025-08-05
2025-11-09 20:54:39,939 - INFO - root - Page:4, Index:8, Scaling Large-scale GNN Training to Thousands of Processors on CPU-based Supercomputers, https://arxiv.org/pdf/2411.16025, 2025-05-26
2025-11-09 20:54:39,939 - INFO - root - Page:4, Index:9, HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization, https://arxiv.org/pdf/2411.06581, 2025-05-16
2025-11-09 20:54:39,939 - INFO - root - Fetching page 6 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=250
2025-11-09 20:54:47,345 - INFO - root - get_all_titles_from_web 
2025-11-09 20:54:47,345 - INFO - root - Page:5, Index:0, Gradient-Free Training of Quantized Neural Networks, https://arxiv.org/pdf/2410.09734, 2025-09-29
2025-11-09 20:54:47,346 - INFO - root - Page:5, Index:1, QT-DoG: Quantization-aware Training for Domain Generalization, https://arxiv.org/pdf/2410.06020, 2025-06-26
2025-11-09 20:54:47,346 - INFO - root - Page:5, Index:2, Constraint Guided Model Quantization of Neural Networks, https://arxiv.org/pdf/2409.20138, 2025-09-12
2025-11-09 20:54:47,347 - INFO - root - Page:5, Index:3, Accumulator-Aware Post-Training Quantization for Large Language Models, https://arxiv.org/pdf/2409.17092, 2025-07-30
2025-11-09 20:54:47,347 - INFO - root - Page:5, Index:4, DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation, https://arxiv.org/pdf/2409.14307, 2025-07-09
2025-11-09 20:54:47,347 - INFO - root - Page:5, Index:5, Robust Training of Neural Networks at Arbitrary Precision and Sparsity, https://arxiv.org/pdf/2409.09245, 2025-09-23
2025-11-09 20:54:47,347 - INFO - root - Page:5, Index:6, VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing, https://arxiv.org/pdf/2408.05758, 2025-05-27
2025-11-09 20:54:47,348 - INFO - root - Page:5, Index:7, DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers, https://arxiv.org/pdf/2408.03291, 2025-06-19
2025-11-09 20:54:47,349 - INFO - root - Page:5, Index:8, Temporal Feature Matters: A Framework for Diffusion Model Quantization, https://arxiv.org/pdf/2407.19547, 2025-07-11
2025-11-09 20:54:47,351 - INFO - root - Page:5, Index:9, EfficientQAT: Efficient Quantization-Aware Training for Large Language Models, https://arxiv.org/pdf/2407.11062, 2025-05-19
2025-11-09 20:54:47,351 - INFO - root - Page:5, Index:10, Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT, https://arxiv.org/pdf/2407.11041, 2025-10-31
2025-11-09 20:54:47,352 - INFO - root - Fetching page 7 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=300
2025-11-09 20:54:53,912 - INFO - root - get_all_titles_from_web 
2025-11-09 20:54:53,912 - INFO - root - Page:6, Index:0, BoA: Attention-aware Post-training Quantization without Backpropagation, https://arxiv.org/pdf/2406.13474, 2025-06-06
2025-11-09 20:54:53,913 - INFO - root - Page:6, Index:1, A Rescaling-Invariant Lipschitz Bound Based on Path-Metrics for Modern ReLU Network Parameterizations, https://arxiv.org/pdf/2405.15006, 2025-06-13
2025-11-09 20:54:53,913 - INFO - root - Page:6, Index:2, Scheduling Weight Transitions for Quantization-Aware Training, https://arxiv.org/pdf/2404.19248, 2025-10-01
2025-11-09 20:54:53,913 - INFO - root - Page:6, Index:3, GPTVQ: The Blessing of Dimensionality for LLM Quantization, https://arxiv.org/pdf/2402.15319, 2025-06-03
2025-11-09 20:54:53,913 - INFO - root - Fetching page 8 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=350
2025-11-09 20:55:01,153 - INFO - root - get_all_titles_from_web 
2025-11-09 20:55:01,153 - INFO - root - Page:7, Index:0, Squat: Quant Small Language Models on the Edge, https://arxiv.org/pdf/2402.10787, 2025-07-01
2025-11-09 20:55:01,154 - INFO - root - Page:7, Index:1, L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models, https://arxiv.org/pdf/2402.04902, 2025-07-21
2025-11-09 20:55:01,154 - INFO - root - Fetching page 9 with URL: https://arxiv.org/search/?query=Quantization-Aware-Training&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=400
2025-11-09 20:55:07,954 - INFO - root - ------------------------------------------------------------------------------------------------------------------------
2025-11-09 20:55:07,955 - INFO - root - File already exists, skipping download: d:\ChatPaper\academic Papers\Quantization-Aware-Training\A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat.pdf
2025-11-09 20:55:07,958 - INFO - root - File already exists, skipping download: d:\ChatPaper\academic Papers\Quantization-Aware-Training\FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error.pdf
2025-11-09 20:55:07,959 - INFO - root - 跳过已处理论文 A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies：d:\ChatPaper\academic Papers\Quantization-Aware-Training\A Quantized VAE-MLP Botnet Detection Model_ A Systematic Evaluation of Quantizat.pdf
2025-11-09 20:55:07,959 - INFO - root - 跳过已处理论文 FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error：d:\ChatPaper\academic Papers\Quantization-Aware-Training\FP8-Flow-MoE_ A Casting-Free FP8 Recipe without Double Quantization Error.pdf
2025-11-09 20:55:07,960 - INFO - root - summary time: 68.21 seconds
