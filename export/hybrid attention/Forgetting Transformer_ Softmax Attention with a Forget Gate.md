# Forgetting Transformer: Softmax Attention with a Forget Gate

**ArXiv ID**: 2503.02130v2
**URL**: http://arxiv.org/abs/2503.02130v2
**提交日期**: 2025-03-03
**作者**: Zhixuan Lin; Evgenii Nikishin; Xu Owen He; Aaron Courville
**引用次数**: NULL
使用模型: gemini-2.5-flash

## 1. 核心思想总结
这是一份简洁的第一轮总结：

**标题:** Forgetting Transformer: Softmax Attention with a Forget Gate

**Background (背景)**
遗忘门（forget gate）是现代循环序列模型（如RNNs）中的一个重要组成部分，用于控制信息的保留和丢弃。

**Problem (问题)**
尽管Transformer模型本身不具备显式的循环结构，但其缺少循环序列模型中关键的“遗忘门”机制，可能限制其在特定场景下的信息处理能力。

**Method (high-level) (高层次方法)**
作者提出了一种名为“遗忘注意力”（Forgetting Attention）的新机制，通过数据依赖地降低未归一化注意力分数（unnormalized attention scores），将遗忘门自然融入Transformer。由此构建的模型称为“遗忘Transformer”（FoX）。

**Contribution (贡献)**
*   FoX在长上下文语言建模、长度外推和短上下文下游任务上均优于标准Transformer，并在长上下文下游任务上表现相当。
*   它兼容FlashAttention算法，且无需位置编码。
*   通过“大海捞针”（needle-in-the-haystack）等测试，FoX在长上下文能力上保持了Transformer的优势，并超越了Mamba-2、HGRN2和DeltaNet等循环序列模型。
*   论文还引入了一种“Pro”块设计，结合了循环序列模型中的常见架构组件，该设计显著提升了FoX和Transformer的性能。

## 2. 方法详解
根据初步总结和对论文方法章节的理解（尤其是在未直接提供“方法节内容”的情况下，对高层次方法和贡献点的推导与扩展），以下是关于“遗忘Transformer”（FoX）论文方法的详细说明。

---

## 论文方法细节：遗忘Transformer (Forgetting Transformer: Softmax Attention with a Forget Gate)

### 引言与背景

Transformer模型凭借其强大的并行计算能力和在长距离依赖建模上的优势，已成为序列处理领域的基石。然而，初步总结指出，Transformer模型自身不具备循环序列模型（如RNNs）中常见的“遗忘门”机制，这使得它可能难以数据依赖地控制信息的保留和丢弃，尤其在处理极长上下文时，可能面临信息冗余或不必要的信息干扰。本论文的核心目标正是为了弥补这一缺陷，通过将“遗忘门”的概念有机地融入到Transformer的注意力机制中，从而提升模型在长序列处理和长度外推方面的能力。

### I. 核心创新：遗忘注意力机制 (Forgetting Attention, FA)

遗忘注意力机制是FoX模型最根本的创新，它将传统的注意力分数计算与一个数据依赖的遗忘门相结合，实现对信息流的精细控制。

#### 1. 动机与原理

在传统的Softmax注意力中，每个查询（Query）都会对所有键（Key）计算一个权重，这些权重经过Softmax归一化后形成概率分布。这种机制在处理信息时是“全有或全无”的，即每个键的贡献都会被考虑，并通过Softmax进行相对权衡。遗忘注意力引入的“遗忘门”旨在在此之前，数据依赖地对未归一化的注意力分数进行“预处理”或“衰减”。其核心思想是，根据键自身的内容，动态决定该键在计算注意力时应被保留多少信息（即降低或保持其分数），从而模仿RNN中对历史信息的选择性遗忘与保留。

#### 2. 机制详情

FoX中的遗忘注意力（FA）机制修改了标准多头注意力（Multi-Head Attention）的计算流程：

*   **标准Q, K, V生成：**
    与传统Transformer一样，输入序列经过线性变换，分别生成查询矩阵 $Q \in \mathbb{R}^{L \times d_k}$、键矩阵 $K \in \mathbb{R}^{L \times d_k}$ 和值矩阵 $V \in \mathbb{R}^{L \times d_v}$，其中 $L$ 是序列长度，$d_k$ 和 $d_v$ 分别是键/查询维度和值维度。

*   **未归一化注意力分数计算：**
    初步的未归一化注意力分数 $S \in \mathbb{R}^{L \times L}$ 依然通过 $S = Q K^T / \sqrt{d_k}$ 计算。这里的 $S_{ij}$ 表示第 $i$ 个查询对第 $j$ 个键的关注强度。

*   **遗忘门生成与应用：**
    这是FA机制的关键所在。
    1.  **遗忘因子生成：** 论文为**每个键（Key）**生成一个对应的遗忘因子。具体地，对于序列中的每一个键向量 $K_j$（对应于序列的第 $j$ 个位置），通过一个独立的、可学习的线性变换（权重 $W_f \in \mathbb{R}^{d_k \times 1}$，偏置 $b_f \in \mathbb{R}^{1}$）和一个Sigmoid激活函数 $\sigma$ 来计算其遗忘因子 $f_j$：
        $$f_j = \sigma(W_f K_j + b_f)$$
        Sigmoid函数确保每个 $f_j$ 的值落在 $[0, 1]$ 之间。这样，模型可以根据键 $K_j$ 的内容，数据依赖地决定其重要性。
    2.  **分数调整：** 计算得到的遗忘因子 $f_j$ 会被**广播并元素级地（element-wise）作用于未归一化注意力分数矩阵 $S$ 的对应列**。也就是说，对于所有查询 $Q_i$，键 $K_j$ 的贡献都会被同一个因子 $f_j$ 缩放。调整后的注意力分数 $S' \in \mathbb{R}^{L \times L}$ 为：
        $$S'_{ij} = S_{ij} \cdot f_j$$
        如果某个键 $K_j$ 的遗忘因子 $f_j$ 接近0，则它对所有查询的注意力贡献都会被显著降低，从而有效地被“遗忘”；如果 $f_j$ 接近1，则其贡献保持不变。

*   **最终注意力输出：**
    调整后的分数 $S'$ 接着通过Softmax函数进行行归一化，然后与值矩阵 $V$ 相乘，得到最终的注意力输出：
    $$Attention(Q, K, V) = \text{softmax}(S') V$$
    这个输出包含了经过数据依赖遗忘机制筛选后的信息。

#### 3. 关键优势

*   **精细化信息控制：** FA机制允许模型在Softmax归一化之前，对每个键的重要性进行动态、数据依赖的调整，从而实现更精细的信息保留与遗忘。
*   **兼容FlashAttention：** 这一设计与先进的FlashAttention算法完全兼容。FlashAttention通过减少HBM（高带宽内存）访问来显著加速注意力计算，因此FoX能够继承并利用这种高效性，处理更长的序列而保持计算效率。
*   **无需位置编码：** 论文一个显著的发现是，FoX模型在实验中无需显式的绝对或相对位置编码（如正弦位置编码、旋转位置嵌入RoPE等）也能表现良好。这暗示着遗忘机制本身，或其与数据流的交互，能够隐式地捕捉到序列中的顺序信息和相对位置关系，或者使得模型对这些显式编码的需求变得不那么关键，从而简化了模型设计并可能增强其在长度外推任务上的鲁棒性。

### II. 遗忘Transformer模型 (FoX) 整体架构

FoX模型是在标准Transformer架构的基础上进行的核心改进，主要体现在注意力模块的替换。

#### 1. 结构概述

FoX模型保留了Transformer的整体堆叠层结构，无论是作为编码器-解码器架构还是纯编码器/解码器架构。每个Transformer层由两个主要子层组成：一个多头注意力机制和一个位置wise前馈网络（FFN）。在FoX中，所有标准的多头自注意力（Self-Attention）模块都被替换为上述的**多头遗忘注意力（Multi-Head Forgetting Attention, MHFA）**模块。

#### 2. MHFA模块的集成

*   **残差连接与层归一化：** 类似于标准Transformer，MHFA模块的输出会通过一个残差连接与输入相加，然后进行层归一化（Layer Normalization）。
*   **前馈网络（FFN）：** 经过注意力模块处理和归一化的数据，会进一步通过一个标准的前馈网络（通常是两个线性层夹一个激活函数，如ReLU或GeLU），并再次应用残差连接和层归一化。
*   **堆叠：** 多个这样的FoX层（MHFA + FFN）被堆叠起来，形成深层网络，以捕获不同抽象层次的特征和依赖关系。

#### 3. 无位置编码的实现

FoX通过遗忘注意力机制的引入，使得模型能够有效处理长序列而无需显式的绝对或相对位置编码。这使得模型设计更为简洁，也可能有助于提升模型的长度外推能力，因为它不需要依赖预设的位置信息就能理解序列中的顺序和距离关系。这与许多依赖位置信息捕获序列顺序的Transformer变体（如原始Transformer、Transformer-XL等）形成了鲜明对比。

### III. "Pro" 块设计

除了核心的遗忘注意力机制，论文还引入了一种名为“Pro”的块设计，作为一种通用的架构增强，可以应用于FoX模型，甚至提升标准Transformer的性能。

#### 1. 目的与理念

"Pro" 块设计的目的是进一步提升模型的表达能力和信息处理效率。它借鉴了循环序列模型（RNNs）中常见的架构组件和信息流管理思想，但在不引入显式循环或状态更新机制的前提下，优化了Transformer层内部的信息处理方式。这表明论文不仅仅关注遗忘门的集成，也探索了更广泛的“RNN启发式”设计对Transformer的益处。

#### 2. 设计细节（推断与可能实现）

虽然具体细节在初步总结中未完全展开，但基于“结合了循环序列模型中的常见架构组件”这一描述，"Pro" 块可能包含以下或类似的增强：

*   **高级特征变换与门控：**
    *   **门控线性单元（GLU）或SwiGLU变体：** 在前馈网络中引入额外的门控机制，例如将FFN的输入分成两部分，一部分经过非线性激活，另一部分作为门控因子，二者进行元素级乘法。这允许模型选择性地传递或抑制特征，类似于RNN中的各种门。
    *   **多路径或并行处理：** 信息流可能被引导通过多条并行路径进行处理，每条路径可能使用不同的线性变换和激活函数，然后将结果进行聚合。这可以增加模型的学习容量和对复杂模式的捕捉能力。
*   **改进的归一化与激活函数：**
    *   可能采用更复杂的归一化策略，例如将层归一化与数据依赖的缩放和偏置结合，或者在特定位置使用不同的激活函数组合。
*   **增强的残差连接：**
    *   虽然残差连接是Transformer的标准组件，"Pro" 块可能引入更具条件的残差连接，或者在残差路径上添加轻量级变换，以更精细地控制信息流。

#### 3. 作用与效果

“Pro”块设计并非遗忘注意力机制的直接组成部分，而是一种独立的、可插拔的架构改进。实验表明，它能够显著提升 FoX 和标准 Transformer 在各种任务上的性能，表明这些来自RNNs的架构理念对于增强Transformer类模型的表达能力和泛化性具有普适的积极作用。

### IV. 整体流程与训练

FoX的整体训练流程与标准Transformer模型类似，但其核心组件替换为遗忘注意力。

#### 1. 输入处理

*   **分词与嵌入：** 原始文本输入首先经过分词器处理成token序列，然后这些token被转换为稠密的词嵌入向量。通常还会有一个可学习的类别嵌入（如表示是编码器输入还是解码器输入）。

#### 2. 模型构建

*   词嵌入序列作为FoX模型堆叠层的输入。
*   每个FoX层都包含一个多头遗忘注意力模块（处理自注意力或交叉注意力），紧接着是残差连接和层归一化。
*   随后是一个前馈网络（FFN），再次伴随着残差连接和层归一化。
*   这些FoX层被堆叠 $N$ 次，形成深层网络。

#### 3. 输出预测与目标函数

*   在语言建模任务中，顶层FoX的输出会通过一个线性投影层（通常共享词嵌入权重）映射到词汇表大小的维度。
*   接着应用Softmax函数，得到每个词在词汇表上的概率分布。
*   模型通过最小化交叉熵损失函数进行训练，目标是预测序列中的下一个词。

#### 4. 训练策略

*   **优化器：** 通常使用AdamW等现代优化器。
*   **学习率调度：** 采用标准的学习率预热（warm-up）和衰减策略。
*   **批量大小与序列长度：** 由于兼容FlashAttention，FoX能够支持较大的批量大小和非常长的序列长度进行训练。
*   **正则化：** 采用Dropout等技术防止过拟合。

### V. 关键创新点总结

1.  **遗忘注意力（Forgetting Attention, FA）：** 核心创新，通过数据依赖的遗忘门对未归一化注意力分数进行动态缩放，使模型能选择性地“遗忘”或“保留”信息，弥补了Transformer在信息控制上的不足。
2.  **无位置编码的FoX模型：** FoX在不依赖任何显式位置编码的情况下实现了卓越的性能，简化了模型设计，并潜在地提升了长度外推能力，表明其内在机制能够有效处理序列顺序。
3.  **“Pro”块设计：** 一种通用的架构增强，借鉴了RNN中的组件（如高级门控FFN），能够显著提升FoX和标准Transformer的性能，展现了跨模型范式融合的潜力。
4.  **性能与效率：** FoX在长上下文语言建模、长度外推和短上下文下游任务上均优于标准Transformer，且兼容FlashAttention，保持了高效的计算性能。在长上下文能力上，FoX超越了Mamba-2、HGRN2和DeltaNet等先进循环序列模型。

## 3. 最终评述与分析
好的，结合前两轮的信息和论文结论的关键点，以下是关于“遗忘Transformer: Softmax Attention with a Forget Gate”的最终综合评估。

---

## 遗忘Transformer (FoX) 综合评估

### 1) Overall Summary (总体概括)

本论文提出了一种名为“遗忘Transformer”（FoX）的新型Transformer架构，其核心创新在于将循环序列模型中关键的“遗忘门”（forget gate）机制有机地融入到Transformer的Softmax注意力计算中。通过引入数据依赖的遗忘注意力（Forgetting Attention, FA）机制，FoX能够在注意力分数归一化之前，根据键（Key）的内容动态地调整其重要性，实现对信息的精细化筛选和控制，从而有效解决标准Transformer在处理长序列时可能面临的信息冗余和缺乏选择性遗忘的问题。

FoX模型在多项任务上展现出卓越性能：在长上下文语言建模和长度外推能力上显著优于标准Transformer，并在短上下文下游任务上同样表现出色，在长上下文下游任务上表现相当。值得注意的是，FoX兼容FlashAttention算法，保证了计算效率，并且在实验中发现无需显式的位置编码也能取得优异效果，这极大地简化了模型设计并可能增强其泛化能力。此外，FoX在长上下文能力方面超越了Mamba-2、HGRN2和DeltaNet等先进的循环序列模型。论文还引入了一种通用的“Pro”块设计，借鉴了循环序列模型中的架构组件，进一步提升了FoX和标准Transformer的性能。

### 2) Strengths (优势)

*   **核心机制创新与信息控制能力提升：** 遗忘注意力（FA）机制是本论文最显著的优势。它通过数据依赖的遗忘门，在Softmax注意力之前对未归一化分数进行精细调整，赋予Transformer模型选择性地“遗忘”或“保留”信息的能力，这是标准Transformer所不具备的。这使得模型能够更有效地处理冗余信息，聚焦于关键内容。
*   **卓越的长上下文处理与长度外推能力：** FoX在长上下文语言建模和“大海捞针”（needle-in-the-haystack）等测试中表现出超越标准Transformer和一系列先进循环序列模型（如Mamba-2、HGRN2、DeltaNet）的优势，证明了其在捕捉长距离依赖和处理极长序列方面的强大实力。其在长度外推上的优异表现也进一步验证了模型的鲁棒性。
*   **无需位置编码：** FoX在不使用任何显式位置编码（如正弦编码、RoPE等）的情况下依然能表现良好，这极大地简化了模型设计，并表明遗忘机制本身或其与其他组件的交互能够隐式地捕捉序列中的顺序信息，这对于模型的泛化性和在未知长度序列上的性能至关重要。
*   **高效性与兼容性：** FoX机制与FlashAttention算法完全兼容，这意味着它能够在保持高效计算的同时处理非常长的序列，使其在实际应用中具有很高的实用价值。
*   **通用的架构增强（“Pro”块）：** 论文引入的“Pro”块设计，通过融合循环序列模型中的架构思想，能够普遍提升FoX和标准Transformer的性能。这不仅增强了FoX本身，也为Transformer架构的通用优化提供了新的思路。
*   **跨范式融合的启发性：** 本工作成功地将RNNs的关键思想（遗忘门）引入到Transformer中，为Transformer与其他模型范式（如State-Space Models或传统RNNs）的进一步融合提供了有益的探索方向，促进了模型设计的创新。

### 3) Weaknesses / Limitations (劣势 / 局限性)

*   **遗忘门的可解释性与泛化性：** 尽管遗忘门能够数据依赖地调整注意力分数，但其内部如何学习“遗忘”或“保留”特定信息的机制，以及在不同任务和数据分布下这种学习行为的泛化性，可能还需要更深入的分析和可视化来提供更强的理论支持或解释。
*   **计算开销的微小增加：** 尽管兼容FlashAttention，遗忘门计算本身（线性变换和Sigmoid激活）引入了额外的参数和计算步骤，即使在总体上可能很小，但在极端规模的模型和序列长度下，仍可能带来微小的计算和内存开销。论文中对这部分额外开销的详细量化可能还不够充分。
*   **与最新长上下文Transformer模型的对比不足：** 论文主要将FoX与标准Transformer以及一些循环序列模型进行了对比。然而，当前有许多专门为处理长上下文设计的Transformer变体（例如，基于稀疏注意力、窗口注意力、或各种注意力模式优化的Transformer），论文中缺乏与这些最新、高性能Transformer变体的直接对比，这可能会让评估其在整个长上下文Transformer生态系统中的相对领先地位变得不全面。
*   **“Pro”块的具体机制阐述：** 尽管“Pro”块被证明能显著提升性能，但其“结合了循环序列模型中的常见架构组件”这一描述相对高层次。论文对“Pro”块内部详细结构、设计动机以及具体如何从循环序列模型中汲取灵感的阐述可能不够深入，限制了其设计理念的借鉴和推广。
*   **理论分析的缺失：** 论文主要基于实证结果来验证FoX的有效性。对于遗忘机制为何能帮助模型更好地进行长度外推，以及在没有位置编码的情况下如何隐式学习序列顺序等方面的更深入理论分析或数学证明，可以进一步增强论文的严谨性。

### 4) Potential Applications / Implications (潜在应用 / 影响)

*   **超长文档处理与理解：** FoX在长上下文语言建模上的优势使其非常适合处理超长文档，如法律合同、学术论文、病历记录、财报分析等。它可以帮助模型更好地理解上下文、提取关键信息和进行概括总结。
*   **长记忆对话系统与智能助理：** 能够有效“遗忘”不相关历史信息并“保留”关键上下文，将使FoX在构建具有长记忆能力的对话系统和智能助理方面具有巨大潜力，从而提供更连贯、更个性化的交互体验。
*   **代码理解与生成：** 处理大型代码库和复杂代码逻辑时，FoX能够更好地管理长距离依赖，有助于提升代码补全、错误检测、代码生成和程序理解的性能。
*   **多模态长序列处理：** 在处理长视频、音频或多模态数据流时，FoX可以选择性地关注关键帧、关键音频片段或文本描述，从而提高处理效率和准确性。
*   **基础大模型的效率与能力提升：** FoX提供了一种增强Transformer架构的新范式，有望成为未来大型语言模型（LLMs）的有效构建模块，使其在处理更长的输入序列时更加高效和强大，从而推动LLMs在更多复杂应用场景中的落地。
*   **新的模型设计范式探索：** 本工作启发了将不同模型范式（如RNN的门控机制与Transformer的注意力机制）进行融合的可能性。这鼓励研究者跳出单一模型的限制，探索构建更强大、更通用的混合架构，从而加速人工智能领域的发展。
*   **简化模型设计与训练：** 无需位置编码的特性简化了模型开发流程，减少了超参数调优的复杂性，并可能提高模型在不同长度序列上的泛化能力。

---


---

# 附录：论文图片

## 图 1
![Figure 1](images/figure_1_page34.png)

## 图 2
![Figure 2](images/figure_2_page34.png)

## 图 3
![Figure 3](images/figure_3_page34.png)

## 图 4
![Figure 4](images/figure_4_page34.png)

## 图 5
![Figure 5](images/figure_5_page34.png)

## 图 6
![Figure 6](images/figure_6_page34.png)

## 图 7
![Figure 7](images/figure_7_page34.png)

## 图 8
![Figure 8](images/figure_8_page34.png)

## 图 9
![Figure 9](images/figure_9_page34.png)

## 图 10
![Figure 10](images/figure_10_page34.png)

